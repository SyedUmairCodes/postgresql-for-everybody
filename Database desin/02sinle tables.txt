# Working with Tables and PostgreSQL

So now we're actually going to type the SQL commands that actually work with the data that's in the tables. And so we start with INSERT, SQL INSERT. And again, we have this situation where it's kind of wordy. INSERT INTO is kind of the keyword, name of table, and then a parenthesized list of the column names from the schema, the word VALUES, and then there is a one-to-one correspondence. There are two here. There's got to be two there. One-to-one correspondence between the values, and away you go and then you put a semicolon at the end of each line. And then you would type these into your pgsql, right? And that would insert records. The DELETE command, talking again to pgsql, says DELETE FROM, users is the name of the table, WHERE clause, you're going to see WHERE clauses on a lot of SQL commands. And the reason is that SQL is not a procedural language. I've already mentioned that. And so there is no real concept of a loop in SQL. The DELETE FROM implies loop. And if you don't say WHERE, if you take this off, it would be like delete all the rows. Delete all the rows from users, if you took this off. You could imagine it sort of implies that. Delete all the rows from users. This is kind of like a loop. It'd be like for blah, blah, blah, in rows do something. But you don't say that, because DELETE has kind of an implied loop around it. And so if you don't say that, you just emptied out your entire user table, literally, you emptied out your entire user table. So you're like oh, wait, wait, wait. I didn't really mean to do all of it. So really, all I meant to do was delete all the rows from users where the email equals this. And so it's like a loop plus an if statement. So DELETE FROM users, WHERE is a loop plus an if statement. Loop through all the rows and users, and then if email equals ted@umich.edu, delete it. Otherwise, skip it. Now, there is no way that it would be efficient to write that loop to read all that table and then delete one line. This is an abstraction. Because that's not how it's done in a database. It's got some complex place. It has got a little data here, and a little data here, and some index over here that points to this little bit of data. And then it just like goes and changes this to deleted. Instead of reading tons of data, which there might be like a million records in users, it doesn't read them. If we're doing sequential master update, literally a delete could take four hours. Because we're copying all of it to the new one and we're taking all the records except one and copying them, and that could be four hours, right? That's not how it works in a relational database. How it works in a relational database is there's some complex structure on disk that you have no idea what's going on, and there are some little mark that happens and that's that, and then maybe a week later it cleans something up. You don't even know. Did I mention that I love SQL? I do love SQL. Because you'd don't have to write loops and you don't have to write if statements. But you have to write WHERE clauses, otherwise you delete all your rows. UPDATE. So create, delete, update, read, I'm doing it in the wrong order. CRUD is the right. We're getting through CRUD right now. So UPDATE. UPDATE, the name of the table,
Play video starting at :3:54 and follow transcript3:54
SET is an SQL statement. This is the column and the new values. You can actually have a comma-separated of those. Name equals Charles, email equals, etc. And then you've got to be careful because this, like DELETE, is an implied loop. And so you need a WHERE clause. You can literally update every column in a database if you want. It's rare. But you might update them to like set them to null or something like that. So you have a WHERE clause that says this implied goes through all of them, and then there's some if statement that when this is true, then do it. Now the interesting thing is, is if there were two records that had email equals csev@umich.edu, it would update both of them. Okay? So this doesn't just do one record, it does every record where this is true. Like if I said where email has an at sign in it, then it would get rid of all of them because it would match. So it's not just one, this actually will return. If you run it, it'll return like one row updated or 11 rows updated. Again, you're asking for something to be true. When this is done, the following is true. All the users that email of csev@umich.edu have their name set to Charles when this is done, please magically do that for us, and don't make us tell you how to do that.
Play video starting at :5:25 and follow transcript5:25
Retrieving is SELECT. Now that you know all of these, you'll probably use SELECT first. So the SELECT command kind of implies this WHERE clause being here, right? SELECT WHERE email. SELECT star, you give it a list of columns, could be name comma email. SELECT star says give me all of them. FROM and SELECT are keywords, and then there's a WHERE clause. Because again, this has like an implicit loop on it. SELECT has an implicit loop on it that runs over and over and over and over again. So SELECT star FROM users just says get me all the users. Right? SELECT star FROM users WHERE email equals csev@umich.edu. Again, there's an implied loop around this SELECT, and the WHERE clause reduces that. Now, again, it doesn't actually read all the data. It's got clever indexes and it finds it fast and maybe it takes very little disk access or maybe it's already in memory. You don't know. We don't care. We love SQL. Sorting is an important part of these things. So you can use the ORDER BY. SELECT star FROM users ORDER BY email. That basically says I would like this ordered. It's ascending order normally and you can say DESC for a descending, add DESC toward the end there and then you can descend it. You can do wildcards.
Play video starting at :6:48 and follow transcript6:48
This basically is looking for things where the name has the letter e in it anywhere because the percent signs function as the wildcards. This wildcard the way I've got it, unless you put some very special indexes on here, this actually probably would force what's called a full table scan, which means it's really going to have to retrieve the records because it can't build an index. All those WHERE clauses, it could build an index to short-circuit, and probably, I keep talking about index. So I'm going to talk about index in a second. But I'll talk about them right here. So let's just say, going back to this like sequential thing, where you got like a terabyte of data, right?
Play video starting at :7:31 and follow transcript7:31
And the index is basically like a little set of shortcuts into this. You can think of it as a set of shortcuts to say jump. And so the index is smaller than the data so you kind of can jump and then you can look for a little bit of data, That's kind of how an index works. The problem with this LIKE, is that there's no good way unless you make a very special kind of index to take advantage of an index. So this has to do a full table scan which means it has to pull all the records to check the WHERE clause and read all the way through the data from beginning to end. Later, we're going to talk about query optimization and query planning,
Play video starting at :8:14 and follow transcript8:14
because at the end of the day Postgres can look at this and say, do you realize that because you asked for that, I got to do a full table scan? Because it will tell you that. And it can look at some of these other ones and it can say, oh, that doesn't take a table scan at all. That uses the index. Actually, that one will take the full table scan but let me go to a different one like this WHERE clause, WHERE email equals csev@umich.edu, the query planning will say, "I've got an index for that and this is very efficient." And so you can tell when your application starts to slow down, you can ask for help from the database and say, hey, what are we doing? So the LIKE clause is actually really useful for some of the data mining things because it's a wildcard. You're like oh, let's go look around. It's a little slower than a WHERE clause sometimes, a WHERE clause that has an exact match. Although for some indexes, I'm so obsessed. For some indexes if you have like quote blah, blah, blah, blah, blah percent at the end, if it's a prefix lookup, then it can actually use indexes to improve its performance. Part of why we even use databases in the first place is to go fast. And that's why I'm always talking about performance, performance, performance. Again, go back to the 50 terabytes to log in and somehow it happens in under a quarter of a second.
Play video starting at :9:31 and follow transcript9:31
This LIMIT and OFFSET clauses are how we do like paging. So if you're like going through a long list of things and it's like, this is 1-25. Next. And then it's like this is 26-50. Next. So this actually is well supported in databases. So you actually don't have to take your server and your database and somehow retrieve all the rows and then only show the first 25. Right? You actually ask for the first 25. So do your magic thing that it's very good at using index, etc. and give me only 25 rows back, as compared to your application code throwing them away. And you can use this OFFSET that says skip ahead a certain amount and then give me the next 25. Now, I'm making these short because all my examples are short, about four or five rows. But the only other thing to make sure to mention is that these offsets go from row zero. Which is like Python lists. So that's not all bad. You just got to remember. So that OFFSET 1 is really the second row, not the first row. But these are super-efficient and really necessary for looking through long lists. Counting is another common thing. Counting is more efficient than retrieving all the rows and then looping through the rows and counting them. You can say, look, I just want to know. So COUNT star as just count me the rows and users. Now, the interesting thing is it might actually read all of the users to do this, but chances are good that it actually knows somewhere in its own internal structures how many rows there are, so it just tells you. The WHERE clause might actually require a little bit of data to be scanned, but not so critical. So you can count them, which is a lot cheaper than actually retrieving them and sending them back to you. So this is the stuff I showed you so far.
Play video starting at :11:23 and follow transcript11:23
You're like, I went to college and I'm taking a database class, and that's it? The answer is yep. Sort of. This is the core of SQL. The core of SQL is beautiful. It's simple. It's really easy to understand. It's in a sense easier than learning how to write programs, far easier than learning. People look at SQL and they're scared of it. No, you shouldn't be scared of it because it's just like this. You have this little syntax that allows you to like reach in and grab things and poke things, and pull things out, and delete them, and it's a really beautiful abstraction, which goes back to the notion in the '60s and '70s when they defined SQL. They said, look, let's give something that's simple for the programmers to use that hides the complexity. And that's when this abstraction is most powerful. Now, we have not yet begun to do complex things. We've only done a single table, and it gets a little trickier and more interesting, I think fascinating, when we start exploiting relationships between two tables. Because that's really where things work the most. So up next, we're going to talk about datatypes in Postgres, the different kinds of things that you can express when you're building a schema for your table.

# Data Types in PostgreSQL

So we talked a little bit about how the different kinds of commands do the CRUD operations and WHERE clauses and things like that. Now we're going to talk about what kind of data types that you can define in the schema of your tables. So we've got some text fields, we've got binary fields, sometimes those are called blobs, numeric fields, and then auto-increment fields. VARCHAR was the one that we did before and that's an efficient storage for characters of varying length from one character up to 128 or whatever. CHAR is a fixed space. Usually we use these for shorter strings. I mean, once your string gets above 64 characters you might have a character that has a blah blah blah blah blah and then it ends and you don't want to waste the time for that. But there are certain time places in databases like for GUIDs, Global Unique Identifiers, that you know they're going to always be like 64 characters and so CHAR is used for that. I tend to use CHAR when I know like I'm going to store hash, which is exactly 64 characters, or a GUID which is exactly 64 characters. When you can do that, database loves you, especially if you're going to fill it all up. If you're not filling it all up you're not doing yourself any favors. But if you have 64 and it's always 64 and it's never lower, then the database actually kind of loves you if you can say that. So use VARCHAR unless you really know that it's kind of a fixed thing. Sometimes you have fixed things that are only like two characters long, it likes that too. So you would never say VARCHAR 2. I don't kbnow, somewhere between 64 and 128. I would always use VARCHAR, say, above 64 just kind of my own, whatever. And the way it allocates it is the VARCHAR has a count and then the actual data, so you can pack it in a lot more tighter. Whereas the CHAR is just chunk, chunk, chunk, which means it's kind of uncompressed in a sense. But if you're going to fill it all up, there's no way to compress it. Okay. Text fields. This is what I talked about it before is if you want a text field and you don't want to have the database enforce a length on you, well, then just call it a text field. And these can be short, medium, or long. There's a count at the beginning, but the count, it's not that big of a deal. The key thing is that you generally don't use these for indexing and sorting. So you don't use these in a ORDER BY, you don't want to use these in a WHERE clause, you might use them in a LIKE clause knowing that it's going to be a full table scan. So if you're doing like a blog post and comments, or even just whole web pages that you're storing because you're building a spider to read web pages, text is great. And Postgres only has one, which I really like, lots of other databases have like a whole long list of different kinds of texts. But basically, you kind of have the things where you know the length and the things that you don't know the length and so that I think is a really simplifying notion that Postgres has. But it also has a character set. So what I mean when I say both the CHAR, VARCHAR, and TEXT have character sets, that means that they are not simple eight-bit characters and so the sort of traditional Western character set that we generally call Latin-1 or ASCII, that has a 127 characters and it fits into eight bits, and that's called a byte, which we're going to see in a second. And so you know that it's very efficient. Whereas when you have characters outside of the Latin character set, which might be like a cedilia or a Asian character set, those are longer than eight bits and so those are sometimes they can be eight bits, they can be 16, and they could even be 32 bits.
Play video starting at :3:39 and follow transcript3:39
And so if you have 100 characters that have a character set, that could be up to 400 bytes. But the nice thing is that we humans, when we're typing into forms or typing comments in a blog post, we're typing in those character sets and so you can't just say well, I'm going to write a web application that doesn't accept Asian characters or a web application that doesn't accept Spanish characters that there's only like the ASCII characters. So it's really important for databases to know the things that have character sets. And also in different character sets the sorting is different. So these are all the things that the database people have figured out for us. So character sets are very important, indexing, sorting, end-user input, and so CHAR, VARCHAR, and TEXT handle character sets. Now, why do I make such a big fuss about that? You also can store things that don't have character sets. Now, you could do this for like a GUID, a global unique user ID which you know is just numbers and letters A through F, which could fit in one of these things, and you know that. It's a string that has up to a 128 different characters in each one. So this BYTEA is the place that we store data that we're not letting the database know its character set and so you can store blobs of information, small images, etc. Integer numbers, there's a couple of different integer sizes. The normal integer that we use is a 32-bit integer that's 2 billion. That's used for most situations and then you can save space with a small integer, but then you're limited on the range. And big integers are much larger than 2 billion. But mostly we just make integer columns. You can have a number of different floating point columns. REAL is a 32-bit floating point that has seven digits of accuracy. And what we mean by accuracy as it has seven accurate digits, but you can put the decimal point anywhere in there.
Play video starting at :5:58 and follow transcript5:58
We've always had these 32-bit REAL numbers in all of computation from the beginning of time that 32-bit number with seven digits of accuracy is really only good for approximate computations. If you're taking like an average of the weather temperature over a long period of time, it's probably good enough. But if you're doing some real precise stellar calculation where you're calculating forces as like stars smash into each other, REAL is not good enough because then the errors creep in because it's only got seven digits of accuracy. So that's where DOUBLE PRECISION, we call it DOUBLE PRECISION because it's twice as big and usually for scientific computations where you're going to do lots of computations over and over and over again, like in a simulation, you always use DOUBLE PRECISION. Now it turns out that neither of those are good for money because the way fractions are represented in REAL and DOUBLE PRECISION, are there fractions with powers of two in the denominator? And so it turns out that like in America you have dollars and cents. Well, there's a 100 cents in the dollar so a cent is one-one hundreth of a dollar, but that one-one hundreth is not accurately represented in REAL or DOUBLE PRECISION. So we have this NUMERIC thing where you say okay, I would like two digits and I'd like 14 digits with two digits of decimal, and then it's perfect. So you can't represent money and there's lots of movies that like talk about like what happens when you can't represent money accurately where people like take the fractions of interest that are not represented in REAL and whatever, but if you're doing money, use NUMERIC. Dates, lots of things are important in dates. There is sort of a date and a time, but the thing we tend to use a lot is a thing called a TIMESTAMP. A TIMESTAMP is a 64-bit number and it represents minutes and seconds from 4713 BC to this big long AD. Years ago, this was a 32-bit number and we had this problem that the Unix time, which was the number of seconds since January 1 1970 and 32 bits, and that was going to run out of space in 2038. And so here's another xkcd. There's an xkcd comic for just about everything nerdy. And so clearly Postgres has switched to 64 bits for their timestamps. And if they hadn't, if they've stayed with 32-bit timestamps, in 2038 all the Linux systems and all the databases were going to blow up. But when they go to 64-bit, we can go to almost 300,000 AD without running out of space. And I'm not going to worry about that. Okay? Like 2038, we're getting to 2019, and 2020, and 2022. 2038 was coming, but they just went to 64 bit, and all these computers are 64 bit, and all these databases are 64 bit. So we don't have to worry about timestamps running out of time in 2038. So up next, we're going to talk about the things I've been talking about all along about how these things work with performance and how fast they go, etc., etc.

# Database Keys and Indexes in PostgreSQL
So I've been talking all along about how we're doing all this work to be super fast and super efficient, and I've been drawing pictures of indexes, and stuff like that. So finally, it's time to talk about them. So we're going to talk about indexes now. So the first thing that we're going to do is we're going talk about keys. Keys are when we're making connections between tables. We connect one table to another table, and I kept saying that's the relation, right? That's the relationship between this table and the other table. And we need to be able to put columns in tables that are kind of like our handles for rows. They're our way to reference a row super efficiently, right? And later we're going to figure out how to build these tables, but we're going to start today just talking about how we put a number on every row. Now, it turns out that we can pretty much say this is row 1, 2, 3, 4, 5. So we kind of build this sequence, and it's an auto-incrementing sequence and it's automatically. So if two data records are coming in from multiple sources as fast as possible, the database kind of takes them as a funnel, and then carefully assigns them sequential numbers. You never get a duplicate, no matter how fast these records come in. And so the database sort of says, "lock, add one, insert, lock, add one, insert, lock, add one, insert," and so that's something that database takes care of us regardless of the number and the speed of the records that are coming in. And in Postgres, and if you ever look at other languages like MySQL or SQLite, it's harder than this. But PostgreSQL is like, "I'll make this easy," because it turns out that we do the same thing over and over, and in all those other languages, all those other databases, I simply copy and paste this long ugly line. But in Postgres, we just say SERIAL. So id is a serially incrementally automatic increment thing that just gets every time we insert it. Now we don't have to put it on an INSERT statement, because it's automatically generated by the database. So we'll still be inserting on name and email. And then we can say PRIMARY KEY. And so what PRIMARY KEY does is it says, "build an index for this," and it's the index we're going to use the most. This is going to be an integer number, and integer indexes are like scorchingly fast. There's been thousands of people who have researched how to make integer indexes super fast, and you don't need to know any of that. So you just need to know that each row has a little handle on it 1, 2, 3, 4, and then there is an index here that's PRIMARY KEY that we can go and find one of these super fast. So that's this index that sits here. And by saying PRIMARY KEY, and you're not really telling it how to implement a primary key. You're just communicating the fact that I'm going to use this particular little id field in a very special way and be prepared, database, be prepared because I'm going to use it that way. The other thing we can do in this one that you can see is this UNIQUE constraint. This is what we call a logical key, and so the uniqueness basically says, "we are not allowed to insert the same email address twice on this column, that column is unique." And this is what we'd call a logical key, and that means if you try to insert it twice, it's going to blow up. Now, the way it works is there's all these rows with emails in them, and then there's another index, which is the addresses, and then it looks in this index and says, "oh, wait a second, you already have one here. No, you're not allowed to insert that." And so the uniqueness is a just like the 128, is a constraint in the schema that you are communicating, and then the database is going to enforce on you. Right? Now, it turns out that this index that it makes, it kind of realizes that index can then be used to speed access to certain records, and in this particular index it's probably going to make it so that sorting and prefix searching is going to be super fast. And like you don't even need to know that. All you just say is like, "Ah, this is unique." And like, "Oh, I know what to do with that. I will implement something really nice for you. I've a little surprise for you. The next time you do a SELECT statement on your one million records, it's going to be 20 times faster just because you told me that." So there's a whole bunch of functions, I won't go through them all. NOW is the function that we use for dates and NOW is 98 percent of my function use. Sometimes I'll do concatenations or sub-strings. Those things that you can do with functions. Now, indexes. I've been talking about the indexes from the beginning and now I kind of talk them. So like talk about them a little more. So when I log into Twitter, they got to go my password find my password among 500 million users, and the key thing is to shorten the scan, right? I keep saying that it's a couple of terabytes and if you go through it, even on a fast disk drive. if you go through it sequentially, it's minutes. Try to back your whole hard drive up some time. And that's how long it takes to read all the data on your hard drive. So you're not really scanning the whole data, right? So the whole idea of indexes is they're shortcuts. They're shortcuts so you know exactly where to go. So let's take a look at the two most common indexes. Trees and hashes are the most common index. So B-trees, I mean all this data ends up stored on disk, and an index is more data on disk. And so when you add an index like UNIQUE or PRIMARY KEY to a column, you actually are telling the database store more data. So you're telling the database don't just store the data I gave you, but store the data about where that data is. And so the idea of a B-tree, and the B kind of stands for balanced or binary, is that you go to a place, and then it's sorted, and you can kind of pick a range, right? And then you pick a range, and that tells you perhaps another index block to read, and then you pick a range in that index block, and then that tells you another index block to read, or maybe it tells you to go right to disk. And so instead of having a million disk reads, you get to do sort of log of a million, the log, it's the log and that's why it's balanced. So to go through a million records, you might actually only have to hit the disk in six blocks of the disk. The first block, the second block, the third block, and then the actual data block. Right? So that's called a B-tree. So let me draw a picture of the B-tree sort of in the way that I've been drawing it all long, so to connect back to this. So remember I'm talking about some data that's like let's just call it a terabyte of data, and if you have to scan a terabyte of data, you're going through it sort of in sequence, right? And so what you do is if this has some kind of a sorted structure, kind of going back to that sequential master update, you can basically have an index that gives you ranges. So you can break this into ranges of data depending on the size of each disk block. And then for each of the ranges, you make a little sort of a index that shows you the start and the stop in each of those ranges. And then you come and you read the index which itself as a small amount of data, and then based on that, you can pick the right range. You come in, you read a small amount of data, figure out which of these things by reading here, and then go straight to the right range on the disk. So I'm just showing you a two-level, there might be more than one level. And so that's the basic idea, but this is sort of sorted, but there's also blocks so there's sorting within blocks, and the whole thing doesn't have to be sorted, and so that's the idea of a tree index. Now, the cool thing about tree indexes is that they are good for exact match lookup like if you're looking up the email address of someone who just typed it in a login form. They also help for sorting. They also help for range lookups because you can end up knowing that you just have to grab a chunk of the data ultimately for range lookups. They're also good for prefix lookups, because prefixes are like ranges because you kind of grab a little bit, and that turns into a range, and then you scan that range sequentially, but it's a lot smaller than scanning the entire thing sequentially. So, hashes. So a hash is a little different. And the hashes are often used in things like those integer keys to make them super fast. So a hash is a computation. You scan the string and you compute a number. It could be as simple as adding up the letters and dividing by a million or adding the letters up and treating every letter like a number, like J might be 11, and O might be 14, whatever their sequential numbers. Add all those numbers up, and then divide by a million, and then take the remainder of the division by a million, and then use it as an offset, right? And so it's a calculation that you do. Now if you go read about hashing, you'll find that there are people who spend their whole life researching the best hash function, and if you read you'll hit things like MD5, SHA1, SHA256, these are all hashing algorithms. Actually, the NIST organization that helped build SQL, the NIST organization that helped build SQL also helps build hashes. Hashes are standards, and there's actually competitions that people spend years of their life figuring out the best way to take large and small blobs of text and have the best possible hash. The cool thing about hashing as a database technique is it really reduces the number. So if you think about the number of in a B-tree if you have a large number of records, the number of accesses goes by the log of them. Log is really a great reduction. If you can say the log of a million is like six, the log of 10 million is like seven. So log is really slow growing. But the hashes are even shorter. Now the problem with hashes, and the reason we don't use them for everything, is that hashes are only good for exact match. You are prefix matching, they are no good. The data gets completely non-sorted, so that's no good. So if you're looking up like a name or you're going to do sorting, then hash doesn't work. It turns out that what hashes are really good for is primary keys or GUID, Globally Unique Identifier kinds of lookups. And when they do that, they are super scorchingly fast. Okay? So hashes are awesome. Now, I'm just telling you about these two indexing mechanisms not because I want you to decide between them, because often the database will automatically decide for you which kind of index. You just kind of say this is a primary key index, chance are good that'll be a hash, or if you say UNIQUE on a string field, chances are good that's going to be a B-tree, because those are how those things work. But in general we even leave some of these decisions to behind the abstraction, and just say, "hey, database you're smarter than I am. You got a lot of PhDs that work on all this stuff, and so you just worry about that, I'm not going to worry about it." So SQL is my favorite language. I'm glad to be teaching it to you. Unfortunately, everything in the world can't be written in SQL, but we can certainly describe our data with that, and the whole idea is we describe the shape, the schema of the data to be stored, and then we have a simple set of primitives that allow us to store and retrieve the data, the create, read, update, and delete. So thanks for listening and welcome to SQL.

# 