# Lecture: JavaScript Object Notation
Welcome to our lecture on JSON. Next, we're going to talk about Postgres. But for now, I want to talk a little bit about JSON, give you a bit of historical context. The URL for these lecture notes is https://www.pg4e.com/lectures/06-JSON. So one of the things that JSON is very much about is data serialization. And data serialization is a problem when we are transfering data structures between programs in different languages or just programs in the same language running across the network exchanging data. And so the two most common data structures that we use inside of programming are linear structures and key-value structures. And in Python, we think of those as a list as a linear structure, just 0, 1, 2, 3, and then a dictionary is the key-value structure. But if you look at languages like JavaScript or PHP or Java, they all have these. And while we have more sophisticated data structures, they kind of all fall into a in general you can kind of model them as either key-value pairs or as linear lists. The only other data structure is actually a tree of information and we'll talk about that in a second. But the problem is and what the word serialization means is if you have a dictionary in Python and we need to send that to PHP or to JavaScript where it needs to be an object in JavaScript, you need a format that both Python and the JavaScript can agree on as the interchange format. We call it serialization because it was what was sent across the wire back when we had networks that were made of wires. And so it was a serial set of bytes, a set of characters that were sent across the wire and if you cut it in the middle and you watched what went back and forth between the two systems, you'd say that's our wire protocol and it goes by serially. And you'd say, well, okay, we're sending a dictionary to an object, but what's going across the wire? And so the serialization format. And an another word for this is marshalling and unmarshalling. So you marshall the data to prepare to send it and then you unmarshall it when you receive it. They're basically identical terms. Now in the early days, so from sort of 1990 like 1990s when HTML came out. And so in the early days, we looked at this less than, greater than HTML format and we're like we can use this to represent data. So we came up, the world came up with the thing called XML which stands for Extensible Markup Language. And it's basically a tree of tags like array and slash array. And then now we have some entries. And array, they don't mean anything special, you just put stuff in. But we were using this format to serialize or to marshall and unmarshall data between systems. You also would tend to serialize it when you put it in a database or wrote it to a file and then read it back from the file you tended to serialize it. So you could think of as you're editing in Microsoft Word, for example, Microsoft Word is like you're moving stuff and it's updating its internal structure, but then it has to serialize that data into a docx file. And in that case we use XML. So XML was the format of choice for serialization and we used it a lot. There was all kinds of libraries, frameworks, and strategies for XML that were very, very, very popular. But then what happened is as we moved from sort of servers talking to servers on networks and then instead we started having servers in the back end talking to browsers and browsers talking to JavaScript. Originally, we just had request/response cycles where you would go to a page, it would paint, and then you'd click on an href, an anchor tag, and you would go to a next page. But as we wanted to add more interactivity, we would write more of our application code in JavaScript. And sometimes we would talk to the server, pull data back, and then update the document object model without actually refreshing the whole page. That's how things like Facebook has a little red thing with the number of messages that either update or just show up when something happens. It's not changing the whole page, it's just changing a little corner of the page. And this pattern when it first came out. Well, we still call it AJAX, the pattern was basically let's take XML and have the JavaScript in the browser read the XML from a server. Now that server often didn't have it in XML. It just took whatever dictionary or list or whatever it had and it would turn it into an XML and then in the browser you'd take the XML and turn it back from the XML into an array or list or whatever it was. And that was how we started. The problem was is that XML is a hierarchy, it's a tree, and it's really good at representing things with trees and it's a bit self-documenting because you can choose the names of the tags in XML not like HTML, but in XML you choose the names of the tags. So people could look at the XML, and go like I kind of have a guess as to what that is. Although lots of XML is hard to read, this particular one says, I'm an array and I got an entry and each entry has a key and a value. I kind of know what that means. This kind of looks like an array of dictionaries or an array of objects or a list of dictionaries or an array of objects. But the problem is in general, it was complex to go through that and reconstruct the arrays when you actually just wanted to send an array or send a list or send a dictionary or send an object. And so Douglas Crockford, and I really encourage you to watch this video like video interview that I did of Douglas Crockford in Yahoo a number of years ago. Douglas Crockford said, you know what, there is this format that we use to specify object and array constants in JavaScript. Why don't we just use that as the serialization format? So that's why it's called JavaScript Object Notation. It should really be JavaScript Object and Array Notation, but that would make it JSOAN, but it's not. JSON, J-S-O-N. And so the idea was is that you have this way in JavaScript of just saying, you know, and in Python, we do the same thing. We can say x equals curly brace blah colon blah and you can make yourself a dictionary. And so this is like making an object in JavaScript, which is the same as making a dictionary in Python. And they can be nested so you can have an object that has a key that the value of which is a list or whatever. And so Douglas Crockford said, let's just use this as our serialization format. It's nice. It'll be nice to pretend that in 2000 Python was part of the reason he picked it. He picked it because it was JavaScript. And it just so happens that Python looks pretty much identical to it.
Play video starting at :6:58 and follow transcript6:58
JSON is a little more restrictive than either dictionaries or JavaScript code. JavaScript is actually looser. So what he did was he basically came up with a subset of the JavaScript syntax, the constant syntax format, that was a little easier for serialization deserialization, because he really knew that we were all going to have to build a bunch of libraries and then those libraries for Java and for PHP. JavaScript kind of already existed, although by now we've actually built libraries instead of just like executing JavaScript, which was kind of dangerous. The earliest parsers were you just ran the code which is a little scary actually, but now we actually parse it. So, he knew that we were going to build all these back-end languages like PHP and Java and ASP and we had to teach them JSON, make really cool JSON libraries. So in Python, you just say import json and then, poof, your serialization, your load s and dump s and doing serialization and deserialization, and it's awesome. But he basically said we'd better not make it as flexible as JavaScript because JavaScript allowed constants and variables and all kinds of substitution. And so he was just like look, it's got all be strings etc., etc., etc. So he put up a website called www.json.org. It's kind of fun to look at, it's crude, it's simple, but it's precise and it was a way that allowed people to carefully develop. It's very well thought out. Everything Doug Crockford does is really smart. And so it allowed people to start building libraries and testing those libraries. And if one library like the PHP library and the Java library disagreed with one another, then you could at least go to json.org. And so it really kind of brought some order to this general notion of using JavaScript constants as a serialization format.
Play video starting at :8:48 and follow transcript8:48
And like I said, we can't claim that Python was too particularly influential. But it's really nice that Python and JavaScript are the two languages that are probably the most, going to be the widely most widely known languages and they look the same. And JSON looks like Python, it looks like JavaScript, and it looks like JSON, so that's kind of nice. So ultimately, JSON became very dominant. Later things like JavaScript in the server with NodeJS made it so that JSON was even more natural there and then databases that are based on JSON started to emerge like MongoDB. So if you had JavaScript in the client, and Node in the server, and then a MongoDB in the back end, you were basically doing JavaScript everywhere and JSON everywhere. And it's really, really pretty.
Play video starting at :9:36 and follow transcript9:36
It turns out that it's still difficult to write applications in that environment. But it was nice because you were using one language and one serialization format from the database on back. And so we'll talk a little bit about the emergence of these NoSQL databases, which was really a code word for all intents and purposes for JSON databases for many situations. But then what happens is that as these NoSQL databases became popular, things like Postgres, MYSQL, and other databases are like, you know, we could just add a JSON column and kind of do what you fancy brand new databases that are nowhere near as mature as us. And so in an upcoming discussion, we will talk about some of the what's going on between the NoSQL and the SQL movement, and how you can somewhat get the best of both worlds in a current generation relational database like Postgres. [MUSIC]

# Interview: Douglas Crockford - JSON
So JSON is the world's best-loved data interchange format.
Play video starting at ::49 and follow transcript0:49
It, I discovered it in 2001.
Play video starting at ::55 and follow transcript0:55
I don't claim to have invented it, because it already existed in nature. I just saw it and recognized the value of it, gave it a name and a description, and showed its benefits. But I did not invent it. I don't claim to be the first person to have discovered it. There were other people who I later found out had come with, along the same idea in 2000. The earliest instance I found of JavaScript being used as a data interchange format was at Netscape in 1996, so it's a, an idea that's been around for a while. And if you look at other data representations like the property lists that were used at, at NeXT and then later at Apple, except for a couple of cosmetic changes, it's the JSON notation as well. So it seems like it's an inevitable sort of representation for data, at least data that is intended to be consumed by programming languages. And ultimately, that's all data. I started with JavaScript. But my first application was facilitating communication between programs written in JavaScript and servers written in Java.
Play video starting at :2: and follow transcript2:00
So I recognized that even though it was born out of JavaScript, it could be and should be language independent.
Play video starting at :2:6 and follow transcript2:06
So I simplified it as much as possible, took as much out, tried to make the simplest possible specification for how to structure data and put it on the wire. And that ultimately became called JSON. [MUSIC]
Play video starting at :2:29 and follow transcript2:29
In 2001, I was in a company I had started called State Software. And we'd developed a platform for doing applications which could be delivered through unmodified web browsers, what today is called AJAX. But in 2001 that was kind of a radical idea, and not many people would believe that was even possible or, if it were, that was a good idea.
Play video starting at :2:53 and follow transcript2:53
But we produced some brilliant demonstrations and we were starting to make some progress in trying to convince, you know, potential customers that they should adopt the style of application and development.
Play video starting at :3:4 and follow transcript3:04
And as part of the description, we'd say and then we use this JSON idea for communicating the stuff back and forth. And they'd say JSON, what's that? And we'd say it's this thing we found in JavaScript and it's really great. And they'd say oh, we can't use that, we just committed to XML. So, no we can't. I'd say, but XML was wrong for all of these reasons, it's, it's hugely expensive, it's much harder to use, and all of that.
Play video starting at :3:28 and follow transcript3:28
Well, we can't use that thing you did because it's not a standard. I said, it is a standard, it's a proper subset of ECMA-262, which is a standard. They said no, that's not a standard. So I decided if I want to be able to use this thing, I need to make it a standard. So I bought json.org, and put up a web page, and sort of declared it's a standard.
Play video starting at :3:52 and follow transcript3:52
That's it, that's all I did. I didn't go around trying to convince industry and government and, and everybody that this is what they should do. I just put up a website. Basically, a one-page website. And over the years, people discovered it and realized, oh yeah, this is so much easier. I'm just going to do that. [MUSIC]
Play video starting at :4:14 and follow transcript4:14
The thing I never understood about XML for data interchange, ok, so basic, generally the pattern is, you've got a query. You send it to the server. It gives it to the database. And you get back this XML thing. Then you have to send queries to that in order to get the data out of it. And I said, why can't you just give it to me in a form where I know what it is and I can use it immediately, and so that was the main benefit of JSON, I think. It wasn't that curly braces are so much better than angle brackets. I mean, ultimately none of that matters. The thing that mattered was that the data structures that JSON likes to represent are exactly the same data structures that programming languages represent. You know, when AJAX was formulated, the X in AJAX was supposed to be for XML. And the smart kids right away realized, oh this is too hard. We, we don't want to be doing XML here. And some of them discovered, hey, you can use JSON here instead and it's so much easier. So much faster.
Play video starting at :5:11 and follow transcript5:11
So they started doing that. And for a while there was a debate, you know, where some people were arguing
Play video starting at :5:17 and follow transcript5:17
Jesse James Garrett said the X stands for XML, so you can't use anything but XML. That didn't last very long. There were a number of other alternatives to XML that were being considered around those times, but JSON was the only one that was designed specifically for AJAX. The, probably the boldest design decision I made in designing JSON was not to put a version number on it, so there is no mechanism for revising it. So, JSON, we're stuck with it. Whatever it is in its current form, that's it. And that turns out to be its best feature. And because it wants to be a low-level thing and it's, it's basic infrastructure and it's the thing that you pile everything else on. You know, it's sort of the equivalent of alphabet in a language. I mean, we might make up lots of words and lots of ways of having sentences, but it's very uncommon to make up new letters. And that's sort of the place where JSON lives. So it's good that it's not going to change. I, I expect maybe someday we'll find that there are really important things that JSON doesn't do, like cyclical structures. Graphs are not easily represented in JSON. They can be, but it requires a level of indirection, a little bit more work. Some day we might decide we don't want to do that work and then we replace JSON with something else. We will not extend JSON to do that, we'll replace JSON. And even after we do that replacement, everything that was ever developed that still uses JSON will still work, because JSON will never change.

# Lecture: Python and JSON
So now I want to talk a little bit about using JSON in Python. We're really lucky in that sort of JSON has been in Python for a long time and it has a really beautiful support for Python Python has really beautiful support for JSON and the data models that are JSON are very similar to the data models in Python. Other languages like say Java or PHP, their mapping is fine, but it's not as, the Java one is probably the most difficult one because Java is a very highly structured, highly typed. Python is a less structured language, because it uses lists and dictionaries over and over and over and that's pretty much what JSON does. So we have a JSON library, we've got a couple of bits of sample code right here like www.pg4e.com/code/json1.py. This little bit of code shows how you actually create an internal data structure and then serialize it. Remember you've got an internal data structure, you serialize it, you send it, you receive it, you deserialize it, and have you have a new internal structure internal data structure. So I'll show you all those. This json1.py, we import the json library and then we start making
Play video starting at :1:15 and follow transcript1:15
a dictionary, a variable named data, and we say data sub name equals Chuck. And then we say data sub phone equals a new dictionary because we're going to put a dictionary inside of a dictionary. And then we're going to say data sub phone sub type equals intl, data sub phone sub number equals a phone number. Then data sub email equals a dictionary, and then data sub email sub hide. So that's like a dictionary within a dictionary and a key within a key of a dictionary. And then we're going to serialize it. So dump s stands for dump this out as a string and the first parameter is either a list or a dictionary, and best practice is at top levels to be a dictionary. indent equals 4 is a pretty printing that says, you know, don't just run it out all in one line because the computers wouldn't care, if we're going to show it to humans, put it out with an indentation and show the matching, you know, do indentation and make it look pretty. So this produces the following output, right? We have an outer dictionary, and then under the name, the key that's name, we've got Chuck. Under the key that's phone we have another dictionary and that dictionary itself has type and number. And then email has a single value of hide equals yes. So that's as simple, you simply construct whatever data shape you want
Play video starting at :2:29 and follow transcript2:29
inside Python of lists and dictionaries with whatever nesting you want, and then you just say, convert that to JSON and send it out. So that's pretty beautiful.
Play video starting at :2:39 and follow transcript2:39
So you send out the serialized data, and then you receive it and then deserialize it and json library has a similarly simple load s. And so this is from pg4e.com/code/json2.py. So we're going to import the JSON. Now in this case, instead of reading it from a network or opening a file or whatever, I'm just going to make a big long triple-quoted string of that exact same JSON, as if we somehow received it. In this case, I'm having Python be on both the sending and the receiving end of this. But then data is a string, and
Play video starting at :3:17 and follow transcript3:17
then I call json.loads and pass the string data in. Now this could blow up if there's a syntax error because JSON has a syntax you can, like if you have a single quote that is supposed to be a double quote that should be, you use a single quote instead of a double quote, you can make so many mistakes in it. And so this could blow up and so you might have to try put that in a try block, print out an error message, or whatever. But when it's all said and done, info, this variable, comes back and given that this outer thing is a curly brace, we get a Python dictionary. And then we have dictionary sub name, we have dictionary sub info sub email sub hide. So we can just look these things up. And so it's really quite natural and quite nice. You just see this serialized, nested structure, you deserialize it, and then it's just an internal structure that's really easy for you to use. You can also serialize lists. The best practice is to make objects, but there's json3.py and json4.py. And you can look at those, they look almost exactly the same as those other ones. We put a for loop in, of course, because it's a list.
Play video starting at :4:28 and follow transcript4:28
Now if we take a look at doing this in XML with www.pg4e.com/code/xml1.py, you would read some serialized XML data and there is a built-in library in Python to parse XML. And so say take a tree from string. Now the key thing that's kind of weird about XML is it can be nested infinitely deep, right? And again, it's self-describing, which is quite nice. So we know what a person is, we know that, things like that. And so you'd call tree.find. And if you watch the Douglas Crockford interview he talks about how you end up with a tree that you then send queries to. Well, that's what we're doing here. We're actually like querying the tree. The tree is not a natural internal Python data structure like a list or a dictionary, it's a object that we have to send queries to. And those queries kind of work their way through, etc. And the key is when you do JSON, when you do JSON, you get back a native Python dictionary or list. And so that's, again, this is kind of why XML is not preferred when you're just sending lists or dictionaries back and forth between various languages. Now that is not to say that XML doesn't have its place, I mean, Microsoft Word has documents that are DOCX, and that's an XML format. And you have like chapters and sections and headers and lists and paragraphs and sub-paragraphs and figures and that's all very structured data, very hierarchical. And so XML is a great format for that. And XML and HTML are very similar and HTML is also a very hierarchical document because it's trying to represent and mark up text to produce a pretty looking document. So I don't want to beat up too much on XML. I mean, JSON is superior for those things where we're exchanging lists and dictionaries with different programming languages or across the network. XML has its place as well. So up next we'll do some demonstrations with some code that uses JSON.

# Lecture: PostgreSQL and JSON
So now that we've talked a bit about JSON and JSON inside Python, we're going to talk a little bit about how we store structured data inside Postgres. So PostgresSQL support for JSON has evolved over time. I kind of mentioned, and we'll talk a little more later, about how it is somewhat a reaction to the emergence of JSON-based NoSQL databases, and everyone seemed to like JSON and liked using JSON inside databases. And traditional relational databases and Postgres before 9.4 didn't support that. And so people were like well, you know, I want to use JSON, so I'm just going to leave Postgres. Now Postgres has a lot to like and a lot going for it. But the question was, was the market going to all shift to NoSQL databases or were the relational databases going to add support for JSON. And so you can see the support for JSON sort of coming into Postgres sort of slowly, which I think is an excellent engineering strategy where they would put in things like H-Store and then the simpler JSON format, and then eventually the JSONB format, which is what we pretty much use today. So there are three supported column types, and it looks a little weird to have all three of these column types. But if you think of it from a historical evolutionary perspective, it's not really nearly as weird. So the H-Store column is sort of a precursor to all this, and I think it's a beautiful, beautiful structure. In a sense, it's like saying I'm going to put a dictionary in a column, just a bunch of key-value pairs. And later we'll talk about the idea of, do you really model every single thing in your UI in another column in the database? Or do you sometimes just kind of throw them together in one column because you're not querying, etc. And H-Store is an excellent way to take 40 columns and collapse, 40 columns that don't really need WHERE clauses, but you can actually put WHERE clauses in H-Stores too. So it's a way to also have an expandable schema. So you can throw a new column in there, a new data element in there, without actually alter tabling and adding a new column. And that's not to say that everything should use H-Store or that you should never use H-Store, but there are times when it's really nice to just have a schema-less little corner of each of your rows. So H-Store looks like a dictionary, a maps to 1, b maps to 2, and that's in one column. It's like of like an array. So Postgres has an array column and it also has a key-value column. And so that maps very well to a list, a dictionary, an array, an object, and all this sort of linear and key-value structures. So H-Store is great. It's kind of a good complement to arrays. But you can't do nesting in them, right? So you've got one set of values and that's it. You could name your keys with XYZ something something, you could get a naming convention, but you're not allowed to go deeper. So the first JSON that was in Postgres 9.3 and earlier was really a glorified text field. And here are things you can do with text fields and I'll show you in some of the demonstrations. Some of the demonstration you've already seen, where I'm doing things like digging into a text field using a regular expression and building an index on that and then using a WHERE clause with that same regular expression and having it do an awesome lookup. It's amazing, right? And so at some point think of the JSON as just text, and then you do sort of regular expression work inside that. And then you build some functions to make some of that regular expression a little bit easier, etc., etc., etc. And then you have things like the Generalized Inverted Index, the GIN, and you teach it a little bit about JSON, these text JSON fields, and away you go. And that's text. So I could see how you'd put that in an early version because you can get that in earlier and quicker. And then the JSONB, which is the really cool column type, it's not a text field. It parses it, it knows that it's JSON. It keeps the key-value pairs very nice and very dense. I can just imagine all the cool optimizations that it can use storing JSON when it knows it's JSON. It knows what the rules are, and perhaps it even knows that there's a lot of similarity between some of this JSON. So the JSONB in many ways has the advantages of the indexing and looking up and the dense storage of the H-Store, as well as the flexibility and the nesting and the coolness in general of JSON when you want to do the stuff that's sent across networks in JSON, etc. You can read up on the Internet whether you should be using H-Store, JSON, or JSONB. But I would just say when in doubt use JSONB because that's where the market is going to expect that Postgres will have to invest more. And so if you want to use something that you know is going to be sort of focused on by the performance of JSONB is critical to the future of Postgres. The performance of H-Store is not nearly as critical, because with JSONB, Postgres is competing with things like MongoDB and you see blog posts like the ones I've got, where people are like I tried Mongo and I quit, and I went back to Postgres and that makes the Postgres people really happy and JSONB is the key thing that makes that happen. So now that we've talked about the things that are not JSONB and a bit of the history of JSONB, went too far. Let's actually talk about the JSONB. I'm not going to talk about H-Store much more or JSON. And so I will do a nice walkthrough of this, but basically what we're doing in this one is I'm going back to my iTunes export. It came out as XML data, I converted it to CSV in an earlier part of the class, and now I'm going to convert it to JSON and we're going to load that all up in a JSON to kind of play with our first real JSON database. And so we're going to make a table that just has got a primary key, id primary key, and a body that's of type JSONB. And we've got a clever little copy command, that clever little copy command is reading a file of JSON that every line is itself a JSON object. We're reading it as CSV but it's kind of tricky. We're telling it the quote character is something non-printable and non-existent and the delimiter character is something non-printable and non-existent, which means every line becomes a column, basically. So it doesn't matter because we're inserting it into body, we're only looking for one column and so it's reading line, line, line, line, and becomes row, row, row, row, row with that column, except that it's JSONB. So that means as this is being parsed and put in and if there was a type, a mistake in the JSON or the JSON wasn't pretty, this thing would blow up and say bad JSON. And believe me, I've done this. As I was making it all work, I blew it up a couple of times. So the copy command loads in all that stuff, and then we are going to play with some of the operators. The coolest operator is I call it the double arrow. It's dash, greater than, greater than. What that basically says is when we see SELECT body arrow, arrow, count, and the count has to be in quotes because the keys in this JSON are strings, right? The count is a string. So you've got to to look up the thing inside this JSON body, under the key count. And then this colon, colon int that says convert it to an integer. And we can do this in a WHERE clause where the body arrow arrow name is Summer Nights. So that's basically saying select the count, the number of plays from jtrack where the name of the track is Summer Nights. The other thing you can do in a WHERE clause is use the contains operator, the at sign greater than operator. This is particular to JSONB. This equal operator for the body name Summer Nights, that was a string comparison, actually. Because we retrieved the name and then converted it to a string. The double arrow converts it to a string. The single arrow body with one arrow, one greater than, that would convert it to JSONB, and that would make that equality not work. But what you have on both sides of the at sign greater than is in effect a JSONB document. And so it's really an intersection. It's saying does body intersect with the tiny little JSON fragment of name Summer Nights. So the key name goes to Summer Nights. And so it looks through all the whole JSON and it looks to see if it overlaps. Is there a mapping, does it overlap. Where there's an overlap, that's why it's contains. Does body contain name Summer Nights? And so that's what the at sign greater than is. And then the question mark actually simply says, does this JSONB body contain a particular key? And you can use these things in ORDER BY like SELECT body double arrow name AS name FROM jtrack ORDER BY body double arrow count cast to integer, and then descending. So that's basically the major fun operators to play with that we have. We will create some indexes. We don't have to use GIN indexes. You can also just use B-tree indexes. So in the situation I can say, look, I would like you to create an index. I want you to look inside the JSON and I want you to make an index for body double arrow name. So that's no different in a way than a column. You could actually pull that out and make a column called name and then index on it, or you can just index right in the SQL. And so I think that's really beautiful. And you don't have to just have one of those things. And so it means that WHERE clauses, like this WHERE clause, would be optimized and use the index to speed themselves up. We can have a GIN, the Generalized Inverted Index, on the body and that really looks for the key values, and that speeds up things like this question mark operator. And then if we do the more richer inverted index and we do jsonb_path_ops, that looks at all of the key-value pairs, and gives you an inverted index of the key and the value. And that's what speeds up things like this WHERE body contains name Summer Nights. And so these indexes, we'll play with those indexes and create those indexes and then run queries and watch their performance based on those indexes that we create. And so Postgres really understands the JSONB. It's very efficient. It's very fast. And it's a critical competitive element of Postgres in the market going forward.

# Demonstration: Music Tracks and JSON
Hello and welcome to another Postgres walkthrough. In this situation, we're going to start talking just about JSONB in Postgres. Everything we've been doing is leading up to this. And so we're going to take a look at my original from many years ago export of my library in iTunes and I've converted it into JSON, and so it's got curly braces and double quotes and colons. And I've got one record per line in here. That's the format to do really simple imports for really simple JSON. So I'm going to go in and I'm going to drop this table if this is not going to exist, but it's not there. So we're fine. I'm going to drop the jtrack table then I'm going to create the jtrack table with a id column and a JSONB body. There we go, we're just like that body is JSONB and B stands for better but I like to think of it as binary because it's compressed and it's efficient and the indexes are like really intricate, it knows a lot. It's not just a big text field. The old JSON was, but JSONB is not just a text field. So the next thing we're going to do is we're going to copy down on a terminal in a terminal, this jstxt file and that was not found, but here we go. So if I take a look at this library.jstxt file, you see that it's got this JSON in it, one per line. Now what we're going to do is we're going to pull this in with a \copy command. Now the copy command is only to this pgsql and so we're going to copy into the jtrack table into the body column from the file library.jstxt as a CSV, now it's not CSV, where the quotes are an irrelevant non-printing character and the delimiter is an irrelevant non-printing character. So Ex01 is actually is ASCII number 1 and Ex02 is ASCII number 2 and neither of those characters are in because really the trick is we're only going to pull one column in and we want the whole line, and we want CSV processing, but then we're breaking CSV processing by telling it the quote is a thing that doesn't exist, and the delimiter is a thing that doesn't exist. So the dear CSV, treat each line as just a thing. Don't parse it at all, because we're only inserting one thing anyway. I wish there was a way to just tell it's like it's lines, insert lines. We've done this before in previous things. It's kind of like if Postgres would just let us say insert this thing as lines, because often that's it. And so we just inserted that. So we can say SELECT COUNT star FROM jtrack. Three hundred eighteen records, there was 318 lines in there. So there we go, okay? So let's take a look at what we've got. Let's look at some selections. So we can see, we can pull this out. When it shows us this stuff, it reconstitutes it. So this in a sense is not that string that we put in. It's been parsed, it's been stored, and now it's reconstituted. And we can kind of tell that it's a little bit different because we can say what kind of thing is this using the pg_typeof function. It is I forget the semicolon. It's a JSONB, right? It's not really a text string, and that's a string. It just happens to be showing it to us as a text representation of that data structure. We can pull one thing out with this body dash arrow arrow. Greater than greater than.
Play video starting at :3:52 and follow transcript3:52
The key thing to remember is the thing that comes afterwards all's got to have quotes because the keys are quotes in JSON, and so we're going to pull that out. Now you can pull out one, and we'll see in a second, you can pull out one or both. This is the one you tend to want to use over and over and over again with two arrows because that says and convert it to a text field. So that's the kind of SELECT statement. Boom, now we got that. But we can see if you only have one arrow, we can use typeof body dash greater than name, and we'll see what comes back is actually a JSONB. And so JSONB are both documents and fragments of documents, and so this body name is really a tiny little document. And then if we want to, we can convert this to text. But the problem is if we do it this way, you'll see it's not going to work. If we say body name colon colon text, we get a jsonb and that's because the association for the parentheses, for the colon colon, it's a higher precedence operator and so it happens before the arrow operator happens. And so you've got to put parentheses around it.
Play video starting at :5:16 and follow transcript5:16
Oh, this one doesn't work either because we're going to do the typeof. So if we do this parentheses, it's going to do this and then convert the result of this function. So that means that the function is going to be converted to text. So that's going to break just as bad, it's still the word jsonb so it just means that you do sometimes have to add extra parentheses so that it actually pulls out the name, and that is a JSONB object. And then we do a cast into text and that's really what you want. So we get the text. But why are we even doing that? Because if you just do double arrows, it says get it and turn it into text. Right? And so that's really nice. I like explaining in gory detail some of these syntaxes because we just throw them out there and we don't know why. So this is another example, SELECT MAX of body count. This is body double arrow count is a string, but then we're going to convert it to integer. Now you've got to be careful because the MAX will work without converting it into an integer, but it'll be a string max which is like sorting, which means 99 is greater than 1,000 because nine is greater than one. So when you're doing numeric things, you got to be careful to cast them to integers. So there we go. So that actually gets the integer. If you take out the cast at the end, you will see that you'd get 93, which is nowhere near the numeric maximum, but it is that letter nine, that's like a letter, might as well be Z, for all we care. And so this means that body arrow arrow count is a string, so that's the max string from a correlating perspective. from a sorting perspective, from an ORDER BY perspective. So if we're going to order by something, we're going to this body count cast as an integer, we'll do a SELECT body name AS name FROM jtrack ORDER BY body count cast as an integer and then we're going to descend it LIMIT 5. So we can see my most five popular pieces of music that I played. And that's because this is some really mellow music that I play in my car all the time. So it gets a really high play count. And so even though count is an integer in JSON, you still need to pull it back that way because it comes back. This first one comes back, body arrow count comes back as a JSON object and then body arrow arrow count comes back as a text. So you've still got to cast it because the double arrow makes it a text. You see that the single error makes it a JSONB fragment and then the double arror makes it a text and then you have to say colon, colon int one more time. And you've got to get your parentheses right. So it just is a little bit tricky, okay? So let's take a look at a WHERE clause. We're going to count the number of tracks that have a name of Summer Nights. And because it's a body arrow arrow name equals Summer Nights, that says convert it to a text string for me which again most of the time you'd just use arrow, arrow and we see that I have one track. We can also say that this is a JSONB operator. So that equal operator is a string operator. But the JSONB operator, the at greater than, says "Does this body contain this JSON fragment?" Now it's going to convert this. There's an implied it's smart enough to, there's an implied colon colon jsonb on the end of this. Let me actually go just put that in there. It's kind of this is implied because that is a text string here.
Play video starting at :9:17 and follow transcript9:17
So that's implied. Let's even put a parentheses in to make it super explicit for us. But it doesn't need that because it knows that if it sees a string as one of the parameters of the at sign greater than, to convert it to JSONB. So this second version here with a parentheses and colon colon jsonb is really just, it's being explicit of something that's done implicitly, right? So these two things and we'll see when we start doing indexes that these two things give us the same thing, but this one is a string comparison and one is a JSON comparison with the contains. So then what we're going to do is we're going to show how to use this concatenation operator. We've used this concatenation operator to concatenate strings but now what we're going to do is use it to concatenate JSONB. And so the body is a JSONB and we're going to add favorite equals yes. Now, this is a string. It's just a string, and inside the string is JSON. It's got to be syntactically correct JSON "favorite" "colon" "Yes" and it's implied, there's an implied conversion to JSONB because it's being concatenated in. And again, the concatenation operator says, "Oh, if it's a string I'll convert it to JSONB if the left-hand side is JSONB". And so that's going to work. And so we're going to look up the body. We're going to look at the count variable, convert it to an integer. If there's more than 200, we're going to go ahead and get that. So this is basically going to update the body, right? So it's going to update the body. Let's do this first and take a look at some of these things. So here we are. So what we're doing is we're going to read and find the count and then we're going to add at the end a new key. So that's what this update is going to do. So we take the JSONB, it actually pulls it out of the database, sort of reconstitutes it, adds this little thing to it, and then stores it back. And that's what an update does, right? It pulls something out of the database like update x equals x plus one. We've done that many times before. So it found 33 rows that the body count is greater than 200 and it added favorite yes. Now, if we run this greater than 160, we should see that some of these now have favorite yes. Now, again it's not a string, it's JSONB. But not all of them, right? So this first one here Winter Wonderland has a count of less than 200, and so it never got favorite. So now what we have is we have some that have favorite. We could also could have written a Python program to tweak this JSON and the fact that we're just, this is very simple. But I'm showing you how this concatenation operator works, right? And so we see some with and without the favorite but then it gives us a wonderful time to play with this question mark operator. So SELECT count FROM jtrack WHERE body question mark favorite. That's just saying show me, select the things where the body has the tag favorite, I mean a key of favorite. So that's not saying a key-value pair. That's just a key. So how many do I have? There's 33 that have favorites equals anything. I'm not asking what it's equal to. I'm just saying is favorite in that JSON and there's 33 of them, okay?
Play video starting at :13: and follow transcript13:00
So let's throw a large amount of crud into our jtrack just to make sure the indexes work. So I'm going to put 4000 lines of unrelated JSONB junk. So we're going to select this parentheses this is a string concatenation. We're going to concatenate some type colon "Neon", "series" "24 Hours of Lemons" "number" colon That's a string, that's just a string. This a string concatenation, the vertical part here. And then generate_series, if you recall from our previous lectures, creates a vertical replication of rows generating the number 1000, 1001, 1000 all the way up to 5000. Then I'm concatenating with a close curly brace. So that's going to, if you were to just look at this thing right here, that would just be a look like JSON. It would be valid JSON and carefully constructed to be valid JSON and then I take that concatenated string and then I cast it as JSONB so that I match the type of body. And so let's go ahead and insert all those. Insert 4000 more rows. Sometimes you have to insert a bunch of extra rows or your indexes just, you'll do an EXPLAIN ANALYZE and you should have used the index and it's like, well, there's only 12 rows, I might not use the index. And I cannot for the life of me figure out how to convince Postgres to either tell me I would have used the index if there were more rows or a little thing that says pretend there's a lot of rows in it because I'm really trying to debug my index. But you see when you look online at blog posts and stuff, you see that they just put a lot of rows in there. And so I've just got to figure out how to generate series and put some more rows in. So we're going to have to make some indexes. These indexes aren't there because I just built it but we'll drop them anyways. So let's go ahead and create three indexes. I'll create them. I'll fire them off, create them all, and then we will see. Sometimes you've got to wait. So I'm going to explain. We'll let Postgres in this tab grind away and make sure those indexes are ready, So our explains start working. So I'm going to use three indexes. One is a B-tree index and in that my expression is to pull out and convert to a string the name key, right? So pull out the name, not the fact that the name exists, but the fact that name equals the name of the track. So like Summer Nights. So this B-tree is going to hit that WHERE clause where body name equals Summer Nights. So that's a B-tree index. And so this is more of a traditional old-style index. B-trees want to index, in a sense, a field. One string is what they want, or a combination of strings. You can put more than one in there. And so the key is that this B-tree will hit body name equals Summer Nights, but in a query like body artist equals Queen, that will not hit it because we did not make an index on body artist. We could make another index on body artist if we want, and then this would work. Now the key is, the operators of the WHERE clause are critical. And so the question mark and the at sign deal with the GIN indexes. The equal sign is a string comparison. In this respect, it's not at all a JSONB operator. It's the data stored in JSONB, but the WHERE clause is reading the JSONB, pulling out the artist or name, and then doing a string comparison. And so that's why it hits the B-tree. There's just no way that these last four would hit the B-tree. and there's no way that the first four would hit the GIN, because equal is not a GIN operator. Okay, so that's the B-tree. I should stop here and talk about like how gorgeously beautiful this is. You can have a JSONB and you can dive in and you can make a beautiful B-tree index. You can put a logical key deep inside of JSON if that's what you want. And it for all intents and purposes is going to be super fast, as fast as even if you pulled that name out and made another column as you've seen me do in a couple of other places. So that's like awesome, okay? That's easy and awesome. You got to know how to construct your WHERE clauses to make use of that particular thing, but that's like any other query. Okay, so let's take a look at the GIN one. So CREATE INDEX jtrack_gin ON jtrack USING gin body. Now, we're going to do a GIN of the whole body. Now, the default is that what this is going to click is this is going to figure out whether or not this is going to remember the keys, not the key-value pairs, okay? And so it'll click this body question mark favorite, which is like, "Is that key there?" But then to get the things like name Summer Nights or artist Queen or name Folsom Prison Blues and artist equals Johnny Cash, which is what we're doing in these last three, we're going to make another index that's asking for the path_ops. And so the difference between a GIN on the body and a GIN with JSONB path objects on path_ops on the body is that it looks at key-value pairs and is super fast. Now, of course, the jsonb_path_ops is bigger and slower and harder to maintain, but it does hit on more of the WHERE clauses. So the GIN and the GIN path_ops hit these question mark operator and does body JSON contain this little bit of JSON, right? So let's just take a look at hopefully which of these things hit and which of these things don't. Now, you might think it'd be nice if because we've got a GIN with a json_path_ops that it would figure out, "Oh, name equals Summer Nights," and in effect convert that to this body at greater than name colon Summer Nights, but it doesn't. But it could. Some future version of Postgres might decide to make the equal sign operator smarter when it's dealing with JSONB, but who knows. We can't expect that. So let's take a look at these one at a time, see if our indexes are done. No. Oh, yeah. Okay. I freaked out because I saw a sequential scan. That's what you're supposed to do whenever you do an EXPLAIN ANALYZE. But this is body artist. I did not make. I made it on body name, and so that's supposed to be sequential. Now, if we name Summer Nights, this is going to use the B-tree index, and there you go. It hit the B-tree index because that is the thing that I made the B-tree index on, is body arrow arrow name, and so it hit just fine. And again, I think of this as just as efficient as if I had made a column, right?
Play video starting at :20:30 and follow transcript20:30
So if we ask, look for the ones where the word favorite, where there's a favorite key, not any particular key-value pair, that's going to successfully hit the jtrack GIN because the GIN says what keys are what. I don't know, I tend to feel like if I was doing this, I can't figure out how often I'm going to want this unless I have some really diverse documents that have widely differing syntax and I've got some kind of a thing where I just have an indicator that says, you know, blah, blah, blah equals blah, blah, blah, you know, and I'm just like, "It's a marker," which is what this favorite yes is. It's kind of like a marker. So just the presence of it. Again, the GIN is faster and smaller than the GIN with the path_ops. Now, we're going to have some fun. So now we're going to say go find me the name Summer Nights, and now we're going to be using the GIN path_ops, right? So we're doing name. So does this body contain this little JSONB fragment name Summer Nights? And the answer is, "We'll find it," and this is going to find it. And that means that there's an index that's smart enough to look at nothing but this and pick the rows and then only retrieve those rows. Now, because we did it, it doesn't really care which of the keys we're looking at. We can say artist Queen, because it's got that one too, right? So it did all of them. Unlike the B-tree where we picked which one it did, the GIN path_ops just picks them all, just all the combinations, and then we can say, and read this one carefully, SELECT COUNT star FROM jtrack WHERE body contains "name" "Folsom Prison Blues" "artist" "Johnny Cash". This is like an AND because we're saying, is this entire subset of JSON, name, Folsom Prison Blues, artist, Johnny Cash, is that contained within the body JSON? Got it? So that's like an AND operation. Okay?
Play video starting at :22:41 and follow transcript22:41
And that one hits the JSON path_ops, and it works just fine. So that is sort of really beautiful, really impressive like most things in JSON, in Postgres. Once you get down to doing the work and you know what to do, it works out pretty cool. And so this last little bit,
Play video starting at :23:6 and follow transcript23:06
we're going to do an update. This is just kind of an advanced thing. So let's say we're going to take, we want to add 1 to a count, right? So we want to take this count somewhere here, count 55, and we want to add 1 to that. It is an integer and it knows it's an integer, but we've got to get stuff a little tricky in this. It kind of gets you to the point where you have to construct this stuff, and it takes a while to figure out. And that's why when I figure this out, I write it down. So let's start by saying SELECT body count jtrack plus 1 because we'd like to be able to do that because it's an integer, right? And hey, give me this thing as an integer, that would be my instinct. I'd like to be able to use that syntax, and the answer is, sorry, you have given me a JSONB and an integer, and I'm not going to allow you to add those things together. So that's where you've got to cast it. So cast the body count, cast colon colon int. Now, we can actually calculate the updated. So now it was 55 and now it's 56. Let's just select it so you can see what it is without the plus 1. So it was 55. And so now if we add 1 to it, cast it to an integer and add 1 to it, we're going to get 56.
Play video starting at :24:33 and follow transcript24:33
So we can do a select here and add 1 to it for the Summer Nights, and we're going to add 1. So we can also take the plus 1 away and we see what the old one was. So we can put a WHERE clause on here. That's just putting a WHERE clause. So 35 going up to 36. Now, this is the mess. This part here is the mess. UPDATE jtrack SET body equals jsonb_set. So what this is doing is it's actually, I would love for this to be some kind of concatenation or something a little prettier, but they put this into a function. And so what it does is it takes the old JSONB and it takes a path, which means go find the count thing. That's a really weird syntax, but it's the right syntax. And then take the body count, convert it to an integer, add 1 to it, then convert it to text, and then convert it to JSONB. I don't like this syntax. I think there's a more elegant way to do it. And perhaps in a future version of Postgres, they will make this more elegant. And then I can throw a WHERE clause on this. So UPDATE jtrack SET body equals. So this is retrieving the entire body, tweaking it with this function, and then storing it. Now, the hard part is really retrieving and then storing this new data, so it's not all that bad. It's just I don't like the syntax of it, it's a little bit hard. I have it here in case you want to do it in the future. For me, I would be like, no, I think I'll just make a column. I'll just pull it into its own column, and then mess with it there. Okay? So now that we're all done with this, I'm just going to clean things up. I like to drop my tables, and so I'll DROP TABLE jtrack CASCADE because I'm using a Postgres server that has a limited amount of memory. So I hope that this sort of walkthrough of the general use of JSONB has been useful to you, how the operators work, how the casting works, etc.

# Lecture: Using a JSON API
So the next sample code that we're going to look at is a bunch of code that is going to actually pull JSON from an online API. In particular, the Star Wars API. Which is a fun little thing that has data about Star Wars films and characters and kinds of creatures, etc. And so we're going to do this in a way that's a little more complex because we're going to spider. And we actually don't know what data we're looking at. It's not like the email data where we're going 1 2 3 4 5, we're actually going to retrieve a document, then you read the document to see links to other documents. And then you retrieve those links and you put them in our database as to be retrieved. So this is an application that maintains a to-do list of URLs to retrieve. That's kind of the spider web crawler aspect. So the table that it's using is a little more complex. We certainly have a primary key and some JSON. We have the URL we've retrieved, we have the status of it. Whether or not it's been retrieved or not. And then we have some created_at and updated_at stuff. And so like I said, this works like Google search, in that it looks to, it find URLs and eventually it retrieves them all, which is a few hundred. So when we run this program, it starts by inserting a few known URLs as the starting points. And then it goes and grabs 10 more documents. And you can see the 200 is the fact it's successful. And then it retrieved 2200 characters and it counts the to-do list at end of each run. So there was only 2 in this to-do list. Now there's 34, because it found some more. And the to-do list goes up and then as it retrieves things it goes down, goes up and it goes down. And eventually, you know, this loaded 10 documents and ultimately it had 35 more to go, but we can stop. This database persists from some run to run. And at that point you can take a look, in another window you can look at your Postgres with your psql client. You can see where it's at, because again this is just Python is one client and psql is another client.
Play video starting at :2:16 and follow transcript2:16
So then you can start it back up. And in the beginning it says, yep, I'm back. I've still got this database. I've got 35 to go, give me five more, five more, five more, five more. And eventually, there's about 200 plus documents. And then when you get loaded, we will start playing with that retrieved JSON by using SQL, we'll make some indexes, and review a whole bunch of ways to use

# Demonstration: Star Wars API 1/2
ello, and welcome to another demonstration walkthrough for Postgres.
Play video starting at ::5 and follow transcript0:05
So the one we're working on right now is talking about loading JSON from an API. We're going to do a bit of spidering. We're going to create a little table.
Play video starting at ::15 and follow transcript0:15
And we're going to have a series of URLs and this is going to be done spider style. The data we're going to pull is a thing called the Star Wars API. And its a REST-based API that returns JSON. And one of the things we're going to do is we're going to spider in a way like a Google web search does, in that we are going to actually read the documents, and then in documents there are more URLs that we're going to parse out, films, species, the vehicles. But then if you go to like species, well, let's go to films. If you go to film 6, so we're going to parse this, then we're going to spider it and we'll go to film 6. And then you look at film 6, and there's a bunch of people in it. So what we're going to do is we're going to keep reading these until we have in effect read all the data. We're going to use the stuff inside the data to guide our future searching. Okay? So we'll talk about all of that stuff. This is free and open. It's really cool. It has a rate limit. I hope we don't hit the rate limit. So, let's take a look. You need to get the code for pg4e.com/code/swapi. SWAPI, Star Wars API, we're going to load that down. And I hope by now you have myutils.py from a previous example, and hidden.py is already set up, okay?
Play video starting at :1:43 and follow transcript1:43
And let's go ahead and drop the swapi table in our psql. It didn't exist so it didn't matter. And swapi is going to create this. So let's go ahead and start another browser here.
Play video starting at :1:59 and follow transcript1:59
And I'm going to start running python3 swapi.py. And so now what's happened is it's actually got it started by, so if we go look in here, let's take a look at
Play video starting at :2:22 and follow transcript2:22
Let's find a SELECT url
Play video starting at :2:29 and follow transcript2:29
WHERE status not equal to 200, that will get us what we want. You can always run the SELECT, because, so SELECT url FROM swapi. So these are URLs to do, let's do url comma status. I probably should say where status is not equal to 200. I should say, status IS NULL, it might be a better. Yeah, that gives us sort of WHERE status IS NULL and let me that back into my SQL code.
Play video starting at :3:12 and follow transcript3:12
And the ones that are 200 are the ones that I've retrieved.
Play video starting at :3:17 and follow transcript3:17
Let's make that be equal 200 and it IS NULL. So 200 is a HTTP code. So if you look here we've got a SERIAL id, we've got the url to retrieve that's UNIQUE. We have the status, which is the HTTP status, 200 is good. We're going to leave status empty, then we've got a JSONB body, and then we got a couple of TIMESTAMPTZs. And so what the program does to get itself started is it inserts some known URLs, films 1, species 1, and people 1. And then, let's take a look at it.
Play video starting at :3:52 and follow transcript3:52
swapi.py. So by now I hope you know how secrets work and how cursors work. Some print statements, how to use the SQL, myutils doQuery just to kind of make it a little less whatever. So what we're going to do is we're going to check to see if there are URLs in the table and if we got no URLs, then we are going to insert films sub 1, species sub 1. It's just a little insert using. All right, that's the string substitution using f-strings. Where it basically takes this, it reads the variable obj and replaces that, looks a little bit like a Django template, which is kind of cool. And so that's what inserted all of these records that we already see, that code is in there because it created it and it didn't find it, so it just inserted these things. And there's this cool little status function that I've got and I call a couple times. It checks to see how many we've got, how many where the status is null, how many where the status is 200, which is successful, and then how many that have added error. Now if all goes well, we shouldn't have an error. But if you hit a rate limit or the network messes up, you will start seeing errors in here. You'll see this status having like 404s, or 500s, or something, right? 200 is good, right? And so it's going to ask us how many. We got a total of three. We've got three left to do. That's because these have not been retrieved yet. How many good ones did we get and how many error ones did we get? That's just all SELECT statements reading from it. So now what we're going to do is we're going to ask how many documents, right?
Play video starting at :5:46 and follow transcript5:46
We got some, actually accumulation variables, how many documents do you want? So we're going to look for a URL from it where the status is NULL, and only grab one. queryValue is like pull read a row, run the SELECT statement, read the row, pull the zeroth element out of the row, or none. And if url is none, we're done, There are no unretrieved documents. Now we're going to start reading it. Actually, let me go ahead and start this thing. Let me do five documents. Is that going to work? Yeah, now see, it takes a while actually because one of the things going on right here is it's adding, it's finding and adding more links to the database. So just we retreived five documents but if I do a SELECT COUNT star FROM swapi, we've got 45 rows, and most of them are unretrieved. So it retrieved three documents, one document, got two new ones, three documents, got 34 new ones, they're adding these up, right?
Play video starting at :6:55 and follow transcript6:55
And so this is a queue, you can think of this. Now this is a restartable process, it's a spider. And so if I like get out of this, and say, well, we loaded five documents, we have 40 to do. Five are good and we're starting over. So now I can start over. And it says, oh, I know, based on reading the database, what I've got to do. So let's do 25 documents. Now this is a slower process and so this is why it's important for this to be restartable. So far the API's going well and it's not causing problems so we'll just kind of let that crank along while I talk a little bit more about the code. All right, so we can SELECT COUNT. So we got 50 in there, 51, right? So there's a lot we're gaining URLs as we're doing this. Oh, and then it committed so we're going to have probably a bunch more. So we got 25 documents loaded.
Play video starting at :7:55 and follow transcript7:55
Let's just load 100 more and go talk. Go load, go little Python program, go, go. Okay, so let's take a look. Some of this you've seen in the gmane example.
Play video starting at :8:7 and follow transcript8:07
We're using the request library. This time we're going to grab the text and the status code. And then we're going to do an update, we're going to, the key is to set the status also so that we don't double retrieve. Now hopefully this is a 200. But if it's an error, we're going to actually have that in there. Put the body in and set the updated_at equals NOW where url equals percent s. What we're seeing here is one, two, three percent s's, which means we need a three-tuple when we run the query. So we're going to run the query with the SQL and send in our variables, status, text, and the URL, and that does the insert and I just update my aggregation variables. And like before I make it so I can hit Control-C at various places. Any other kind of exception I just dump a whole bunch of debug stuff out. And away we go. And so then each time through oh, that's what we're seeing, the count where the status is null; This is a pretty chatty API, a pretty chatty thing. And so I'm going to select count where it's null and then I'm going to print what the status was, what URL I got, and how many I have left to do. So this is the to-do list. It sort of grows. You'll see that the to-do list kind of grows and then it sort of shrinks.
Play video starting at :9:29 and follow transcript9:29
Shrink, shrink, shrink, shrink, shrink, it's shrinking, and it might grow again. We're starting to run out of things to do, so let's go do another 100 documents and see what, we're done. Look, there's only so many documents. So it doesn't run forever, which is good because we don't want to rate limit the poor API. Okay.
Play video starting at :9:49 and follow transcript9:49
So, the next thing we do, here's is the to-do list, is I'm going to take the returned body and I'm going to do a JSON operation in Python. I'm going to load JSON from a string, which parses it. This is going to blow up. I probably should have put a try/except around here. When I write these things, I tend not to put the try/except in until it starts blowing up, because then I'm ready to debug it. Like this API would be blowing up. But it's giving me back good JSON. So I haven't written the code to deal with if the API gives me bad JSON because it's kind of hard to test. So, yeah, whatever. Okay, so then what I'm going to do is I'm going to look through all the linked data. Now, this is what I was showing you before, when you look at one of these things. I'm going to look through films, species, vehicles. This is a nested loop. So I'm going to look through all of the vehicles, all of the starships. You'll notice that this is all in the form of URLs. So I'm going to look at films, species, vehicles, starships, and characters. Maybe they're not there. So I'm going to pull out of the parsed JavaScript, because the parsed JavaScript in this case is really just a Python dictionary. This becomes a Python dictionary name Luke Skywalker, films is a Python dictionary, films, keys in that dictionary that points to a list. So I'm going to grab this list at the end of films, and that's what I'm getting here and that's what I'm calling stuff. Now I'm checking to make sure that stuff is a list object because if it's not a list object, I'm just going to skip it. I don't know what. I'm only capable of handling This is like a guardian pattern where I'm only really capable of handling lists. So I don't want this next line to blow up, so I'm just like aargh! This next line would blow up unless it's a list, so just guardian it so that at least I don't blow up. Somebody might put an error message in here or something, but I'm just like, I think the data's pretty clean. If the data is starting getting yucky, I'd start putting error messages in there. Then what we're going to do is we're just going to loop through that list. It's pretty simple, right? Got a list, go through all these things. And I'm going to insert into it with just nothing but the URL. But ON CONFLICT url DO NOTHING. So what I'm just saying here is if it's already there and there is a UNIQUE clause, right? There's a UNIQUE clause. Somewhere up, there is a UNIQUE clause. url is VARCHAR 2048 UNIQUE. So that basically is going to trigger this ON CONFLICT of the url field, don't insert. So that just saves me. You can also do ON CONFLICT UPDATE, but we don't care about that. All we're doing is adding to our to-do list in this case and if it's already been done, I don't care. So do nothing.
Play video starting at :12:34 and follow transcript12:34
Again, we have one percent s in this SQL, and so we have a one-tuple of (item, ). And then I'm going to commit and that's probably why this runs so slowly is because I got that commit inside this for loop.
Play video starting at :12:54 and follow transcript12:54
So I'm, every insert that I'm doing here is committing. So I probably would be well served,
Play video starting at :13:7 and follow transcript13:07
I would be well served to bring this commit. So when you download this, well, I didn't erase any spaces at the end. So when you do this, it'll run faster, because this query is going to insert, but basically I'm only going to commit once per retrieval, okay? I was committing everything I was inserting. So that meant that every time this number went up, it was running a commit, so I didn't have to commit quite so often. Oh, it's finished. So it loaded 207 documents, loaded 202 in that particular run, total was 207. They're all good. There was no errors. So we're done. So we've got everything here. And you should see it the same as well. And because I just fixed this little thing, it's a choice whether you commit every time, if it's going to blow up. So maybe you do want to commit every time, but I think committing right here, after you've gone through the. If the JSON is working, and you're going through it, these inserts aren't going to blow up, so you might as well kind of queue them up and sort of blast them in a batch to the server. And then basically every 25th record, I'm going to run a commit.
Play video starting at :14:20 and follow transcript14:20
Actually, I could just take this commit out because I'm kind of already doing that. Every 25th record, I'm going to run a commit. I'm going to wait for a second and then sleep, mostly so I could hit the Control-C if it's blowing up.
Play video starting at :14:33 and follow transcript14:33
And at the end, I do a summary and I close the database connection. Summary, again, is the thing that puts out this little cute line right there. Just helps me know what the heck's going on. And so the darn thing is done. And so I have now crawled it, right? So this is a crawling thing where it starts with a couple of URLs but retrieves the stuff, looks at what's in those URLs. And then adds to the to-do list unless I've already seen it. So I'm only going to hit every one once. I'm careful about that, right? I only hit it once. So, for example, now if I look at some of these things, url comma status. Well, there is none of those.
Play video starting at :15:15 and follow transcript15:15
Let's just do LIMIT 1. LIMIT 5.
Play video starting at :15:22 and follow transcript15:22
So url comma status so you can kind of see that over time,
Play video starting at :15:27 and follow transcript15:27
the successful, you got a status 200, which for us inside this database means that I got a successful retrieval of this particular URL and I can now say url, status then I'll say JSONB and it'll just be ugly at that point. because JSON B is really big. Actually, it's body.
Play video starting at :15:48 and follow transcript15:48
It's gigantic. So url, status, there's a URL, there's the status, and here is like a whole bunch of JSONB. And you'll also note that the JSON, yeah, the JSONB is not the exact same format as the JSON that came back, which has a bunch of pretty spaces in it. And that's because the JSONB in the database is parsed and compressed, right? So this is a reconstitution of the internal data structure that Postgres has stored.
Play video starting at :16:20 and follow transcript16:20
Okay. So we now have all this stuff done. And so let's come back in a second part and play with it now that it's all loaded.

# Demonstration: Star Wars API 2/2
Welcome back to a continuation of our Star Wars API. We have loaded all of our data. We have retrieved it, we've spidered it, we found the new links and we've loaded it all and so we're completely full. I can say SELECT COUNT url here. SELECT COUNT url FROM swapi.
Play video starting at ::33 and follow transcript0:33
So we're all good. We got 207 documents loaded and so let's play, right? So we can look for the url inside the body tag and convert it, right? So there we got a string. We can ask for the url of body where it contains director colon George Lucas. So we can have a WHERE clause. It's working now. Now again, there's no indexes involved in here, right? We don't have an index yet. So if we do an explain, we're going to find that that just is a sequential scan. SELECT body url FROM swapi WHERE body contains director George Lucas, and it's a sequential scan. It's exactly what we'd expect. We haven't put anything yet. So let's go ahead and insert 4000 meaningless rows of Dodge Neon race car SQL. It doesn't even have a url field or nothing. So it's just extra crud, 4001, and now we're going to create our index. We're going to create a generalized reversed index, swapi_gin on the body with path_ops. And so json_path_ops gives me the key-value pairs and allows me to look up by key-value pair. It's kind of a smoother thing. So then we've got to be careful because it might take a little while. Let's take a look at. Let's do an EXPLAIN SELECT to see if we can turn this SELECT body url FROM swapi WHERE body contains George Lucas. Well, it's done already. So I don't have to go about the thing of like, "hey, let's wait a while." Sometimes it happens fast. Sometimes it doesn't happen fast. But if again, if you do this EXPLAIN SELECT and it doesn't get you what you want, wait a bit. Have a cup of coffee and come back. It shouldn't be too much coffee. Okay. So we got an index. So there we go. So let's like look at some stuff here. Let's say I want to find the films that aren't George Lucas. So we can say, what are the films that George Lucas was the director of? Do that. There's four of them. So let's see if we can figure out how to find the films that George Lucas is not the director of. So I put a big NOT around this, NOT body contains George Lucas jsonb. I cast that to JSONB. And oops, we got a whole bunch of URLs. A lot of URLs, not so good, right? Oh, and there's even blanks. There's even some of those pages are giving me trouble because I put all that extra crud in to make it big. But so we can fix that with a WHERE clause, right? We can ask what are the things where the body url has got a pattern of https://swapi.co/api/films followed by something. So we can use a LIKE clause. So we can of course do that, and then we'll find only the films. So those are the films. There's only six records that contain films. And if we do an explain, we will be bummed out because we'll see that it's a sequential scan. Now, one of the things you probably could do is you could say, "you know what I think what I'm going to do is put a B-tree index on body url." And that should hit on prefix queries. We could see. But we can look for the films that aren't directed by George Lucas, the Star Wars films not directed by George Lucas, by saying, NOT body contains director George Lucas AND body is like a film, is a film. So that will do it and we will find it. But that will be a sequential scan. So that will show us the films, the three films in the Star Wars that were not directed by George Lucas, which is fun.
Play video starting at :4:52 and follow transcript4:52
So we can do this LIKE clause. You can see the url, the people clauses. We can see the people with a LIKE clause and in a sense you could think of the prefix of this URL as a type. You can see all the species. Species and their URLs, right? And that would be kind of a useful thing to do. But it means that everything is a sequential scan. So what we're going to do is we're going to put a little effort and we're going to hack the JSON, Okay? We're going to hack the JSON. We've done this before and so we're going to hack the JSON by a regular expression. And so first I'm going to show you the regular expression. The regular expression is https://swapi.co/api/, and then parentheses is the start of an extraction, bracket a through z is the letters a through z, plus means one or more, and end the extraction. So if you run this and all it's going to do is the regular expression extraction on the url. But you'll see that it says films. If I make this LIMIT 10, see films, species, whatever. These nulls are those Neon records that I put in, so don't worry too much about that. Now, if you recall in the previous thing, when we're going to mess with the JSON itself, we have to ultimately use the concatenation operator here. We're going to have to do the JSON deconcatenation. This is a string concatenation double vertical bars concatenation. So eventually we've got to do a JSONB concatenation, which means we're going to eventually construct a big long string, and then convert it to JSONB and then merge that into body just like we did before. So let's start by looking at this string which really is a string concatenation here. type colon concatenated with the regular expression that pulls the word, the type, out and then a double quote. So first we've got to build we've got to build a string that represents the new JSON that we want to put in. And so this is a string type colon films, right? That is the new JSON. Now, in order to blend it with existing JSONB, that's a JSON string, we've got to cast that to JSONB by adding the jsonb cast on there, right? So we'll cast that to JSONB. So it's going to look roughly the same except that this is JSONB, a type of JSONB, which means we can blend it together. And so if we now look at this, we're going to select the body, this is going to be gigantic, concatenated, a JSON concatenation, not a string, of this big long string converted to JSONB. And so I'll select this but but it'll be really big. Yeah. So somewhere in here, let's find it, let's find it. Let's find type, type films, right? So we've got the URL but we extracted this little bit in type films, okay? So we've added a bit to the JSON and we've done this, well we added one to a count in the previous, in one of the previous examples. But now we've added something to this JSON, right? So we're going to do an update, which is again this whole body concatenated with JSONB. That's the record we just selected. So we're going to take and pull out the body for all of the records, add the type variable to it. So this is going to do it for all the records.
Play video starting at :8:47 and follow transcript8:47
So I've updated 4,208 records. And now you can select those records. SELECT swapi body FROM oh, no, sorry. Yeah, wrong wrong wrong. body type comma body url FROM swapi LIMIT 5. What did I get wrong? What did I get wrong? Oh, I'm getting too many of the blank ones so don't worry too much about that. Okay, so now I've added that. So now I can use this in a WHERE clause, right? SELECT body url FROM swapi WHERE body is type species LIMIT 10. So I'm going to pull out the URL, and then I'm going to use a WHERE clause which will actually find me some stuff and that's because these blank ones, it went past the blank ones and got to some that actually had type equals species, because these Neon, those are all the Neon ones that I just did so the dang thing would actually do my explains the right way. So I now have this new thing. I can talk to it. My index should understand it. And so I should be able to explain that, and if all goes well the new field that I did, should do a bitmap check swapi_gin. This UPDATE statement here, it updated it and then within like a couple of seconds the GIN was up to date. And if I had done this update and then like right away did the EXPLAIN SELECT, the index might not have been completely updated. So the indexes lag a little bit and this is where I keep saying that the insert performance, update performance, versus the query performance. And GIN is. This is a GIN with text_ops, so it's the most difficult to update. But with only 4,001 records, it's only a few seconds. And so that shows you how to like augment it, which is another thing. And so now I can actually say, show me what's going on with George Lucas and let's actually just run that SELECT statement, SELECT url George Lucas. And I can explain that. And let me NOT that.
Play video starting at :11:46 and follow transcript11:46
Oops, not a note. I sure hope that I'm going to get my index to work because I worked really hard to make this work. So we're saying, I got my NOT parentheses wrong here.
Play video starting at :12:14 and follow transcript12:14
I'm coding right in front of you. Hopefully I get it right. This is not as easy as it looks. Let's see if I got it. No. What happened?
Play video starting at :12:27 and follow transcript12:27
What just happened? The body doesn't contain George Lucas type films.
Play video starting at :12:46 and follow transcript12:46
Oh, yeah, so this is not going to work because this is all the things that don't have the combination of type films. So let me try to fix this.
Play video starting at :13: and follow transcript13:00
It's going to take a little work.
Play video starting at :13:9 and follow transcript13:09
Let's see if I could make this work. So the first not AND body
Play video starting at :13:22 and follow transcript13:22
contains type films.
Play video starting at :13:34 and follow transcript13:34
Let's get this one right. So what I'm going to say is I'm looking for things that have the body that contain type films. I'm going to add an extra parentheses back there. It'd probably be be better if I flipped this, it would make more sense if I flipped this to the end.
Play video starting at :14:3 and follow transcript14:03
Make more sense to me at least.
Play video starting at :14:10 and follow transcript14:10
So if I got my syntax right, I'm interested in URLs that have a body of type films and do not have the director of George Lucas. And so I've got to have my parentheses. Oh, I'm pretty sure that's going to work, boom. Yes! Now comes the big test. Get rid of those. Those aren't any fun. Those don't teach us anything. The always payoff is did I get this in a way that my indexes work, and I sure hope so. So let's run the explain. Hello, Postgres. Did my indexes work? Yay, my indexes worked. I always feel great when I don't see sequential scan. I see heap scan. I see that swapi_gin worked. So there you go. You will see this without the mistake. There we go. type is films and body is not and the director is not George Lucas. There are three films in the Star Wars set for which George Lucas was not the director. And that is all I have for you. I hope as always that you found this demonstration useful. Cheers.