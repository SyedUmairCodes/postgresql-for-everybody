Welcome to the Course!
Welcome to Course 3 of Postgres for Everybody. In this course, we're really going to focus on two topics, and that is text and JSON. Computers do numbers really well and that, for example, is why we use numbers for foreign keys because 64-bit integers are something computers store efficiently, move around efficiently, compare efficiently, etc. But the large fraction of the work of the data that's being stored are strings, names, comments, blog posts. What good are numbers? I mean, we use them for like average weather temperature or something, and so it's useful. I mean, zip codes, they're not even numbers. They're like strings. Phone numbers are strings. So text is important and natural language is important. Texts of conversations and search engines, etc. And JSON is really the biggest thing to happen to relational databases in 20, 25 years. And I've been talking about it kind of all throughout the course about how important it is that databases do a good job with JSON. JSON allows for applications to evolve so that the structure, I mean, we did data modeling. We talked about every column matters, a 64-character width column versus a 2 million-character column matters. We want that for efficiency for maximum packing. But at some point, that sort of prearranged contract of you know exactly what columns they're going to be and exactly what size they're going to be is great, but it's also limiting for some applications. Sometimes you just want to throw a little bit of data in, not something you're going to sort on or put in a WHERE clause or whatever. Although you will see that with JSON you can index pieces of JSON that make it so that it works great in WHERE clauses. And so the flexibility that JSON gives us is really an improvement to relational databases. But then what we don't want to do is we don't want to give up all of the amazing performance that we get by a carefully constructed schema where we agree with the database system in advance how we're going to use it. And so this section is going to talk a lot about how unstructured data can be treated like structured data. So I hope you enjoy it.

# Allocating Rows to Blocks in PostgreSQL
Hello and welcome to our lecture on PostgreSQL full text indexes. So, you can get this material online, the PowerPoints. I've only got PowerPoints for the diagrams in case you want to reuse them, because I basically switched my style in this particular lecture from doing it all in PowerPoint to taking what I usually build as my lecture notes and just recording those and giving you my lecture notes rather than making lecture notes, turning them into PowerPoints, and then giving you the PowerPoints. I hope that you will find this to be more useful. I'd be curious if you like this format compared to the all-PowerPoint format. So you can get these materials online and add whatever you want. So first, I want to talk about row layout. We've been talking about the CREATE TABLE, and how schemas and how important it is to be precise about if this is a short text column or a long text column, etc., etc. And that schema becomes a contract because that's the way that Postgres can very efficiently store the data. And the amount of data that you scan is the thing that determines your performance. And so now I want to talk a little bit about how a schema is represented on disk. So here we have a little tiny, a messages table, one we've used for a couple of examples, where we have an email message, a timestamp, and then some relatively large bits. And you see that the id is small. It's exactly four bytes of memory and the timestamp is eight bytes. Then the other things are long. We don't exactly know what their length is. Now, remember we can insert and then we can update these. And so you might have an email address that's four characters or 20 characters and replace it with an email address that's 10. The size of a record can go up or it can go down based on updates. And when we modify data, we don't want to rewrite the entire file because that's like let's just say a minimum one gigabyte file takes about two seconds. Even on the fastest hardware, writing or reading a gigabyte takes several seconds of time. And so Postgres organizes its disk files with free space on them and they're organized into blocks of fixed length. And by having blocks of fixed length, typically 8K, 8,192 8K blocks,
Play video starting at :2:34 and follow transcript2:34
they're able to sort of compute a block offset really easy, so they don't pack the blocks in either. They make fixed blocks. So they can read the entire block into memory. The block is often a unit of locking. The block is often a unit of caching. So one of the things the database does is like actively used blocks end up sitting in the memory. So if you're hitting a bunch of queries at the same time and they keep reusing little blocks of files, then those blocks will stay in the memory. And so everything that the database environment is doing is making it as efficient as it possibly can be. And so for transactions, which we briefly talked about, they're often the unit of locking. So we think of locking a row and a block has somewhere between 5 and 50 rows maybe, or even less than 5, maybe 2 and 50 rows, but it doesn't matter. So let's take a look at the shape of a block, right? So here we have an 8K block, and remember that 8K blocks are they're all 8K. And so they are are placed on disk and they're stored one after another on disk. And within the block, there's some header stuff and other things that's in there, but the basic idea is the rows are inserted into this block sort of from the back to the front. Now, each of these rows has columns, some of varying length, and different rows have overall different length and the columns are in there. So then you insert another one, and you insert another one. And the rows grow from the end of the block forward and then, all they have is little. these offsets are like probably 2 to 3 bytes, probably 2 bytes. And what they are is, they're just like an array of where within that block the start of a row is. And so they're are small. The whole block comes in and then you can quickly go to each row. You can read them in order. You can read all of them. And then the middle here, which is really not to scale on my picture, is the free space. So you have growing from the end of the block the rows. You've got going from the front of the block, you've got the offsets. Now, the rows are generally way bigger than the offsets. Now, think about this from an update perspective where so if we were going to just take this row and there's like 10 characters in an email address and we're going to replace it with a 15-character email address, we can shift just a little bit of data. Five characters, we're going to shift everything five characters to the left and then we make 15 characters. Then we put the new email address in where it belongs. And then we just rewrite the whole block. And we've lost some of the free space. Same thing would happen if we rewrote an email address with a smaller email address. We'd shift everything. Actually, we might not shift everything at that point, but the point is, we can make small changes to this. Now again, in my picture, free space is not to scale because you don't want to make free space in my picture, it kind of looks like it's 70 percent of it. This is a block being filled up. But the idea is that we can make tiny little modifications and then change these offset values and then rewrite the whole block to disk. The whole block is read in. We fiddle with it in memory, which is a very cheap operation, and then we rewrite it back out to disk. Now you might say, "Well, what's the best idea of block size?" And they go round and round and round. And there's a lot of things in Postgres where just accepting the default. is probably what you want to do. And so 8K is generally the default. If you have a block that's too small and then your rows don't fit well, and you sort of fit one row in a block, then you've got kind of a wasted free space. If you have too large a block, then you're pulling too much information in and out just to like read one row, and you start using up all your memory, your caching. If you have 8K blocks or 16K blocks, you can cache 50 percent of the blocks, 50 percent fewer blocks if they were 16K. And so you can go read up on that. Google and say "What's the best block size for Postgres?" And they'll tell you that it's probably 8K and the next best block size might be 4K. And it has to do with the fact that solid-state disks, unlike spinning disks, so spinning disks have rotational delay and then once you get to the data on the spinning disk, pulling in 8K is smarter than pulling in 4K because the rotational delay is by far the greater consideration. In SSD disks, there is no rotational delay. And so then what happens is the thing that dominates is the actual amount that you transfer. And so that would advocate for smaller block sizes on SSDs and larger block size on hard drives. The problem is if you make the block sizes too small, then you get fragmentation and you can't do some of those dynamic things where you're either inserting a new row or you're taking some data, you're updating a row, and so the too tiny of a block size, then you get either too much free space, because you can't fit enough rows in a block, or you'll get too little free space. Then when you pack two rows in, and you've run out of space and you do an update, then you've got to move stuff all around in your database and it's not good. So there's a lot of different ways to think about this, but the key point to take away from all this is that we read entire blocks into memory. So we don't really know where a row is exactly on disk but we do want to know where a row is in a block. We know that this row is in this block and we read the block in and then we scan through the block and then find the row. So we can't go straight to a row. We can go straight to a block. So indexes, which is what we're really talking about now, indexes are basically a set of hints that map from a row that we're looking for to which block those rows are in. And that's what we'll talk about next. 

# Index Implementation Details
Now we're going to talk about indexes. We just got done talking about rows and blocks, and how a big file is broken into blocks. And then within there, we have free space, little gaps to do inserts and deletes, and sort of reorganize stuff without having to rewrite whole files. That's really important. Random access is a critical element of databases.
Play video starting at ::19 and follow transcript0:19
Now I want to talk about how we limit the number of blocks that we have to load for a given query. And that's the purpose of indexes. And so assume that there's some kind of a logical key or maybe a primary key, and we're going to look something up. Maybe it's got a prefix, you know. like a LIKE with a prefix. But we'll just talk about logical keys. And so if you could somehow store the actual logical key variable, the value of the logical key, and then the block that it's in, and if that was so small that you could keep it in the memory, you could look in this little memory table that says oh, here's a clue. It says colleen@umich.edu is in block 21456. And boom, go get block 21456, and away you go. And so that's what an index is, it's a key-to-block hint. And there's a lot of different kinds of indexes, but then you're storing like 30 characters per row instead of 2000 characters per row. And then you can make those really as small as possible. And so the basic idea of an index, right, is we have this little cheat sheet of quick ways to go get a block. And then the index informs a WHERE clause. And the whole goal of a WHERE clause, you don't think of a WHERE clause like an if statement. You think of the WHERE clause as, it's sending it to all the rows. And then the rows respond with WHERE clause is true. And what we're doing with the index is we're making it so we can, in effect, ask the disk to send us a whole bunch of data, and then throw away the ones we don't need, or just give us the ones that we do need. So the less we ask of our disk drives, the faster our database system is going to be.
Play video starting at :2:2 and follow transcript2:02
And so it's really small. And so that's what we're going to talk about, is the different kinds of indexes that you can use in Postgres. So there are two kinds of indexes, forward and inverted indexes. And there are a number of different kind of forward indexes. You might call them normal and weird, instead of forward and inverted. But that's what we call them, forward and inverted. So the classic index that we use a lot, and we'll talk about this, is the B-tree index. Which is a balanced tree, which is a tree that fans out. And we try to minimize the depth of this tree, so that even if we have to read some of the index off of disk, we don't have to do too many. We sort of, for a certain number of records,
Play video starting at :2:47 and follow transcript2:47
the amount of reads that's caused by the index is like one, two, or three. And then you read one block, and then there you go, as compared to reading hundreds of thousands of blocks.
Play video starting at :2:58 and follow transcript2:58
Another one we'll look at is called a Binary Range Index. And it's a gross simplification from the B-tree, but it's appropriate for certain things. Hash is like hashing, where you have this little table. that instead of the key, it takes a hash of the key, which is almost always much smaller, and then goes straight to the block. Hashes, just like in everything, are great until they collide. So the hash is supposed to spread the keys out. And if you can make a hash work, you can make a hash work. And we'll talk about the different inverted indexes as well.
Play video starting at :3:33 and follow transcript3:33
In the regular indexes, where we're looking from a key to one or a few rows, you might as well just leave it default. Don't even tell it what kind of index to use. Let it figure it out. And it's almost always going to use B-tree. B-tree is good for string keys. It's good for number keys. It's good for date keys. It's a great balance between the cost of insert and the cost of updating. The BRIN, the Block Range Index, that's actually a lot smaller and it's very fast. But it depends on data that's kind of sequentially increasing. Maybe you do it on a date field, or even a primary key where it's one, two, three, four, five. And then you have these blocks, and they sort of have 1 through 10, 10 through 20. And then you just keep the largest and smallest number. And so we'll talk about the inverted indexes as well. But let's take a look at how a B-tree works. So you can read this. I'd encourage you to read it, it's pretty well written. Some Wikipedia pages are harder to read than others. I'm going to pop this image up to make it a little bit bigger. Now the key here to remember is, this is the index. And the whole idea of a binary tree is that it reorganizes itself. It moves things around. And so, for example, let's just say, and you can put many more of these things. Remember, your keys are there, and then it points to actual blocks. So you could think of like this 1 is pointing to a block, 2 is pointing to a block. These are all blocks that we know exist. And these are the primary key values. Now in this top one where we start, we have some number. And it's way more than four, but we're only showing four for the example. And what it really says is that we keep track of, this first, leftmost one has the keys below 7. The middle one has keys between 7 and 16. And the far right one has the above it. And so if we're going to insert a record, say 22, and we would go find a block on disk. And maybe it'd be in an existing row, we don't know. because all we want to know is which block the row that corresponds to key 22 is. We go through our B-tree. We just add a little entry to 22, and then point that at the right block on the index. And the same, let's just say we're going to do 15. We do 15, we'd look in. We'd find that we'd have to go on this middle one. And we put it in 15, after 12. And then we would put the row, and we'd keep track of which block we put it in. So you can think of insert the row, then insert the index entry to keep track. Now let's just say that we are doing 10. No, let's do 4, let's insert 4.
Play video starting at :6:17 and follow transcript6:17
So we're just talking about index maintenance. So you know it's less than 7, so it's got to go into this index field. Now you've got to rewrite some things, because we know it's going to have to go between 2 and 5. So what you have to do is you have to shove things. So you sort of shove the 12 up, the 9 up, the 6 from one block to the next, and the 5 up into that block. No, the 5 goes to here. And then the 4 is inserted between the 2 and the 5. And then we still have a block for the row, and we keep track of that. So this is how we balance it. And you can imagine if we just ended up with way too many things in, say. the middle block, and we filled it up, we'd have to split it. And at some point, this tree can be more than two deep, and we worry about keeping the depth of this tree constant. So there's many things, there could be hundreds of entries in each one, in each depth. And so you have this tree and it goes out. But the idea is with somewhere like three or four deep, you can have hundreds of thousands of records. And if you had to store these things on disk, you would read the first one, then you'd read second, third, and fourth. And then you'd read the block, which is way better than 200,000 blocks all having to be read and then scanned separately. So it seems like a lot of effort. And this B-tree is kind of beautiful in that, as long as you leave a little bit of free space in these index blocks, it's pretty easy to add new ones. And every once in a while, you have to kind of do a little reorganization. You might have to make the tree deeper once in a while. But it actually does that, and it amortizes the cost, the insert cost, over inserts pretty well. Every once in a while, an insert can sort of be a little more expensive. But most of the time, the inserts are really cheap, the cost of updating the index is really cheap, and the scanning cost is really good. So that's again why we kind of say, hey, B-trees are great, just use them. Now, let's take a look just for a minute at the BRIN, just to give you a sense that this is all about having a sense of a hint as to where to find a block. So the BRIN is something that's in a sense unique, at least right now, to
Play video starting at :8:32 and follow transcript8:32
it's unique right now to Postgres, but it's pretty cool. So the BRIN is a little different. So it's this linear list. And each one of these in a sense points to, so we have four things, points to one block on disk.
Play video starting at :8:49 and follow transcript8:49
And what you do is you scan the blocks, and you look for the minimum and the maximum in each of the blocks. And then you have just two numbers, you characterize each block as two numbers. So this first block is the key, the values for our logical key are somewhere between 1 and 6. The second block, you look at it, somewhere between 9 and 12. Third block, you look at it, 7 and 16. And then, 18 to 21. The key thing to look at here is, there is actually an overlap. 7 to 16 is actually a bigger range than 9 to 12. Which means if we're going to ask where block 10 is, or record 10, we would have to pull two blocks in. We'd pull two blocks in, because that's what the BRIN would tell us to do. It's reduced it from, 100,000 blocks, but we've still got to pull a bunch of blocks in.
Play video starting at :9:40 and follow transcript9:40
And of course, then what we'd do to figure out where 10 really is, because 10 could be in either of those blocks, or it could be in neither of those blocks. Then you've got to read through the blocks. But remember, the blocks are only 8K, so it's not like the end of the world. It's how many blocks you read, not what you do with the blocks. And so if you say find 10, it might say, didn't find it. Or it might find exactly one copy of it. And then if we think about updates. Oops.
Play video starting at :10:9 and follow transcript10:09
If we think about updates, Here we go, click. Think about updates here, come back. Right, so think about updating and, you know, think about adding. So let's put some blocks down here, some more blocks down here, some more blocks down here, some blocks down here. And let's just say that they're sort of partially full, right? So there's a little space in each one.
Play video starting at :10:41 and follow transcript10:41
And let's say we're going to put record 4 in, here comes 4. So we'd look in the BRIN and say, is there a reasonable spot to put that in? And the answer is, well, we've got this block here. It wouldn't even change the index if we put it into this first, leftmost block. So yeah, you go ahead and you write, and you've got some space. And then you actually didn't change the index. You just extend that one, that's kind of nice. It's kind of like hashing, except it's like a range. And let's say, for example, that this block was, let's say this second block was full and the third block was full. And then, let's just say we were going to do, I'd better clear it. Let's say we're going to, we've got some blocks. Come back, pen. There we go. We've got some blocks, got some blocks, got some blocks, got some blocks. We've got four blocks for the four BRIN things. And let's just say these 9 through 12s are all full, right? They're both, the 9 through 12 and the 7 through 16 are full. And we're going to put 10 in.
Play video starting at :11:45 and follow transcript11:45
So you can kind of imagine what's going to have to happen. You might come in here. You'll decide to come in here and you're like, crap, crud, it's full. You might check this second one and say it's full. So then what you would do is be like, I don't know what I'm going to do. I'll make a whole new block. And what you might do is pull a few records from one block, the second block, and the third block. Pull records, put the 10 in there, and then create a new BRIN entry that properly reflects the ranges. So this might be 9 to 10 in the leftmost one. And there might be 11 through 12. And this one might be, you know, 13 through 16. So you can kind of see how that, it might be adjusted. And then you've got to shovel things, shovel things around, etc., etc., etc. And so that's kind of the update process for the BRIN. But the nice thing is if you have numbers, if you're building blocks that are kind of sequentially growing, and the index is like a primary key thing, the thing you're indexing is a primary key or a date, these BRINs just kind of add on at the end. And then it just kind of works. And then you just come in, and the queries are super fast, and the size of the BRIN is really super tiny. So there's a lot of data mining applications potentially for a BRIN index. That might not be, I mean, but that just gives you the sense that Postgres has these capabilities. And when in doubt, we will use the B-tree by default. Now, there are two basic categories of these indexes.
Play video starting at :13:23 and follow transcript13:23
We have forward indexes and inverted indexes. And I don't even like these words. I would rather the forward indexes were like normal indexes and array indexes. The forward indexes, like, I know there's a key value. It represents a column. And from that key value, I will find the row. It's like a logical key or a primary key, and I've indexed that logical key or the primary key. That's a normal index. That's what we've always done for indexes.
Play video starting at :13:53 and follow transcript13:53
B-trees, BRINs, ISAMs, they're all variations of, I've got a key, and the index actually sort of stores the key. So inverse indices work in situations where you have a column, and in that column, there's more than one thing. You could think of it as a Python list of things, or an array of things. Or you could just think of it as a document that has lots of words in it. So in a sense, these columns don't have a single key. They like have an explosive number of keys. And what we're interested in is not necessarily finding one row, but more interested in finding the entire set of rows. So if I'm looking for all the rows where SQL is mentioned, and you could think of this as it's sort of a wildcard, %SQL%. But how could we do that efficiently? Well, you have to break the string into pieces. And then index all the pieces, and keep track of all of the words in any row, and which rows, i.e., blocks, have those things in them. And that's what an inverse index is, okay?
Play video starting at :15:3 and follow transcript15:03
And so we have a couple of inverse indexes. We have a GIN index, which is a generalized, the generalized reverse index. The GIST index, which is a hashing-oriented index. And then there's this SP-GIST, which I wanted, I really want it to be the special one. But it's actually spatial, i.e., for longitude and latitude, or for three-dimensional points or things like that, that have some kind of a spatial clustering. And so the classic use case we have for inverted indexes is text search. And up next, we'll take a look at the kinds of things that we use indexes for, and starting with Google search.

# Building an Inverted Index with SQL
So now we're going to take a look at inverted indexes. Like I said before, inverted indexes are generally used for text search. So let's just say you're going to build a blog or you're going to have a bunch of blog posts in your database and they have a bunch of words in them and you want to have a search box. That's the classic thing. And we will see as we go further sort of these inverted indexes have other uses than just text but text is the easiest thing to start. Inverted indexes have been around a long time, long before Google and even search engines and even the Web. It's a sort of computer science concept. But certainly, the way search has become so important and so enmeshed in our use of the Internet and the Web, we have learned a lot. And so in a way, it's nice to think about how Google thinks about searching and how Google thinks about keywords and how Google thinks about matching. And there's a lot of innovations that Google has added on top of the basic inverted index. So it's kind of nice to see. I've got a couple of presentations here that give you a little sense of Google's take on search. One is the Google I/O 2008 keynote by Marissa Meyer. I was at that conference in the audience. It kind of blew my mind. In 2008, Google decided to show a lot of its magic. Up to that point, they'd been very interested in saying "Oh Google's all magic. Don't worry about it," but they wanted to get us as developers to begin to make use of Google products as developers and so they started talking about it. It's a great talk. I've got a short link right into it where you can go right into the part where she talks about search. It's only about a minute and a half. Then there's another one by Matt Cutts that just tells about keywords and inverted indexes and how they all work and how once they have an index and how they order and rank and that really is a nice outline about the things that we're going to talk about in this class. So the first thing we're going to do is we're going to build an inverted index without using the built-in features of Postgres. And Postgres has some wonderful built-in features, but we're going to first build it by hand so that we better understand the magic. So we can understand how to tune the magic and how to make better use of the magic stuff. So as I mentioned, inverted indexes work on columns that are sort of on multiple things, so like an array. And so basically, what we're going to do is the first two functions are going to make a big difference in how we can do all this stuff. And so the first array is the string_to_array function. And what it does is it takes any string, it's like split in Python, and it just splits it based on a delimiter and gives us back an array. And so we could think of a blog post after you split it is a bunch of words splitted based on whitespace as a bunch of words. So string_to_array is Hello,world. And this also shows you the syntax of arrays, curly brace, Hello world is sort of the pretty syntax of a string array. It doesn't need quotes. It's a bit of a weird syntax. But when you see those curly braces, it's an array of something. In this case, it's an array of strings. So string_to_array is a function. The other function that's really important, that what we're going to do here is unnest. So unnest is similar to the generate_series things we did in the previous, the text-based lecture where you say generate series one through five, and that would actually make rows 1, 2, 3, 4, 5. If you concatenated it with neon, generate series 1, 2, 3, 4, 5, it will be neon1, neon2, 3, 4, 5. And so this unnest is one of these things where if you use it, it takes an array and expands it into rows. It's kind of like a JOIN in a way where it's reintroducing vertical replication. It's turning something that's horizontal into something that's vertical. And that's why it's called unnest. I might call it vertical to horizontal. I mean, horizontal to vertical, actually. Okay? So if we take a look at the string_to_array Hello world and we unnest it, we get two rows. Now, with those two things, we can build an inverted index. Well, what is an inverted index? Well, an inverted index takes a bunch of documents, pulls all the words of those documents out, and then ends up basically with a mapping from the keyword to the document, right? Those are the keywords of document 1, the keywords of document 2, and the keywords of document 3. That's what the inverted index is. And then what we can do is we can look forward through that by going down and finding a keyword, maybe SQL, and we find that document 1 has SQL, document 2 has SQL, and document 3 has SQL. It's not surprising in an SQL class that all three of my documents have the word SQL in them. Once you can parse a sort of complex column of text and you can split it based on spaces and you can turn each of those spaces into the rows, into rows, you end up in a situation where we just have to do a few SELECT DISTINCTs, because, remember, we can select and we can insert from those selected things. And so we're going to basically take a simple text document management system which has a primary key in a text field and we're going to put some documents into it. Then we'll create an index which has a keyword, not unique because remember the keywords have to point to many documents, and then a foreign key back into the documents called doc_id. So we'll create that. Then I'm going give you a whole lecture showing you all the gory detail of how all those SELECT DISTINCT work. Then when it's all said and done, we're going to have this table called docs_gin, which is a mapping between all the keywords in the document, space-delimited words in the document, and not worry about punctuation or anything. I didn't put punctuation in, otherwise I had to do something about the punctuation. And it just basically says that there you go, The word SQL is in document 1, SQL is in document 2, and SQL is in document 3. And away we go. And then what we can do is we can use a WHERE clause to look things up here and then use that to get a set of rows and then that set of rows works its way to a set of blocks. And so that's what we're going to cover in the next thing. Then the thing we'll do after that is, once we do it the hard way, we will show you the easy way to do it leaning on all of the Postgres SQL built-in text processing capabilities, which are way better than the manual stuff.

# Demonstration: SQL Inverse Index
Hello and welcome to a walkthrough on building inverted indexes using SQL. So this is sort of an extension of the lecture. We're going to go into some more detail. Now the key is there is this file, the sample SQL commands for this lecture, that you can be working with, and that I'm expecting to have, and here's the URL for it www.pg4e.com/lectures/05-FullText.sql. And so you can go through this yourself because these commands just work. Every once in a while you might have to delete some rows or do whatever. And so I'm just going to go through and narrate these starting with this strings and arrays and rows. And I've got a Jupyter notebook that's already in the psql client. So I can do \dt and see I've got some tables. So I'm starting out with a pretty fresh, I've got rid of a lot of stuff, cleaned it all up. So let's take a look at some of the commands. So the first thing that we do is explore the string_to_array function. And that basically is like split in Python and it basically gives you an array of strings from a string and a delimiter. And then unnest, a lot of things happen that are cool like this. And so if we say SELECT unnest string_to_array Hello world, with a blank as a limiter, we end up with two rows, one Hello and one world. So it takes a array that's kind of a horizontal idea or kind of in a single column and it expands it vertically. generate is another there's, we did this in when we were building, filling things up. So, those two things the string_to_array and unnest, are the keys that we're going to do. So what we're going to do is we're going to make a table, we're going to create this docs table, and it's just going to have two columns, a SERIAL, which is a primary key, and then a document, which is text of unknown length, and then we're going to throw three documents into this. Just documents are text documents and where these are strings, so we just inserted the values. This is Python and SQL and I'm teaching stuff, those three lines have been inserted. So the idea is documents are a sort of big columns, text columns full of lots of stuff, We're going to start with smaller things, okay? So, this first breaking the column into one row per word and a primary key, we can run this. I'll run it and you will see what it does. And so, we're going to select the id, and the keyword, and so basically, we're going to go from docs, we're selecting from docs. And then we're going to take d.doc, which is the document, and then convert that, parse it using string_to_array and then unnest it vertically. And then we're going to name that column that we get from this unnesting as keyword, s.keyword. ORDER BY doesn't mean much, and so in the original SELECT I'm getting the id and this s.keyword, which is sort of the vertical expansion. So what this really does is this expands vertically each row and concatenates the id and the word. So row 1 has This, the next word is is, and then row 2 has more people and you can see. And so we've basically turned it into, from a table that had three rows into a SELECT statement that gave us, oops, 24 rows, which is one row for each word, but we haven't lost the document id, and that's the key thing that we're going to take advantage of when we do this. Now there might be duplicate rows because the same,
Play video starting at :4:4 and follow transcript4:04
the same keyword might be in one document twice. And so while we're doing is we're getting the mapping on id, so I'm just going to add DISTINCT.
Play video starting at :4:18 and follow transcript4:18
And then that just discards any duplicates where those words are twice. And so I still got 22 rows, but this SELECT DISTINCT is a cool thing. Now, I am going to make another table, docs_gin, which has a keyword, which is a text field, and then a foreign key into the doc id. So you'll notice that this text_gin that I'm creating is exactly matched to that little SELECT statement. Because it has a document id in that previous SELECT statement, and a keyword. And so, then what we're going to is the old basic trick of taking an INSERT and then following it with a SELECT. So we're going to insert a series of rows into the docs_gin. And using that SELECT DISTINCT that expands the document vertically, but then not losing the id. The ORDER BY doesn't really matter here. SELECT docs_gin and then SELECT DISTINCT really combining the last, the previous. And so that made our inverted index, so that just took that SELECT DISTINCT and it just inserted it into a table, right? So now we have this table and I'm calling it my generalized index, and I can just do a SELECT FROM docs_gin, and you see the exact same thing. Now we have a mapping between keywords and document id's.
Play video starting at :5:39 and follow transcript5:39
And so now I can ask for all of the documents, I'm going to add a SELECT DISTINCT doc FROM docs. I'm going to JOIN on docs_gin WHERE the id matches, and then look for a keyword. So this WHERE clause is looking down the keyword column in the index, and it's throwing rows away, and then I'm matching. So this will give me, Actually I should add an id on that. SELECT DISTINCT id, doc. That would make that one better.
Play video starting at :6:18 and follow transcript6:18
So now we see the documents that have the keyword UMSI in them. And so we used the JOIN through that little gin table, the docs_gin table, because we have a foreign key from docs_gin into docs.
Play video starting at :6:32 and follow transcript6:32
We can also use the IN instead of the equal sign, so we can say G.keyword IN fun and or people. And then that allows us to have more than one any of the above.
Play video starting at :6:48 and follow transcript6:48
That's what the IN operator is, it's like, here's a set, and it's kind of any, you could think of it as any. So part of what I'm doing here is I'm giving you lots of excuses to learn a little bit more SQL. And so I can even send a whole phrase, right? So if I say in this next one, ANY string_to_array 'I want to learn' broken into spaces, then what that does is that string_to_array is an array of words, and then ANY is any of these words. It's kind of like almost it's a little different version of the IN clause. And so I can actually have a whole phrase that string_to_array is going to break into pieces. And so we can find any the word I or want or to or learn, and we can find the rows that say that.
Play video starting at :7:36 and follow transcript7:36
Now, I'm going to do the same thing except I'm going to search for Lemons and Neons, and this is like why stop words are important. And you'll notice that I'm looking for the meaning of, when I say search for Lemons and Neons, the and doesn't contribute to the meaning of my search, right? I really was looking for the meaningful words or maybe search, probably Lemons and probably Neons, and I got two lines. I should probably have an id on that one too.
Play video starting at :8:13 and follow transcript8:13
I'll add an id to that SELECT DISTINCT on the side of this that can go sideways one. And so we found some documents but what they were matching is the word and, they didn't match anything else, they matched the word and because it's any of those words and search for Lemons and Neons. So we're going to do stop words next.
Play video starting at :8:33 and follow transcript8:33
And so, that's basically we've made an index and I keep doing this JOIN docs_gin so that the document id is equal to the docs_gin document id. And that connects the documents, but then I use a WHERE clause to only pull out the things in the docs_gin. So let me add another little SELECT statement here.
Play video starting at :8:56 and follow transcript8:56
Let's just do this one here, add a SELECT statement.
Play video starting at :9:5 and follow transcript9:05
Oops.
Play video starting at :9:9 and follow transcript9:09
Let me do it just without the JOIN.
Play video starting at :9:14 and follow transcript9:14
id, keyword FROM docs_gin AS G.
Play video starting at :9:27 and follow transcript9:27
So here I'm saying SELECT DISTINCT id, keyword FROM docs_gin AS G, WHERE G.keyword equals UMSI. This basically is telling me all of the documents that mention UMSI.
Play video starting at :9:41 and follow transcript9:41
Well, yeah, It's an id, it's called doc_id in that one.
Play video starting at :9:50 and follow transcript9:50
So one of the things I do when I'm coding is I edit things in a text editor and then I go paste them back and forth, and then I'm always getting them right. I'll flip the order of that, keyword, doc_id. So my ultimate query is SELECT keyword, doc_id FROM docs_gin AS G WHERE G.keyword equals UMSI. Okay, so you can see I got these keywords, but really I've got the doc_id. And so this is where I'm using a JOIN, and I'm doing the WHERE clause, reducing what I'm seeing from the docs_gin table, with the WHERE clause. And then I'm saying, oh, here are the documents. And then I just go use a JOIN to pull the actual documents out, because I have the id FROM the docs_gin. And so, there we go.

# Building a Natural Language Index with SQL
So up to now we've done inverted indexes that just treat strings like strings, but the real inverted indexes, if what we're searching is natural language, we've got to do some stuff that, or we can take advantage of some of the realities of natural language to make our indexes smaller and more efficient. So what we're going to do is just like we did before, first we're going to build by hand using SQL, and then we're later going to just do it with Postgres, and of course Postgres will be easy and the main reason to do it by hand is to better understand it. So the whole idea is that we're going to take advantage of what natural language is and the fact that the queries are supposed to have meaning, right? When's the last time you went to Google and typed in the, and, but, if, who, right? Those words we have in language, and they're important for the structure of our sentences, but they're not important for the meaning of the sentences. Because we use and and but and who in so many sentences, but they don't contribute to the meaning, they contribute to the structure of them. And so it turns out those words are, in natural language search queries, effectively useless. So why would we index them? And the term for those words is stop words, I don't know why they're stop words, it's not like that we stop. It'd be better to call them ignore words, for all we care.
Play video starting at :1:24 and follow transcript1:24
So we just do ignore words and we'd say here are the words to ignore, but we technically call them stop words and that's that. So stop words is one natural language optimization. The other optimization we're going to do is just make everything lowercase. So stop words reduces the size and again, generally, when someone types SQL uppercase or sql lowercase, they generally mean the same thing. And so teacher with the first letter capital T or teacher with not the first letter capital T, the meaning is the same, whether we start or end it. If you're looking for teacher stuff or teaching stuff, you don't care about capitalization. So we're going to lowercase it all and then we're going to make stop words. So if we take a look, for example, at this one query I've got sitting here, this SELECT DISTINCT id, doc FROM docs, and I'm searching for, Search for Lemons and Neons, you can see the uselessness of the stop words. And so it found things that had the word and in it but had nothing to do with Lemons or Neons. So there's sort of stop words and meaning words. And so we did this without stop words and we got a bunch of false positives in a way because the meaning of the resulting returned row and the meaning of the query didn't match at all. And the other thing we're going to do is what's called stemming. So the idea of stemming is where you have a whole series of words, so car and cars. Do you really want to separately index car and cars or auto and automobile? And so these, they're stems or lexemes, words that basically, what is the real word so we don't separately index. So it both makes our index smaller, but it also means that when we do it to the query and we apply the same thing to the query where we take automobile and auto and make it be the same stem, we're actually making it so it matches more documents because of the meaning. And so stemming is words with equivalent meanings. So a word and its stem, you map all the equivalent words down to the same stem and then that also reduces things. So, what we're going to do in this one is we are going to build our own manual index. In that manual index we are going to add a table of stop words, and then we're going to add a table of stems that map words in our text documents to their stems. We're going to have to map that both for the creation of the index phase and for the query phase. And then when we're all said and done, we're going to make a much better inverted index. So we're going to make an index that still has things like SQL is in three documents 1, 2, and 3, but case has been removed and there are no stop words. So how would we build an inverted index that maps everything to lowercase, handles stop words, and handles stems? And like before, in this dot dot dot part, I'm going to show you that in another demo. So I'll show you the whole dot dot dot. We'll walk through it all the way, we'll see everything, and that will get us to the point. I'm just showing you where it's going to go. The stemming and the stop words very much depend on which language and so you'll notice in this next part when we're going to do it all with PostgreSQL, it's really important to say which language it is. Now, this little next SELECT that just shows you which possible languages you have in SELECT cfgname FROM pg_ts_config, I think ts stands for text search in this. This shows in a default installation with Postgres, the number of languages that are precomputed, have precomputed stemming rules, etc., etc., etc., etc. Stop words and stemming rules and many other things, right? We're doing a very, very, very simple implementation but, like always, Postgres already has our back covered and has a bunch of things in there for us. But it does mean as you create data models you've got to remember what language it is because you might have to create different indexes for different information in different languages. So that's what we're doing is we're switching to like if it's natural language and we know what's language it's in, we can build much more efficient indexes. 

# Demonstration: SQL Natural Language Index
So now we're going to get a little more complex. We're going to take advantage of the naturalness of our natural language. And so let's take a look at the sample code. Again, this is in https//www.pg4e.com/lectures/05-FullText.sql, and you can download that. And then let's go and search for stop words.
Play video starting at ::30 and follow transcript0:30
So here we go. So the idea is we're going to move everything to lowercase, and we're going to have a set of stop words that are meaning-free, like and, and for, and whatever. So let me go ahead and drop docs with the CASCADE option, and then create the docs. The structure is the same. We could probably just delete the rows from it, but we'll recreate it just for yucks,
Play video starting at :1:2 and follow transcript1:02
and then we're going to insert some records.
Play video starting at :1:10 and follow transcript1:10
Insert the records in, so we got three records inserted, the same ones that we've been playing with.
Play video starting at :1:19 and follow transcript1:19
So this same thing that we've done here, we can break the columns into one row per word plus a primary key with the SELECT DISTINCT, the unnest and string_to_array of the doc column.
Play video starting at :1:39 and follow transcript1:39
But you see that some of these are uppercase and some of these are lowercase, and we want to kind of eliminate the uppercase and lowercase. So what we're going to do is, the only change we're going to make is we're going to take the string_to_array and we're going to convert the document to lowercase before we split it. And that's all we've changed, so we're going to lowercase it all. So we're going to convert the doc column to lowercase before we split it. And now you see all those keywords are lowercase. That already is going to make our lives a lot simpler. Okay? So we're going to create our docs_gin table, which is the same. It's got a keyword and it's got a doc_id that's a foreign key pointing to docs id. And now we're going to create a table of stop words. So I'm just going to put one column in the stop_words. It's a column called word, and then I'll insert into stop_words "is," "this," and "and." We don't need too many stop words, but you get the idea. So you can go Google on stop words, you'll find there are lots of lists of very extensive lists of stop words. We're going to keep it real simple because our text is really simple. So now what we're going to do is we're going to basically have a WHERE clause. And so you can sort of see what's happening here is this is the creating of the inverted index. And so we're going to add a WHERE clause to this thing we did before where the keyword is not in, and then a sub-select will select word from stop_words. So we're going to split the documents into words, lowercasing them, and then we're going to throw away the words that are in the stop word list, and then, we're just going to list them. So selecting DISTINCT id, s.keyword, and FROM docs with an unnest and a string_to_array, and WHERE they're NOT IN the stop_words. So you see that now that we've put that WHERE clause of the stop words in, a lot of words went away. Let me see if I can edit it so that I just don't even have that WHERE clause. So I do the same thing without the WHERE clause. This was what we did in the previous example. Previous example, you see we had 22 rows, and if we do it with the stop words, we have 18 rows. And so all the rows have been thrown away, We've also lowercased it, and so that lower is in there, and away we go. Now, all we're going to do is take that same SELECT DISTINCT, and we're going to insert that into our inverted index just like we did before. So we're going to INSERT INTO docs_gin and then the SELECT DISTINCT with the WHERE clause. So we end up with 18 rows in the docs_gin, and so we can do the same kind of a query. Now, we do have to convert like lowercase. This query is the same as before. All we did is we didn't put as many things in the stop_words. Okay?
Play video starting at :4:33 and follow transcript4:33
So you can see something like I can do the same thing we did, where you can do a multi-word query, but let me go to this next one with the stop word query where I'm looking for a keyword of and, and now I'm seeing no rows. Because, in a sense, what we're doing is we're going through the docs_gin, looking up the keyword, and then we're joining into the docs table from the docs_gin. Because the WHERE clause doesn't find the keyword, because the keyword isn't even in the docs_gin table, so we never put a stop word in the docs_gin. We did all the work of stop words as we constructed that table. Okay? So that's stop words. Now, another important aspect of natural language is what's called stemming. And in stemming, we transform certain words into their stems. In this particular thing, I'm going to say that teaching is stemmed down to teach, and teaches is stemmed down to teach, and teach equals teach. right? So these are all words that we're going to take that are slightly more complicated or flowery ways of the same concept. And so teaching and teach are the same concept, and so we map them from the words. And we could say instruct maps to teach, even. So what we're doing is we're stemming. So I'm going to insert a two-tuples, right? First, I've got to to create the stem table with a word and a stem. So the first column is the original word and the stem is kind of the reduced version of the word. And the idea is it works if multiple words map to the same stem. So let's go ahead and make that. So I've got two rows in my docs_stem table, and so we're going to do the word extraction, and we're going to move it into a sub-query. Just to make it a little clearer what's going on, we've got this SELECT id, keyword FROM sub-query, SELECT DISTINCT, etc. And so there we go. And what I'm going to do is I'm going to join that, right? I just wanted to show you that. So we've got this SELECT DISTINCT id FROM keyword, right? That gives us the IDs and the keywords. The stop words have already been taken out, and by taking that as an AS K, what that creates is something I can join on, and then I can join with what's called a LEFT JOIN onto the stem table. AS S just gives me a nice thing to say, I'm interested in the keyword in the gin table matching the word, which is the leftmost column. It's probably just as easy to show you this because we're going to get three things. So what's cool about a LEFT JOIN, so I looked up all of these things, id and keyword are from the docs_gin, and then you see stem over here. If this was not a LEFT JOIN, it would only show me the rows where there was a match. So what I did was, I looked up in the stemming table what the stem was for the word teaches, and I looked up in the stemming table what the stem was for the word teaching, and they're both teach. But the rest of them are blank because and, and other, and from, and should, they didn't have stems. And that's just because I didn't make stems, but you can see this LEFT JOIN allows us to do a conditional lookup without losing all the data. And then what we're going to do is we're going to is we're going to, in a sense, prefer the stem over the word. But and, and fun, and is, and other, and python have no stems, so we're just going to use those words, okay? So we're going to look this up if it's there, and if it's there, we're going to use it. We're going to look teach up. If it's there, we're going to use it. Okay? So let's take a look at how we do that.
Play video starting at :8:42 and follow transcript8:42
So now instead of just selecting id, keyword, and stem from the two tables, I'm going to say SELECT id and then a CASE. CASE WHEN stem IS NOT NULL, give me stem, ELSE, give me keyword. And AS awesome gives it an alias. And then I'm going to also pull out the keyword and the stem. The rest of this, LEFT JOIN and all that, that's from the previous example.
Play video starting at :9:16 and follow transcript9:16
So there we go. So awesome is in effect a derived column from keyword and stem. When stem is not null, take the stem. So in this line here, where teaching line, I use the stem. In the line like stuff right above it, there is no stem, and so I use stuff. And so this is the way CASE when stem is not null, then stem, ELSE, keyword. This is sort of if-then-else, but it's a data flow-based if-then-else. I love this, it's really a cool way. It's not like if, do whatever, it's a case. It's like when this happens, choose this value. When this doesn't happen, and apply it to every row. Just like most things in SQL, there's an implied loop. Okay? So that's one way of preferring the stem over the keyword. Okay? And there's even a prettier way to do that and it's using a function called coalescing. And the idea of coalescing is that it's a function that takes any number of parameters and it returns the first non-null parameter. So if I say SELECT COALESCE NULL, NULL, 'umsi'. Let me just grab them both and run them both, just to show you how they work. So if you say select NULL, NULL, 'umsi', it gives you do umsi. If you say SELECT COALESCE 'umsi', NULL, 'SQL', you get umsi. It returns the first non-null value and ignores the rest of them. It'll go through as many null values discarding them as you like and then find a non-null value. So we can actually express in a bit more elegant way in this next query, SELECT id, COALESCE stem and keyword. So what I'm basically saying here is COALESCE stem, keyword, the keyword's always going to be there and stem might be there, but stem might be null because I did a LEFT JOIN on docs_stem. So if the stem is there, I'm going to hit stem. If stem is not there, I'm getting a null because the LEFT JOIN gives me a null in rows where there was no match. And so COALESCE just sort of flattens that down preferring the stem with a fallback to the keyword. You can almost think of this as like a get in Python where you have a default value, but it's not. So this is a very pretty way, this COALESCE is a prettier way of grabbing the index, basically. And so you'll notice that teach shows up several places and teach was never in our original thing because we used teaching and teaches and mapped them both to teach, which is their stem. So the idea now is we can sort of go through and look up keywords with stems.
Play video starting at :12:21 and follow transcript12:21
So now what we're going to do is we're going to take that SELECT statement and we're going to insert it into docs_gin. And so we're simply going to take this stemmed, we're going to make a reverse index of stemmed words. Not the original words, stemmed words. Stop words are gone, I think, in this, yeah. No, stop words are not gone. That's going to insert just the stemmed words.
Play video starting at :12:50 and follow transcript12:50
Now we're going to actually add, probably I should take that out.
Play video starting at :13: and follow transcript13:00
Really it's most fun if we do the stop words and the stems. So I'll just delete from docs_gin and now I'm going down to the one where I'm doing the stop words and the stems, INSERT INTO docs_gin, SELECT id, COALESCE stem, keyword from this giant sub-select. And this sub-select has the WHERE s.keyword NOT IN SELECT word FROM stop_words, sub-select within a sub-select, which that's going to throw away the stop words, and then we're going to take that whole thing as K. And then we're going to LEFT JOIN it into the stem, and then we're going to COALESCE the stem and the keyword together so that we will end up inserting stemmed words. So this is stop words and stemmed words all in one glorious, coalesced, left joined, sub-selected magnificent awesomeness. So now, if we say SELECT star FROM docs_gin LIMIT 10, we will see stemmed words, we will see stop words handled, and stemmed words. We have docs_gin has our inverted index, which is a pretty, pretty thing. Now, the key is when we're building a WHERE clause, if we are looking for a word, we have to also look for the word or the stem of the word and we have to prefer the stem of the word. So if we look at this SELECT COALESCE and then we have two parameters. The first parameter is a whole sub-select. I might remind you that sub-selects are inefficient but we're data miners, so we're less concerned about efficiency and more concerned about awesomeness and really getting Ninja Warrior good at SQL, which we're doing. So you can see this COALESCE has two values and one is a lookup to see if there is a stem for the word SQL, we're mapping it to lowercase, and if we don't find a stem, fall back just to SQL. So we've got the stemming here
Play video starting at :15:11 and follow transcript15:11
and the going to lowercase all happening. And so that particular one had no stem, so SQL has no stem. But if I do the same thing with teacher, you see that I get the stem. So COALESCE SELECT stem FROM docs WHERE word to lower equals teaching, and then fall back to the lowercase version of teaching, I've been coalesced into teach. So this way I am using the stem if it's available and using the word if the stem is not available. Now, we don't actually have to worry about stop words because stop words have already been removed before we created the inverted index. So it's never going to find a stop word. Stop words they're as if they never we didn't pull them out of the documents in the first place. We pulled them out of the documents, but then we didn't put them in the index. So that's how stop words work, they just block things. And so if we want to do this stem in some queries, we can just do a SELECT from the docs, joined through the docs_gin where the keyword is, and then this COALESCE statement to do the stem lookup and prefer of the stem.
Play video starting at :16:22 and follow transcript16:22
And so we find three rows that match the
Play video starting at :16:34 and follow transcript16:34
And if we do the other one with a SELECT DISTINCT id, doc where the keyword is the coalescing of the stem for teaching or teaching itself, in this case, the stem is going to work and we're going to find the ones that match the teaching stem. But the original documents have the word teaching in them and teaches, and so that way, we looked for teaching but we also found teaches, and that's because we stemmed them down to the same word in our index and then we reconstituted it all because we're looking at the original documents, not the post-stem documents, okay? The technical term for converting stems, search terms to their stems, is called conflation. I've always wanted to use that in a sentence. So I hope this has been helpful to you in understanding how stemming and stop words work when building natural language inverse indexes.