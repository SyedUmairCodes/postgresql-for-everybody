# Lecture: PostgreSQL and Python
Welcome to our lecture on Postgres and Python. Up till now we've been typing SQL commands through pgsql. It's quite easy in Postgres to, in effect, simulate what pgsql is doing, because pgsql is just a client. The Postgres server is somewhere else on the network. It's got an IP address and an ID and password. We log into it, and then we send commands to it. pgsqlL is just one piece of software, one of many pieces of software, that's capable of taking SQL commands and sending them. Another piece of software that can do it is actually Python. And so and now we are going to and Python can handle text so well. So we are going to do a series of things where we actually sort of use text, read text with Python, maybe talk to an API, parse that, and then put it into a database. And we can actually clean up data. We've done data cleaning in SQL, but when you get to Python, then you have a lot more power than you do in SQL. So the first thing, let me zoom this up a little bit. Now the first thing to know about Python is that you have to have this psycopg2. That's a Python Connection to Postgres or something like that. And then with that you're going to then make a connection. This connection is basically logging in to the Postgres server. That can fail, you might have a bad ID or password, or have the host name wrong. When we do this on my online examples, we'll have the file hidden.py so you don't have to actually have the secrets right in that file. Now you can send commands through this connection, but we tend to want to maybe have in a sense more than one client at a time. So we use a thing called a cursor. A cursor is a lot like a file handle, or a socket if you've programmed network sockets. And so you actually say to the connection, you say, hey, connection, give me a cursor. I would like to start sending SQL commands. And then you start sending SQL commands by typing. The cursor is an object, then you call the execute method in that object and you start sending it strings. And literally, they're just strings. It doesn't have to be constants, they can be strings that you concatenate together to make more things and then you can put substitutable parameters, which we'll quickly see. And so here I have a get me a cursor, drop a table, here's some more cursor execute. Now you can use the cursor over and over again. But now what we're doing is we're actually sending a command, and then after that command is done then we get back in the cursor what's called a record set. And then we can iterate, loop through, by saying give me the next row, give me the next row. So think of how you type a SELECT command and then it brings you a bunch of rows. Well, in Python, you do a select command and you have a handle through which you can see the rows because there could be one row or thousands and thousands of rows. So you tend to have to write a loop unless you know you're getting exactly one. So fetchone says get me the next row, whatever that is. Now a row is a tuple, and if there are no rows, you get none back. And so this is very simple. You can take a look at that sample code. And the first really big sample we're going to do is we're going to read through the text of a book. And this is from the Gutenberg Project. Funny thing, it's called PG. Postgres is PG, but the PG 19337 is one of their free and open text books. This happens to be Charles Dickens Christmas Carol, I think. And we're going to read that as a text file. And then we're going to load that into a database and then build a full text index and then play with that full text index. And I have instructions this loadbook.py. I'm going to do a walkthrough for you in that. There's some utility code that I give you in myutils, there is a hidden.py that puts the secrets in. And then you're going to grab the text by downloading that with a wget, then run the code. The code simply, what the code does is it looks through the file and takes paragraphs, like a paragraph here, there's this paragraph here. And it takes multiple-line paragraphs and strings them into one long text line and then throws away extra blank stuff. And then puts that in a database and puts it in a database that basically has a single field called body, which is a TEXT field. And so this ends up with effectively one row per paragraph in this book, and then we play with it. As the thing is running, as this loadbook is running, you can watch using a Postgres psql in another window. And you can watch it. It actually commits every 50 records, so you'll see that once in a while. And then we're going to play with it and we'll just make a ts_query. And this is really just a different way of getting and filling up a significantly larger database with some interesting information. And then we play with queries and I'll do this all in a nice little runthrough. So, in the next lecture, we'll talk a little bit about probably the most complex thing in this particular week. And that is a application that I wrote that loads a bunch of email data from an online source.

# Demonstration Python and PostgreSQL simple.py
Hello and welcome to another sample code walkthrough for our Postgres class. We're currently working on connecting Python and Postgres. And so the first thing we're going to do is and by the way you can get the sample SQL that I'm using. I really prefer to run through this by simply copying and pasting. After the class, you can keep these SQL files. They're great examples, I refer back to them myself because once I figure something out, sometimes it's like put the two parentheses around it, and like why did I do that? But I would keep these. So this file's at www.pg4e.com/lecture/06-python.sql, and I always put comments at the beginning of these files. So first thing that I want to show you. So what I've got going here is I've got one screen that I'm going to one tab that's going to have my copying and pasting stuff, one tab where I'm going to run my Linux/Unix Commands and run all my Python code, and then one tab that's connected to my database. And you may need another tab that has your tunnel if you're behind IP filtering. So you may need a fourth tab, but I'm just all set. So the first thing that I suggest that you do every once in a while is just do a dt command in pg4e. And you see I've got a couple of databases, docs_stem and stop_words, and I should probably clean those up. So you just say DROP TABLE docs_stem Oops, I can't type docs_stem, CASCADE. Something was wrong there. Okay, DROP TABLE docs_stem CASCADE. CASCADE just means if there's any foreign keys, it wipes out the other tables as well. stop_words. And now I'll do dt. Now, don't delete any of these py4e_debug, meta, or result. Oh, I've got to get rid of this pythonfun too. DROP TABLE pythonfun CASCADE.
Play video starting at :2:21 and follow transcript2:21
So now I've got three tables left, pg4e_debug, pg4e_meta, pg4e_result, and these are all things that the autograder is going to use and/or needs. So if you mess these up, there is a early assignment that gets them set back up for you. So it's not fatal if you delete them, you just got to go back and get them fixed, otherwise the autograders are going to be unhappy. They're also a way for you to look at results of the autograders. We're not doing autograder right now. I'm going to clear my screen and so let's get to this. So the first thing is you've got to have a connector, what's called a database connector, and it's got to be a piece of code installed in your Python. And you can check to see if it's installed by just saying going into the Python
Play video starting at :3:13 and follow transcript3:13
and saying import p s y c o p g 2, is that what this is? I couldn't remember the name, psycops, it's what it seems like to me. And if that works, then you're good. Don't run this pip command. But if it doesn't work and this is the part where it all depends on how your Python is set up and everyone installs Python in a weird way. If you've got an Anaconda or this, I actually don't even use virtual environments and people tell me I'm terrible, but my Mac is nice and clean and I've got a brew-installed Python3. So I can just say pip3 install psycop2 and it says pg2, says I've already got it. So I'm good. That means that, and of course what will happen is these lines, this code will blow up in simple.py. So let me go grab the simple.py code. I just used a wget to https.www.pg3.com/code/simple.py and that in effect downloaded that file. So I can edit that file simple.py and so we see that. The next file I'm told to get is a wget of pg4e/code/hidden-dist.py. Now, the key thing here is if you take a look at hidden-dist, in general when people build code and they're going to give you like sample code, hidden-dist is copied to hidden-dist.py and so the file that really matters, the file that's really going to be used, is called hidden.py and you can move it or you can copy it, hidden-dist.py and then the idea is that hidden-dist has sample data but hidden has your real data. Now I'm not going to show you my real data. I have my real data in a folder right next door.
Play video starting at :5:31 and follow transcript5:31
Oops. Copy hidden.py dot. So I'm copying that, yours won't be in dot slash code and so now sitting here I've got hidden-dist.py and hidden.py, hidden-dist has the sample connection data. That's the connection data. It's same connection data that you're sitting using over here to connect to your database. It's all in there. I'm not going to show it to you. And if you take a look at simple.py, we're importing it and then all of the secrets in this connection. So when you do this psycopg2 connect, you've got to tell it the host you're going to, the database that you're going to, the user that you're going to connect with, and the password you're going to connect with. And that connect timeout is just three seconds. So what happens is this. So this secrets, that's just going and grabbing the secrets, which is just a dictionary, out of that hidden.py file and then I can look it up in that dictionary the keys that were in that file. Now, a connection in Python is, it's not exactly the same as
Play video starting at :6:44 and follow transcript6:44
it's very similar to in a sense this connection, but you don't, when you type commands you get what's called a cursor. And so if I do a \dt in this case and I say SELECT star FROM pg4e_result just for yucks. In a sense, what we would do is we have this connection, but then we would create a cursor and then to use the cursor to send the command and then the cursor would then allow us to read the records that come back. That's too long.
Play video starting at :7:30 and follow transcript7:30
So the cursor would take this command, SELECT title FROM pg4e_result, it would send it to the database, then we would get back in a sense a handle and then we would loop through and read all of the records that come back in the record set. So you see this where we make the connection. Now, the connection can blow up if you have the wrong ID or the wrong password, or the wrong host or whatever, but really if we want to send commands we call it, ask for a cursor. So we say, hey connection, give me a cursor, and a cursor is what we send SQL commands to inside of Python. So I'm just putting SQL, this DROP TABLE IF EXISTS pythonfun. You'll see that I drop often when I'm writing Python code. If I really wanted to start fresh, I'd drop the table and then I'd create the table, as compared to making myself manually drop or create the table. I'm going to print it out and then I'm going to execute it. So the execute, that actually sends it, and sends it to the database. That could blow up if I made a typographical error here, and we'll see a mistake in a second. It will blow up, that execute will blow up. And I'm going to create a table. I'm going to just get a autoincrement serial and a text, and I'm going to run that one. So that's sending that. And then this conn.commit, we do this on the connection object, it basically flushes it all to the database server. Sometimes the connection and the cursor to be efficient it doesn't actually send every command instantly. It kind of queues them up. So commit says look, you may or may not have sent all of these commands, but I want you to send them now. I'm going to wait. So when you see this commit, what you might see is your program running and running and running and you do a commit and it pauses for a second, and it's forcing all that stuff in the database, and the commit does not finish until data's actually written to the disk in the database, which is really quite nice. And then I'm going to do insert 10 records. The first thing that you see here is that there is a percent s, this is a substitution parameter and INSERT INTO pythonfun line VALUES percent s. The percent s is a substitution parameter, and if I'm going to execute that SQL, then I send it a tuple with a one-to-one correspondence. That this variable txt, which is a string, is one-to-one correspondence to these percent s's. Now, this little txt comma parentheses, this is the weird thing we do in Python when we have a one-tuple. We'd go percent thing comma close parentheses, and there's only one of them. But that's the way it knows it's a tuple. And so it's just one tuple. We'll see later when we're at more than one of these things it actually makes more sense. Here's the SQL with the substitution areas as the first parameter to execute, and then a tuple, which is the variables that represent the substitutions. And then I'm going to commit. So these four inserts might actually not be sent to the database, it might just kind of remember them. But when the commit happens, then all those 10 things are going to be there. You can do a SELECT statement. Right? So we send a SELECT statement and print it with a WHERE clause, etc., and I do the execute. Now what happens is then at that point this cursor, once I've executed that SELECT id line FROM pythonfun WHERE id equals 5, then cur is like a handle and I can read it. And the fetchone method is kind of like read one line or read one row, as it were, because you can, in this case, I'm asking for two things, id and line, and so row ends up being a tuple. row sub zero is id and row sub one is line. But if there was nothing, then fetchone will give me back none. Otherwise, I'm going to print it out here. Okay? So that is kind of a return value. That's the pattern of a return value or a return set of records. So the fetchone, and we'll do this later, we'll do it over and over and over again in a loop until row is none and then you've kind of hit the end of your record set. And the whole thing is very efficient and it might call the database once, it might send a network request once or twice, but it's all optimized and you don't worry about it. You just say let me read through those returned records. Then what we have is insert, another insert, where we're going to insert another line with this RETURNING id, and that's where we want to know what was the unique key in the serial column that was assigned. And so then we just give it some more text and we're actually going to send a second line twice, last line twice, and then this insert actually returns a record set and we fetch that record set and then we take the sub-zero element of it, and then that turns out to be the id. Then the next thing this thing is going to do is make a mistake, where this is a syntax error in the SQL to show you how the cur.execute actually blows up. Now this is going to die right here. So it'll never get to the point where it does the connection commit and then cursor close, which is a good way to clean things up at the end of this program. Okay? So let's go back here and just before we start this program. So this is one of the nice things is when these Python programs are running you can simultaneously watch what they're doing. This one here is going to run so fast we can't stop it in the middle. Some of of your later applications are going to run slowly and you can watch what they're doing as they're talking to the database. So the first thing we notice is there's no table here. So now we're just going to run that python3 simple.py. So it dropped the table, it created the table, it inserted all those rows, it selected an id line where id equals 5 and it found the line 5. So I can go take this one over here and I can run it. So you see that's what that SELECT id line FROM pythonfun WHERE id equals 5 gives me back a row 5 that says Have a nice day 4. And you can see I see that same thing. Then I insert a new record. So if I do a SELECT star over here in my psql FROM pythonfun ooh, I'm always transcribing my 'o' and my 'r' FROM, you can see that Oh, no. What happened? Oh, look, see this new id 11? That's pretty awesome. See new id 11, and then I blew up? Let me show you what happened there. Because the thing that freaked me out as I did a SELECT star FROM pythonfun and I do not have a row 11 in my database. Can you guess why that is? I'm going to bring up the simple.py.
Play video starting at :14:44 and follow transcript14:44
So I did this INSERT and I've printed out the new id. But somehow, when I go look in the database, it's not there. Pause the video if you want to think about it for a minute. Just stare at it. Okay. You can hit "Pause", now I'm going to show you. So this is an example where the SQL executed. But because this blew up, it never did the commit. And what happened was both the SQL for the INSERT statement and the SQL for the SELECT statement were kind of running and this ran but it didn't actually flush it to the database until we said conn.commit. So this is an example where this insert sort of happened but it didn't happen, because it didn't get flushed, because it didn't conn.commit. So you got to be careful where all of a sudden, you're like I'm really surprised that my data didn't end up there. And you'll notice that what I tend to do is not sometimes you have a thing called autocommit, where you're going to put a commit after every one of these execute statements. That's a bit of overkill. So you'll notice in some of the things, I commit every 10 inserts or every 50 inserts or something like that. Otherwise, it'd be a lot slower and you'll see when it's doing a commit and we run some of those other bits of code, it's going to work. So there we go. A little bit of simple Python just to sort of see what the basic idea is of talking from Python to PostgreSQL.

# Demonstration loadbook.py
Hello, and welcome to another code walkthrough in Postgres for Postgres class. So we're going to go through now the loading of the text of a book and we're going to be using Python, and we're going to be using a bit of code called loadbook.py that I wrote. So the first thing we're going to do is we're going to grab ourselves a book using wget from gutenberg.org. We're going to grab a pg19337.txt and I'm just going to download that. ls -l. If we take a look at this pg, and the funny thing is that pg is for Postgres but pg19337 actually stands for Project Gutenberg. And so this is a Christmas Carol from Thomas Dickens. So what we're going to do is we're going to write some code that's going to grab out the paragraphs and then merge all these paragraphs into one line and insert those paragraphs into a database so that we can add a full text index to it. So it's going to have some definitions about what a paragraph is, etc., etc. One of things we're going to do is because we're using ts_vector, we are not going to worry about punctuation because actually ts_vector knows all about that. So that's a local file with 186,809 characters. So then we're going to grab this loadbook.py using wget. loadbook.py. We'll pull that one down. And then this one uses a bit of library code called myutils.py. So we'll wget pg4e.com/code/myutils.py. So I have all that downloaded. Now, it always is going to remind you that you've got to fix hidden.py. But you don't have to fix hidden.py if you just finished the previous example. So you don't have to keep editing hidden.py. Then we're going to run it. But let's take a look at the code first and you'll see how this works.
Play video starting at :2:4 and follow transcript2:04
So loadbook.py. So here's loadbook.py and it's going to tell us how to do all this stuff blah, blah, blah. So we import the hidden stuff and we're going to use the time and we're going to ask for a bookfile, then we're going to split it and take off the txt because we're going to automatically name the table the bookfile minus the suffix. We're going to make sure we can open the text file before we bother even opening the database connection. Then we load the secrets and make the database connection just like we did in the previous assignment and then we grab ourselves a cursor. So our cursor is effectively the same as our client, our pgsql client. We just almost do exactly the same thing. So now that we have a cursor, I'm going to drop the table. I'm going to use the name of the book as the table and then I'm going to create the table with a SERIAL and a body of TEXT, right? Just two columns, one is a big text and so I do that. Now, this here's a bit of Python. And we're all suppose to know Python already. I got some counters. I'm doing things like throwing away blank lines. One of the things I'm doing, I guess, I've got to open another pg, one of the things I'm doing is I'm taking multiple lines until I find a blank line and I'm kind of concatenating them together and moving a new line. So it's like one big long line like this. I'm joining them all together, one big long line. I just did that on my text editor to show you what I'm doing. And so one of the things I'm doing is I'm looking, do I see a blank line. If I'm in the middle of blank lines, I throw them away. If I'm in non-blank lines, I concatenate them together. And if I find myself on a blank line and I've got a para, the para is my variable for accumulating a paragraph. If I hit a blank line, then I take insert, then I insert the paragraph, right? So I insert it into the table name with a body and a substitution parameter percent s base in a body VALUES percent s and I cur.execute the thing go ahead and run that SQL and pass in a one-tuple in the weird format parenthesis para comma paraenthesis which is the weird format for a one-tuple. And I'm counting how many times I found a paragraph. So here's where I'm going to do a connection.commit. Now if you made this connection.commit every time through, you'd actually run a lot slower because you'd retrieve something, read something, and then every insert would be a commit. So I'm going to make it so every 50th record I commit, and then every 100th record I'm going to tell it how many was loaded and then I'm going to sleep. And the reason I'm going to sleep is to allow, I put this in a lot so I can blow it up. So I hit Control-C in this particular situation. And that just gives me a chance to slow it down if I've got a runaway train. So then this part here is accumulating. The strip is taking out the newlines and throwing away whitespace at the beginning and end and then I concatenate it. This is what's concatenating, para equals para plus quote space quote plus line. That's what's actually concatenating all the lines together to give me one very long text string. And then when the loop is all completely done, then I commit the connection and I close the cursor and then I print out some loaded messages and tell me what to create the GIN index which I've got here also in my SQL. So that's the basic idea. You can look at it as much as you want. So let's get out of here and just say python3 loadbook.py. Now if I hit Enter, it's just going to assume that pg19337.txt, which is the one I want to do. So now as you're going and so I can go over here and I can say dt and I can say SELECT COUNT star FROM pg.
Play video starting at :6:39 and follow transcript6:39
SELECT, I can't. I wanted to catch it while it was running. This gets to 700. It's at 800. OK, I did catch it while it was running. If you come back over here you'll see it's 100 loaded, 200 loaded, 700 loaded and it was pausing and it was committing. And you'll notice that the numbers that you see here are all even multiples of 50. I didn't catch it at 50. It does wait every 100 but it does commit every 50. So that's why you're not going to see like 143 because it's not going to commit until record 150. And that's this bit of code right there. Every 50th record it actually commits, which means the inserts come to the database in like blocks of 50. This greatly improves the database's performance. And the amount of network traffic that goes back and forth. Okay, so at this point, it's loaded 814 paragraphs. If I do that SELECT COUNT, you see that there's 814 rows. So let's take a look a bit. If I say SELECT star FROM pg19337 LIMIT 1, that'll be kind of long. Oh, that was my first. Let's do LIMIT 5. Yeah, you see how these paragraphs are getting kind of long. Let's go LIMIT 10, see if I get to a really long one, limit. No. LIMIT 20. Okay. Some of them get kind of long. So that's one paragraph and it was one Python code that built all that stuff. Now, if you think about this as an inverted index, if you just broke this on spaces, Christmas comma would be something, right? double quote who would be something, kindness dot. If that was just a pure inverted index based on words as defined by spaces, that would be a problem. Okay? And so the good news is that Postgres knows everything there is to know about this and we are simply going to say, let's make an index
Play video starting at :9:4 and follow transcript9:04
using to_tsvector of the body using the English language. And so that knows everything. What punctuation is in English, it knows, you know, is kindness and kind map to the same stem. And all that magical stuff is happening, okay? And so now let's just see what we're going to do. So select where the tsquery. We're going to look to see if goose is somewhere in the body, right?
Play video starting at :9:41 and follow transcript9:41
And so there it goes. LIMIT 5. Let me add the id from that so you see it a little bit better. So we can see which paragraph it was. SELECT id, body FROM pg19337 WHERE to_tsquery english goose double at-sign to_tsvector english body LIMIT 5. So here we go. So you'll see in paragraph 412, 415, 428, 429, and somehow there's a goose everywhere. I didn't know if we'd find it, right? So there we go. So it's there. Now let's check to see if the EXPLAIN works. Sometimes this EXPLAIN takes a while for the GIN to catch up. But now it's done. And so again, whenever you are doing an EXPLAIN, it looks really complex and all this stuff is useful to somebody somewhere, I'm guessing. You just don't want to see sequential scan. Your sequential scan is what slows you down. It's kind of like in order of algorithms. If you see order n-squared, it slows you down. But we've got a Bitmap Heap Scan. It says that it's using pg19337_gin, which is exactly what we expected, okay?
Play video starting at :11:2 and follow transcript11:02
We can, in this one, see the thing where we do a to_tsquery of tiny followed by tiny tim. The less than dash greater than means followed. And so now we can ask for that. tiny followed by tim and there's 21 bodies where the word tim comes after tiny. It doesn't have to be exactly right after it at that point. And we can do an EXPLAIN on that to make sure that that uses that query. That's a pretty complex thing because that's tiny somewhere after that is tim and so can that use the, oops, I didn't copy it.
Play video starting at :11:56 and follow transcript11:56
I need to explain that, yeah, okay.
Play video starting at :12:13 and follow transcript12:13
Yay! EXPLAIN ANALYZE with a tsquery of tiny followed by tim. But you'll also notice something here that this tiny has been stemmed, right? So it's not t-i-n-y, it's t-i-n-i. So that's cool. Okay, so that kind of gets us through the effort of just kind of loading a book in, using Python to concatenate all the lines of a paragraph into one big long line, making a real simple vector, and then being able to do efficient searches on all that stuff. So I hope this walkthrough was useful to you. Cheers.

# Lecture: Mail Archive
So the next Python sample that we're going to take a look at is working with some email data. Now, I've got some code for you to download gmane.py, and myutils.py, and datecompat.py. We're going to play with some regular expressions, we're going to do some indexing, we're going to do some looking, and we're going to again make some full text databases and do some clever queries. So most of it is actually in the demonstration, which we've had and I give you the SQL commands. The basic idea here is we're going to retrieve again from an online source mbox.dr-chuck.net/sakai.devel, and we're going to be able to pull a series of mail messages down. And mail messages are in a format that's called Mbox. Mbox is an interesting format. It starts with a From followed by a series of headers, which are key-value pairs which is like the key is up to the colon character and then the value is after the colon character. In this case, I went from message four through message six so I got more than one message back. The messages are delimited by a From space. Now you'll notice the From colon is not the delimiter, that's actually just one of the keywords of the header. The other thing is that there is a header that is a series of the key-value pairs and then there's a blank line. And it's as simple as that. There's a blank line and then there is the message text. And blank lines can be in the message texts. You know that you're in the new message when you see From in the characters. Now if there's from, it actually kind of escapes that in the message body. And so this sample code is pretty cool. It's a much more sophisticated. The key thing this is it's like a web crawler and then it uses the database as like a scratch storage to spider it and pull the data in and out. And it can be stopped, it can be restarted. If the server starts having problems, you can sort of stop your crawling process and then figure out what's wrong or what's wrong with your network and then start it back up, and it's pretty cool. And the other thing that it's doing is it's cleaning up the data. And so this is real email data that was just archived right off of a real email server and the header conventions and how addresses are represented, they are a little bit differently. And so one of the things we're seeing in this, and you will see in this code, is the code to clean up different formats of dates and times in different formats of email messages, etc., etc., using regular expressions. And these are just things you can't do in Postgres, but you can do pretty naturally. Now I'll just I'll let you know that when you look at this code, this gmane.py, this wasn't code that I sort of thought through for a few hours and then wrote for a few hours, this took weeks, like two weeks of work, where I was talking to the server and having things blow up and then I would evolve the code. Now the code you're going to use is pretty robust, and it's pretty resilient when it faces errors, but I didn't anticipate before it all started every error that I could possibly have made. And so when you write this code, you start writing a basic thing and if nothing goes wrong it works and you've got your data and you do your analysis. If you just find problems of data inconsistency or server unreliability or rate limits, then you might have to adjust your code. So some of these that I've written, they get a special code when you hit the rate limit and you're like, "Oh, saw a rate limit. Stop, might as well stop, wait a day and go get some more data." Or whatever. So it's an interesting kind of programming exercise in that it's an evolutionary programming exercise where you don't always know what's going to happen when you start talking to external data sources. And then the part of the idea is to get all this data in a nice pretty, consistent format, email addresses, dates, names, the headers, the body, and all that stuff in a way that you can start your data analysis and not worry about all of those crazy vagaries. So take a look at me walking through the sample code. It's quite a long run through the sample code and then what we do with the data once we have it in a database.

# Demonstration: Mail Archive 1/3

Hello and welcome to another walkthrough of some sample code in our Postgres class. So now we are playing with loading email data. This is probably the most complex bit of code that I'm going to use in this class. It's a bunch of Python code that does a lot of cleaning. It's dealing with some real data. This data is actually from 2005 that I have got captured from a mailing list from my open source Sakai project. And so, mail is a weird format, it's a format called Mbox, that the interest it's a simple format From, space is the delimiter Literally, From in the first column, space, is the delimiter. And then there's a series of headers with From colon, Message-ID, they're all colon. There's continuation lines, and then there's a blank line and then there's the body of the message. And I can get to these sort of in groups or one at a time. And I'm going to write a bit of code to sort of pull a whole series of these things into a database, and then play some full text stuff with it.
Play video starting at :1:8 and follow transcript1:08
So let's take a look at the code, here we are at pg4e.com/lectures/06-Python.sql. And we are on this email corpus one. And again, it's going to pull from inbox.dr-chuck.net.sakai.devel We're going to grab from pg4e.com/code gmane.py, datecompat.py, and hidden.py. I hope by now you've got hidden.py. And so the program is pretty self-contained. Make sure that when you start it you don't have a table named messages. So you shouldn't have a table named messages. Drop it if you are getting started. But after that you just say, python gmane.py, and I'll go ahead and run it. And it just asks how many? Now this is a spider web crawler style thing, and so it's asking me how many, and I'll just say, go get me 100 messages. And it just starts retrieving them. And so it's just doing it and it's showing you and it's pretty quick, you'll notice. Now if I go in here and I say SELECT COUNT star FROM messages.
Play video starting at :2:31 and follow transcript2:31
There is 100. Let me get another 100. Oop, not 1000, 100. So you'll see that this count doesn't go up. And that's because again, I'm not committing. I'm pushing rows. Still 100, still 100. Now it just went to 150. It's committing them 50 at a time, just like I did in similar examples. So this is running pretty well. You are supposed to pull in about 300 messages. I'll go about 200. This is a very fast server, it's highly cached, and so you can beat the heck out of this server. A lot of APIs have rate limits. This particular one, inbox.dr-chuck.net, has no rate limit because it uses a really cool technology called CloudFlare, which is free, infinitely scalable caching, so it's super fast. And I use this in my Python for Everybody class as well. Okay, so that's running. And let's see, we got 250, it'll run for a while, so let's take a look at the source code. So the the main code is in gmane.py, and it is a bit of complexity in here.
Play video starting at :3:41 and follow transcript3:41
The biggest complexity of this is that I am spending some time trying to make this code clean the data up a little bit so that my work inside the database is less difficult. And the problem with all this email stuff is that it's doing different servers are involved in it. There are slightly different ways like this From address, you'll see in a second. Sometimes the From address has like less than, greater than in it, sometimes it doesn't. The format of the date is weird and the time zone and all that stuff. And sometimes this is connected together and there's different ways that they do dates, it comes through a Linux box or some other box or goes through Google and these headers are different. So one of the things I'm trying to do is kind of clean this data up so that when I put it in my database, I've got it kind of nice. The other thing I want to do, because I'm going to do full text stuff, is I'm going to pull the subject out. I'm going to split the headers and the body into different fields in my database. And so the headers are the part from the From up till the first blank line. And if I'm doing like full text searches, I really don't want to be searching the headers. Although the subject line is particularly interesting, and we might want to do searches on the subject line. So I'm pulling out the date, the sent date, the subject, the headers, and the body, and that's what I'm pulling out. And I'm using the Python program because it just is too hard to write SQL to do all this stuff. Some people might tell you to write a stored procedure, but frankly, you might as well just do this. In particular, because we can connect to the database. We use the hidden trick, etc. There's these lines about Ignore SSL certificate errors, just do them. Long story that has to do with HTTPS and expired certificates, you don't care. You're talking to an API and nothing here's important, so we're just doing that.
Play video starting at :5:40 and follow transcript5:40
So if we take a look at what's going on here, this is a little different than a classic crawling in that I know what the order of these things are. And I'm actually going to a primary key and the message number from the database from it, so it's like message three to going from three to four, that's just retrieving one message.
Play video starting at :6:5 and follow transcript6:05
That I know it and so I'm just going to just retrieve them in order, hopefully they don't blow up on me. But this one's more reliable and doesn't have rate limits and so it's a little easier to handle.
Play video starting at :6:18 and follow transcript6:18
And so what I'm going to do is I'm going to decide where we're going to pick up where we left off and let me show you how this code works. It's either going to start at message 0, actually which is message 1 because I say start = start + 1. But now I'm going to hit Enter here in the running gmane.py, and it's going to go back to the shell. And I'm going to start it back up again. This is a restartable process because the data is all in, there's 400 rows sitting in the database. The data is all there, so I run python gmane.py, and it knows that we've got to start at 401. So if I just say, give me one message, it'll retrieve one message, and then I can ask, it's 401. And so it doesn't restart. Now, if I wanted to restart, then I've got to manually drop the table. Let me add drop the tables right here. I don't have to drop it, oh you just say DROP TABLE messages and it would work, okay. So let's go back to taking a look at this code. So it's a while loop and it goes kind of it asks how many messages then it retrieves those messages and how many more do you want to do. But it's still working down, you know, message 401, 402, 403. So as it starts, it skips any messages that are in there. So if there is a message in there we do a SELECT, and at this point I want to show you this thing called myutils.
Play video starting at :7:52 and follow transcript7:52
Get out of there and do myutils, so take a look at myutils.py. I just made some library code. And the reason was is that I have a few things, like I want to query a row and fetch one, fetch the first record it's a query like limit one query or something, and I'm going to do that. And sometimes I'll do like a count star and I actually want the value, like not just a row with a single item, a single row with a single item. But I'm just, the zeroth element of that row tuple and I want it. So I'm just like, these are common things I do over and over again. So I put them in this file myutils and if I go up to the top of the file, I import myutils, and then I can use that. So this is just myutils.queryValue. This is what I call when I know that I want a single value. This is just going to give me one row, SELECT id FROM messages WHERE id = 12 or whatever, and away we go. And so this is just saves me typing about eight or nine lines of code, and it returns None. If it's not None, see that is not None, we're going to continue. So that a skipping through rows, and then I create the URL by just concatenating and I get one message at a time. I could actually get multiple messages at a time but then I'd need a loop inside the loop, and so I'm just going to do that. And then I have the code here.
Play video starting at :9:20 and follow transcript9:20
I'm going to of course put a try/except block, I'm going to use urllib, give myself a 30 second timeout. This context=ctx is a compensation for bad HTTPS certificates. I wish someday I could just go off on why that got messed up, and and warn security people. Arrgh.
Play video starting at :9:40 and follow transcript9:40
Then I read the document, check to see if I got a 200, which of course is the HTTP okay. I specifically catch this with a Control-C so I can blow it up and can get out and clean my mess up. So let me let me go ahead and do that. I'll do a python gmane.py and retrieve 10 messages and then hit Control-C, and I actually caught this abort. And what I want to do when I'm aborting is I want to break out of my loop, and then I want to make sure that I commit all the records that I just got done doing and close. And you've seen other things where I didn't do this and it blows up, and I wasn't in a try and except and it messed up. The other exception, who knows what's wrong with this other exception, I just am going to print the error message out. Now one thing I should sort of point out at this point is this gmane.py code, this took me probably two weeks to write and get right. And so all of this stuff, you don't necessarily have to put all this stuff in, I kind of added all these things, some of this try/except stuff. I added it all as I realized, you know, I have this problem and I have that problem. And so one of these things is these programs evolve as you run them and you should write them in a way that let you do that. So you'll notice that my thing is eminently restartable. You just drop the table and it starts itself back up. And so because you're going to be restarting this so many times as you're debugging it. And this code is pretty smooth at this point, it's been run by probably 20,000, 30,000 students in Python for Everybody.
Play video starting at :11:18 and follow transcript11:18
But the point is, when you start, it's not necessarily this sophisticated. And so don't feel like everything that you write has to have a bunch of trys and excepts, although when you're writing code, you can come back and take a look at this code and say oh, that's kind of nice. Because this will always be available on pg4e.com/code, so even if you're not taking a class, you can see all this. Okay. So I retrieve the data and I print out the length and you'll notice that that's what's coming out here, which URL I'm retrieving and how long it is, 3686 characters. And then I'm going to start parsing it, right, okay, and so because I'm retrieving one line at a time, it's supposed to have a From space at the beginning of it, one mail message at a time. And I have this thing about failing and sometimes it's just fail, fail, fail, fail so I have a little code that's smart about that.
Play video starting at :12:12 and follow transcript12:12
So then what I do is I check to see if I got a From the beginning. And you'll notice that that From at the beginning is not necessarily the person that it's really from, there's a From colon header, which is different than From. Then I look for two newlines in a row, newline \n\n, and if I find it, then I break it into header and body. And again, that's going to then be two columns in my database so that I can separately do stuff to the header and the body. Else I kind of trigger a failure and print it out to debug it, and again, once it works, but these little failure things are a good idea and you don't want it to go too far when it's failing. And then I've got to pull out the email address and I use a regular expression looking for the From colon at the beginning of the line, followed by a less than and then a bracket.
Play video starting at :13:9 and follow transcript13:09
And let me check something else here.
Play video starting at :13:21 and follow transcript13:21
I don't know why that was there. I just took that space out. So the From colon with brackets and the From colon without brackets and that's what this code is doing. And if I find one with brackets, I take the brackets out.
Play video starting at :13:42 and follow transcript13:42
And the next thing that's a little bit difficult is the processing of the date. So we go find the date header, so I look for a line that has Date in it, and that's right here. And then I pull out all this stuff. Friday, I want to get rid of the Friday. So that's what this dot star comma does. Then I want to skip a space, and then I grab this stuff. And this is a weirdly formatted date. and so I had to build a separate function, parsemaildate.
Play video starting at :14:13 and follow transcript14:13
And so if you take a look at parsemaildate, parsemaildate is trying to use a the thing that's built into Python called parser.parse. And if it works, it's great, otherwise, I wrote my own date compatibility library.
Play video starting at :14:28 and follow transcript14:28
And I just Googled something about what if the parse mail date doesn't exist, and then I could write my own but I really wanted to use the one that was built in
Play video starting at :14:40 and follow transcript14:40
to convert everything to this ISO format and the ISO format is what then my INSERT statement is capable of doing. So the date part is probably the most complex, and this falls into the category of data cleaning, right? And so then I get a sent_at date, then I go find the subject.
Play video starting at :15:4 and follow transcript15:04
And I look for a newline followed by the word Subject at the beginning of a line and then I do extraction of everything up to the end of that line then I pull that out of the header data, right? So I pull the subject out of the header data and I strip it and I lowercase it, etc., etc., etc.
Play video starting at :15:23 and follow transcript15:23
And then I just insert it all, INSERT INTO messages. I set the id in this one, which is kind of weird. And then every 50 messages I do a commit and every 100 message I sleep so that I can hit Control-C and when the loop is all done, I do conn.commit. And again, I can run this over and over and over and pull messages out. And of course I can watch so at this point I got 403, 403, and again, it's cruising along, right? If I hit Control-C now, I think, I think, or I might not have got it. It might not have been in the right place. And so let's see if the commit worked.
Play video starting at :16:13 and follow transcript16:13
Well, I got three of them there, so the commit worked, okay? So, let me just pull 10 messages down. So that one worked and so what I'm going to do is I'm going to stop and come back and talk about what we're going to do when we start creating these indexes.

# Demonstration: Mail Archive 2/3
Hello and welcome back to our in-progress demonstration of handling email data. So what we've done so far is we've actually run the program and we've got 463 messages. You can just do however many you want, you just keep on going, there's actually thousands of them. And then what we're going to do is we're going to start doing our indexes. And so I'll just start this index because sometimes it takes a little while for the index to run. So I've created an index named message_gin in the messages using the Generalized Inverted Index, and then the computation that is producing the array that's being indexed is the to_tsvector, using the English dictionary on the body. So we're only going do an index on the body. And so that's running right now. And so let's play a little bit and do some review. Actually these'll run, it's the explains that take a while. So we're going to look at the ts_vector and that shows what the index is being made with. So SELECT to_tsvector english body FROM messages LIMIT 1, that is basically showing the stemming. So we see all of the stemmed words like notifications as notif, organization is organ, please has been stemmed, the product is stemmed. So you see this stemming. And so simply running the function on the body is very different, we get a very different thing. So if we look at the whole body, so if I say, SELECT the body FROM messages LIMIT 1. Oh yeah, the perceptive reader will realize that I mistyped from, which I do all the time. So there's a whole bunch of stuff, right? So there's a whole bunch of stuff in here and the stemming, and it threw away semicolons, it threw away parentheses, "ice storm" is somewhere in there. So the index is really built based on this greatly reduced stemmed set of words and that's the inverted index. And the GIN basically says, for every one of these words in here, the stemmed case-fixed words are going to point at, if I say SELECT id comma body FROM messages LIMIT 1, all those stemmed words are going to point at 1. So you can, if you ask where golden is, it'll say, "Well, golden is in Message 1, along with all the rest of the messages." So that shows our stemming.
Play video starting at :2:42 and follow transcript2:42
I can do the to_tsquery.
Play video starting at :2:48 and follow transcript2:48
That is the stem of easier, easier is, so can I change that to easy? So easy turns into ease and the ts_query is in effect taking the WHERE clause, the thing that we would do the comparing to, and making sure that it whatever word is in your WHERE clause or the thing you're looking for, matches whatever things. So like notifications. So notif, notifications should go down to notif, right?
Play video starting at :3:20 and follow transcript3:20
So that does it. So that makes sure that we do the stemming process before we create the thing that we're going to look up in the inverted index because notifications is not in the inverted index at all. And so now what we can say is we can find whether or not a query in the double at-sign. So here I'm saying SELECT id to_query english neon, double at-sign to_query english body. And so that's asking the question is neon, appropriately stemmed, etc., in the body in English? And I'm going to look at the first 10 messages. Now I'm going to guess that neon shouldn't show up anywhere there. Yeah. False, false, false, false, false, false, false, false, false. Right? So there we go. And so it's all falses because the word neon is not in there anywhere. But if I can look for the word english, and now I'm just printing whether it matches, so we can kind of understand how WHERE clauses work. And so if we see easier, that didn't show up. Let's look for something that we know in here that's going to be there, easier wasn't in any of the rows, but let's put notifications because it's in all of them. True, true, true, true, true. Let's look for golden to see if golden, if the ts_query of golden is in the ts_vector of body. And so we got a true. Message 1 is true and the rest of them are false and that's probably because Glen Golden wrote Message 1. And so that gives you a sense of how we use this in a WHERE clause. Now what I'm going to do is just so I can keep track of things, I'm going to actually add a column that wasn't in the original database, ALTER TABLE messages ADD COLUMN sender as a TEXT column. And so then what I'm going to do is I'm actually going to do a substring. Let's just do a SELECT on this, SELECT substring headers and then a complex regular expression FROM messages LIMIT 10. Okay? And so this is pulling out those email messages from names. And then I'm going to do an UPDATE statement so that I get a new column. I'm populating this new column, the sender column, and I'm populating it by running through and parsing all of the headers. And so that actually built the sender. So I can say SELECT now sender FROM messages LIMIT 10, and I'm just going put that on. So instead of having to put this substring headers everywhere, I put that. I'm not going to index it because I want to show you other techniques. Okay. So if I take a look now, this just allows me to throw the sender on various selects, SELECT subject comma sender FROM messages WHERE it's on body Monday. So where ts_query monday is in the ts_vector of the body. And so this is showing now I can easily see the messages and their subjects and who sent them, and so sender is just there so that these look a little prettier. So now we can take a few things. Let's check to see if our index is working by using an EXPLAIN ANALYZE on tsquery of monday into the tsquery of the body. And that was a sequential scan. So now we've waited a little while and we do the EXPLAIN ANALYSE and we do see we get a nice it uses the messages_gin scan, and away we go. Now you'll notice that, for example, we never made a Spanish index. So if we take a look at the EXPLAIN ANALYZE spanish, it actually can do this, right? It actually can run the Spanish select. It could actually pretend these are all Spanish, but it's a sequential scan, right? So if I just run the SELECT, you'll see that it's capable of understanding. I mean, this is really checking to see, I'm doing a sequential scan by looking in Spanish. Now, of course Spanish doesn't tell us much here. The key is to_tsquery spanish is just code that runs to_tsvector. So what it's going to do here, it's going to retrieve all the bodies. It's going to compute the to_tsvector of spanish of all the bodies sequentially, then it's going to do the to_tsquery of monday in Spanish with stemming and all that stuff, right? So we could see what to_tsquery of monday looks like, but it's probably going to look exactly like English, but you can kind of see that. Yeah. So the Spanish doesn't change anything. The point is it runs through all of them sequentially, does the computation, the WHERE clause works. The difference is that because I never made the GIN for Spanish, it's not fast. And so it's not like you can't do things, the index just really makes things faster, okay? And so the last thing I'm going to do here is I'm going to just switch between GIN and GIST. So I'm going to drop the GIN index, boom, and then I'm going to create a GIST index, which is using GIST in the same thing, to_tsvector.
Play video starting at :9:4 and follow transcript9:04
And so now we've created a GIST index. Now to review, the difference between a GIN index and a GIST index has nothing to do with the queries, it has to do with the size of the index, the cost of maintaining the index and the performance of the queries. And so GIN is a simpler,
Play video starting at :9:25 and follow transcript9:25
hash-based index and because it's hashing, it's somewhat lossy, but it keeps it smaller and it's easier to maintain. So the GIST is smaller, easier to maintain. But then when you do selects, you might get extra rows, but then it does actually check even, with a GIST index. If you do something, it's going to maybe under the covers pull too many rows out. Did I forgot my semicolon? There might have been rows that it found that don't actually match the to_tsquery of english of monday to the to_tsvector of english body. It might, but then it throws them away. And so it might actually retrieve but not hand to you. So before it hands it to you, it only sends you the exact messages. And so ultimately, GIN and GIST do the same thing, they just have different performances. And in everything in indexes, you are always trying to trade off insert performance, size of index, and performance of select. And so that's a trade-off. I just did this as GIST to show that it pretty much does the exact same thing. So we'll stop now and then come back for the third bit of this discussion.

# Demonstration: Mail Archive 3/3
So welcome back to the Part 3 of our email archive database full text searching. So we're in pretty good shape. We've got our database all loaded, we've got a nice little GIST index, and we're able to do ts_query WHERE clauses. Now, I want to explore a little bit more of the kinds of things you can do with ts_query. So far, we've been doing single-word queries like to_tsquery english monday. But now I want to add some things. And so it turns out that this little to_tsquery language is actually a query language and interestingly, you can make syntax errors in it. So personal and learning. When you're querying for personal and learning, you are going to find that says, I need both of those words. It's sort of a standard and operation. So that says personal and learning have to show up in those rows.
Play video starting at ::51 and follow transcript0:51
So then we can select the words but in order, and this use the operator that is the less than dash greater than. That says personal and learning, but personal has to come before learning. And so that's ordering. So that basically will give us a different set of rows. I don't quite know why there's no personal followed by learning, but it's not there, we found no rows.
Play video starting at :1:23 and follow transcript1:23
Let's see if it goes the other way. Learning followed by personal and there we go, we have one that has learning followed by personal. So learning followed by personal is the way that I read those. You can add a not. I want learning and not personal. Not personal. Exclamation point is not. Exclamation personal ampersand learning means not personal and learning. So we'll see what we get for that. We've got a whole bunch of learning but without the word personal. So very few people are talking about personal learning or learning and personal in the same sentence. So that's one.
Play video starting at :2:6 and follow transcript2:06
So the to_tsquery, like I said, is a language, and I'm going to make a syntax error when I say parentheses personal learning, and that's because inside this string, it's actually a query language of its own. And so this one is going to give me a syntax error. Oops, need a semicolon there. So it says syntax error in tsquery and it just like it didn't like the syntax of it because I used a special character in there and in a way that it didn't like. And so if you're taking stuff from the user, you use this plainto_tsquery. And so it's like, "Well, whatever. I'm just going to throw away things I don't understand rather than blow up." So I look at the personal learning with the parentheses, and it can work, right? And this is a syntax error from to_tsquery, but it's not a syntax in plain. It just throws stuff away.
Play video starting at :3:7 and follow transcript3:07
We can look at phraseto_tsquery, a I followed by think. First, we'll do it without phraseto_tsquery and that's what we see. That is I followed by tsquery and the phrase one is just kind of like a phrase. And so it's I think is a phrase, which is I followed by think. It really is a transformation from this I space think to I followed by think. And again, you can think of this as something where you're letting the user type something, but you don't want them to have to know how to type all these fancy things that to_tsquery.
Play video starting at :3:52 and follow transcript3:52
And if you have Postgres greater than 11, you can take a look at, this is a syntax that kind of came this minus personal learning, is a syntax that kind of came out of Google that is a way to say not personal and learning. It's the same as this. And so this is websearch_to_tsquery. A lot of people are going to end up writing kind of little grammars, transformers that would take these Google-style things and then convert them.
Play video starting at :4:25 and follow transcript4:25
So that works. Minus personal says not personal plus learning anywhere in it. And then the last thing I want to do is I want to show you something about text ranking. So text ranking is, it's important to know that text ranking is not really participating in the WHERE clause. All of our optimization, and inverted indexes, and all the stemming is to make the WHERE clause go faster, to pick the the documents you're going to see. Once the WHERE clause has thrown away most of the documents, which is its goal, then the documents are retrieved and the rank is a computation based on the retrieved documents. And then you can ORDER BY. I can ORDER BY ts_rank descending, right, and give me the most highest rank. And the ts_rank takes a ts_vector and a ts_query. Cleverly, you probably would want them to be the same, but it doesn't have to be the same as the WHERE clause. But in this case, so what it's saying is like we did pick it and double at said that personal and learning were in this document. But then we're saying how good were they? Were they closer? Etc., etc., etc. And so there's a couple of different ranking functions and you can read up on those. But the ts_rank is just a calculation on the retrieved documents. So let's take a look at this and we're going to print the rank out. So now this is this rank, right? And it's ordered in descending order. So this one here is the most seemingly, most relevant about personal learning. And I think if we looked at that one, we would probably agree with it. And some of these have less and less relevance as they go on. So that's really cool. And then there are more than one ranking algorithms. There's this ts_rank and then ts_rank_cd, and you can read up on those in the Postgres documentation. They both simply are computations on the retrieved record set and they just do a different computation. I'm not sure. In this case, it even threw out some things that were in or out, and that's because it does a different calculation. There's a whole bunch of research on how to do ranking, etc., etc., etc. And so that's where I'm going to stop. I've got some stuff where you can use regular expressions and make indexes based on regular expressions, and I'll leave that for another time and you can take a look at that. That's sitting here in this 06-Python.sql. Okay? Hope that you found all these demonstrations useful. Cheers.

# Lecture: Ranking Search Results with PostgreSQL

So now we're going to talk about how we rank the results. You don't have to rank the results but you may want to and with Google and other search engines, there is kind of an expectation that if you do a search that the most relevant results will appear first. So there's a series of ranking functions inside of Postgres that help you do that. Now, it's important to see that the rank is generally in the SELECT, the SELECT part, not the WHERE clause. The WHERE clause is which rows. That dominates the cost of queries. Calculating the rank, that's actually really cheap, and yeah, you're going to do an ORDER BY in this one, you're going to order by rank descending. But once you have the rows, if you have a million rows and you've pulled 300 out, ranking them, that's cheap. Even if there's an ORDER BY and you have to pick like 20 of the 300 as you're paging through maybe, don't worry, the ORDER BY is the cheap part about it, going from a million rows to 30, that's the expensive part. And that going from a million rows to 30 is where the index saves us. So the index is not all that important for the ranking because if you're going to do a ranking on every record, the fact that you're reading every record is actually the problem, okay? That's the real problem. Then so you don't want to read every record, you want to index to save you from reading every record. We have a WHERE clause. In the WHERE clause, we have a ts_query and a ts_vector, right? The ts_query is personal and learning, and we're just going to check the body of this document. We're using our email messages in this particular one and that's the WHERE clause, it works. And then in the SELECT clause, you see the computation that happens, and it is taking the vector and the query, and it's simply looking at how close they are. It's not picking rows, it's just, "I've got rows, here's a vector coming from the row and here's a query that I used, how close are they?" Okay? And so that's it. You can ORDER BY. Like I said, the number of rows that are going to be sorted in the ORDER BY are hopefully small because the WHERE clause did its job, and so we get some ranking and I had them in descending order there from zero through one. And there are two different, and you can read up on these, there are two different ranking functions. But really all they are is a different calculation with the same data. And there's some theory, and some paper, and someone says, "I think this is a better ranking function." Somebody says this is a better ranking function, you can write your own ranking functions if you want and it just ends up with the ranking. And so the ranking is really kind of an afterthought because the hard part was getting the rows that you wanted to get and away you go. And so I'll just close by talking about how impressed I am by Postgres's depth of capabilities when it comes to indexes. I've got this SELECT statement, SELECT amname, etc. here. I got this off of Stack Overflow. But it's a lot of rows, it's got 159 rows and what it's showing is the different kinds of indexes that you can do and the difference kinds of operations. If your calling did the string index, we had to tell it we wanted to do text operations, and that's because there is code that somebody wrote inside Postgres that knows the difference between a text array comparison and an integer array comparison, and it's optimized. And that's because how you compare text and integers are quite different at the lowest of levels. And so I just type this command and get these 159 rows back just to be amazed at just how much work goes into not only just GIN indexes, but the B-tree, BRIN, and HASH, all these indexes, and how much work these Postgres folks have put in to the software to make our job easier as those of us who have to look through this data and write applications to make use of this data.