# Demo Reading and Parsing Files
So now I'm going to show you just a bit of a demo. Now I'm not going to do this demo here, I'm going to make a separate screen recording because I like to give you screen recordings for things where I want you to follow along. But basically, if you recall in a previous assignment, previous lecture, the thing we did was we made a database and we were getting rid of vertical replication. Vertical replication keeps coming back, right? So here we got some vertical replication, two things like make and model of whatever it is, we got some vertical replication. So we're going to end up with a little table that is our lookup table here. That is this second thing that says B is 1 and then we're going to end up with numbers like foreign key into this table y_id, and we're going to put these in, right? So I could give you as an assignment take this data, make two tables, send one as a foreign key into the other and put the second column in and just put the distinct values. Sound familiar? Just put the distinct values of that second column into here. That's what I want you to do and I want you to do it by hand. So what we're going to do in this particular thing is we're going to automate it. And we're going to make it so that you could really take any and it doesn't just take two tables. You could extend this to me more than one table, I mean more than two tables. But it's going to auto-generate these things and we're going to use things like DISTINCT. We're going to use things like sub-selects, and I'll talk about it. When I talk about I'll go like see how we're using the sub-select. Yes, if this was an online application it would matter, but it doesn't. We're using kind of this batch job that's taking this CSV file and turning it into a set of normalized tables. And so the key thing is this might not be using the best techniques for online applications. But this the best technique for the convenience of getting a nicely normalized database out of a non-normalized CSV file. So I'll go over this and show it to you in some detail and it will use most of the ideas that we talked about in this set of lectures. So in summary, a lot of what we talked about in this set of lectures is the SELECT statement. And SELECT is often the last thing I talk about because CREATE, UPDATE, and DELETE are like they're the big ones. And SELECT, oh yeah, that's how you check to make sure those other ones work. But it turns out, as I said before, the SELECT is where a lot of this relational power matters. So the SELECT like narrows your view which increases performance then this whole DISTINCT thing happens. And so if you go and you look at the documentation on Postgres, you will see that literally we have only talked about almost half of what SELECT does and there's so much more that it does. And again, the SELECT, the looking at this stuff in the database after you've done all this normalization and all this optimization, how you ask to look at it is super critical and super amazing. But we've done a pretty good job at this point covering a lot of the different things that SELECT is capable of doing. So up next in the next set of lectures we're going to talk more about text data. We'll talk a little bit about text data in that demo that I'm going to show you in a bit but we'll talk a lot more about text data in SQL coming up next.

# Demonstration: Creating and Loading a Database
Hello and welcome to another walkthrough of our Postgres. This is a simple three table many-to-many situation. We're going to have users and posts and comments. And the comment is effectively a many-to-many join table, except it has very important information that's stored at that connection in content. created_at the content, there's all kinds of stuff there. A post belongs to a user, so we have a foreign key account_id. And so away we go. So let's go ahead and build these tables, just copy them all, comments and all, and post them into, there we go. Good thing I've got no typographical errors. If you see some of my slides that say TIMESTAMPZ, that was a common typo that I made. So you might want to anytime you see TIMESTAMPZ, turn it into TIMESTAMPTZ. Sorry about that mistake. So now we have this all set up. Now, let's make this favorites also. Now I want to talk a little bit about the ALTER TABLE. So one of the cool things about the database, and usually it's not right after you create it where you notice you've made a mistake, but we're going to fix a couple of mistakes. So if you look at the post table, you see that I made a content of VARCHAR 1024. I'm in a meeting and someone says, whoa, we're going to have to have posts that are more than 1000 characters. So I'm like, oh, I'd better fix that. ALTER TABLE post ALTER COLUMN content TYPE TEXT. So what we're doing is we're changing a table, the table post, we're altering a column, alter column. The name of the column is content. And the new type is text. Now, the key is this is also going to convert live during the database if there's already data. So ALTER TABLE is a very powerful thing. So you can get rid of a column. ALTER TABLE fav DROP COLUMN oops. So that gets rid of a column. And we can add another column, add a column. ALTER TABLE fav ADD COLUMN howmuch INTEGER. And so that adds a column. So we're able to play with the schema of these tables. The schema is very constraining on purpose. That's just how databases work. But we get to change the schema. And automatic conversion. You can convert something from an integer to a string, or a string to an integer. If you do an ALTER TABLE that converts from a string to an integer, it's going to try to convert it and it will have trouble if there's strings in there that aren't legitimate integers. But you'd be surprised at how much you can do, and you can do it while the database is running and while actually transactions are happening, as long as you don't break the software. So if you drop a column the software is looking for, then all the SELECT statements will just like blow up in the next moment. But you can alter these tables, so that's pretty cool.
Play video starting at :3:6 and follow transcript3:06
So the next thing I want to do is I want to load a bit of data. And so you can put, you can make SQL commands living in you see me just copying and pasting this stuff, but you can also do this by putting it all in a file and then running the file. So if I look at this file, 03-Techniques-Load.sql The one thing I'm doing is I'm DELETE FROM account, that's remember how a DELETE statement works is delete all records. And this ALTER SEQUENCE, this basically restarts this serial number so I clear them all out. This you wouldn't do in a live running database, I'm just doing this so I can do it over and over and over again. And then I'm going to do some INSERT statements, fill some stuff up, just sort of to save it. Now, you need to figure out how to get this in here. And so what I'm going to do is I'm going to make myself a terminal. And I am in that same, same directory. I could download that file, and I could upload that file. But the easiest thing to do is do what's called a wget. So wget actually retrieves a file using HTTP and then stores it in the local directory. And I am going to grab this. Actually, that's not quite going to work because I don't I don't have the
Play video starting at :4:29 and follow transcript4:29
I don't have it up on its ultimate final.
Play video starting at :4:36 and follow transcript4:36
This domain name is not final yet, so it's going to have to come from here. So if I do an ls minus l, you see that I have this file here, and if I do a nano 03-Techniques-Load.sql, there we go. And Control X gets me out of that. So I've got that file loaded. So that means that I can go back here in this same place, and I can simply say, read this file. So anything that says backslash, backslash i, and we'll have other things like backslash d plus, those are commands to the Postgres client, the psql client that we're using. They're not actual SQL. And so if you're using MySQL or Oracle, there's like like d plus fav. There's a whole different name for that. And so those are non-standard across databases. ALTER TABLE is pretty standard across databases, certainly CREATE TABLE is pretty standard with a few. SELECT is pretty standard. But I'm going to use this backslash i, and that is going to load a file on from this shell that I'm working at inside of my Jupyter notebook and I copy that file there, and it's going to load it, and it ran all of those commands. See how cool that is? So now I can say SELECT star FROM post, and there they are. And so all those things were loaded up quite nicely and that just saves me some time. Okay. So I'm going to stop now and I'll pick this up a bit later in with the same database.

# Demonstration: Loading and Normalizing CSV Data
Welcome to another walkthrough of SQL. We are now going to work on something that takes all these techniques that we've been doing this week and puts them all together. So what we're going to do is we're going to take a CSV file, and we're going to load it, and then we're going to automatically normalize it. This is a interesting technique because a lot of times what you're going to be doing is taking a large amount of data, and then pull it into a database, but you want it to be fast and small, and so you want it to be normalized. Of course, we're going to reduce the vertical replication. So here we have two things. I'll just call them x and y, Zap A, Zip B, One, Two. So we've got vertical application in the second column, and so we're going to make a one-to-many, not a none-to-many, a one-to-many. We're going to make a one-to-many table out of all this, but we're going to do it automatically, and we're going to use sub-selects, and we're going to use sub-selects and we're going to use SELECT DISTINCT. And watch. It's a pretty cool technique. Okay? So the first thing you've got to do, you're going to have to download this. So I've got a terminal running. This is just in my home directory, and I can call wget, and now I should have this stuff, and if I take a look at this file right here that I just downloaded, which one is it? Techniques CSV. 03 tab Techniques.csv. We see it's a little tiny bit of data. That's just the data we're already playing with, right? Okay. So that's good. So we've got it sitting there. So now what we can do is we're going to eventually load this in, but I wanted to get that done first, and that's the load. Okay? And so I'm going to just type these even though they're not there, so it doesn't matter, and then I'm going to create a table. So the idea is that you load this into a table that matches the CSV. And I'm going to have the x and the y. Oops, don't do that. Undo whatever that was. I want the two columns that just come in because I'm taking these in as texts. Eventually, we're going to convert these into a foreign key relationship, but for now, it's just text. I'm going to hold a little spot to have that foreign key later. So I'm going to do a CREATE TABLE, and that's just to load it in. Then I'm going to have a lookup table, which is just some texts for the name of the thing. I should probably put a UNIQUE in there, but I don't need it. But I could make that a VARCHAR UNIQUE, but for now, I'm just going to make it be TEXT I'm going to have a SERIAL, that's really important, and make that be the primary key. So y has an id SERIAL, a primary key of id, and y of TEXT, and then I'm going to create the actual ultimate table that I want this to be in. So it's going to have an id column, which is a primary key, it's going to have the x value, which is the first column, and then a foreign key. Now I could set all this foreign key stuff up. And I'm going to basically say that there's a uniqueness constraint between x, which is the text, and y_id. So that there's only one combination of Zap and A, you can't put Zap and A in twice. So I'm making my uniqueness constraint two columns wide, okay? So there we go. So the first thing I'm going to do is I already download this file, and I'm going to use the psql command copy. So I'm saying, put it in the table xy_raw, parentheses x,y says here's the two columns coming from the CSV, break it up with the CSV delimiters, it's a CSV file. So that should insert four records, and I can say, SELECT star FROM xy_raw. So there we go. So I've got the data that came out of the CSV, and y_id is currently empty because I didn't put it in this copy. The only thing we've done is loaded these things. Okay? So then what we're going to do is use SELECT DISTINCT. Now, remember, DISTINCT basically ensures that we only get one row. So I've got vertical replication in this y column. So you'll notice that if I do a SELECT DISTINCT y FROM xy_raw, well, let's stick an ORDER BY on there. ORDER BY y. Oops, SELECT DISTINCT xy_raw. ORDER BY, no underscore. There we go, and now it's even sorted. Nice and pretty. Now, here's the cool thing. We are going to use a SELECT sub-select, INSERT INTO y, into the y column, which is that TEXT field, right? And then SELECT DISTINCT. This isn't exactly a sub-query, but it kind of looks like we could make it be a FROM and a sub-query. It kind of looks like a sub-query here. But what it does is it selects distinct y from xy_raw, which is exactly what we did. And I'll throw an ORDER BY on that for yuks before I run it. But then we're going to take the results of the SELECT DISTINCT and we're going to insert that into the table. I'm going to do ORDER BY y. Now, I can say SELECT star FROM y. So what we really did, because we did a SELECT DISTINCT, we only got one of each, that's the key, and then because we have a SERIAL column, we got the 1 and the 2. So we got 1 maps to A and 2 maps to B. That's the key. Now, we have another sort of tricky sub-select. So we're going to update xy_raw. So let's do a SELECT star to refresh our memory FROM xy_raw. So right now, this has the string and it has an empty y_id, so we're going to use an UPDATE statement to populate this id, and we're going to do it by setting it to another query. So it's going to go through every one of the rows in xy_raw, and set its id to be SELECT y.id, which is this little id number from the y table, FROM y WHERE y.y, which is the A and the B, equals xy_raw. So it's like joining this up, connecting this one A row to both of the rows that have A and sticking the 1 in there. It's probably just easiest to run it. It's pretty cool. Now, I can say SELECT star FROM xy_raw. So look at that. So you saw how that UPDATE statement set the corresponding, these now match, right? But what's happened is this y has become redundant. So I can insert this again, I can pull just the x and the y_id from xy_raw into this xy table, which is sort of my ultimate destination table, it's got only the foreign keys, it doesn't have the y bit, so I'm kind of converting it into a pretty little table. But mostly, I'm just throwing away the y. I could do this with an ALTER TABLE, DROP COLUMN y. ALTER TABLE xy_raw DROP COLUMN y. There's two ways to do it. So now, if I did a SELECT star FROM xy, it's so pretty. It has the track or whatever, the x thing, each row has a primary key, and it has the corresponding foreign key that points properly. SELECT star FROM this y table. Right? So it works perfectly, and so we can write an awesome little JOIN. So we SELECT star FROM xy joined to the y table on the foreign key, xy.y_id equals the primary key from the y table, which is exactly what we would do if were looking at a one-to-many, right? And so we're seeing all these things. So we've reconstructed our data and we've used the join, we even see the join values here in the y_id and the underscore id. So this notion that we can use SELECT DISTINCT and then the apply the three most critical tricks here, other than a copy to load the stuff in the first place, are this SELECT DISTINCT insert, just this one here. INSERT INTO y, parentheses y, SELECT DISTINCT y FROM xy_raw, which is the thing that removes the vertical duplication and then assigns the serial number for each one of those things. And then to go through and, in effect, use the string in the xy_raw to look up the id, and put that in the xy_raw, and then the next bit is just to get rid of the redundant parts of xy_raw, and it creates a nice little table that makes all the sense and is properly normalized, and away we go. So I hope you found this useful. I kept this as simple as possible, and we're using really small variables. You're going to do some homework that does this over and over and over again with a couple of different examples. I hope you found it useful.