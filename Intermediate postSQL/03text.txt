# Text in Databases

We're going to talk about texting databases, but really, this is much more than that. After the first two lectures, we've really been just using any excuse to teach you more in depth SQL skills. And you may or may not use some of these skills in text, but they're just skills that you need. So the first thing we're going to talk about is how to generate some test data in Postgres. So we're going to talk for the first time. I've been talking about performance all along, but now we're going to start talking in more detail and actually looking at how you decide about performance. So the problem is every, all my little example so far, like I put five records in and I'll show you some queries. But now I want to put some more data in. So the problem is if you think about it, you gotta let's say you need 10,000 records or 100,000 records or even a million records. So you can check to see how fast a query runs with various indexing options, etcetera. And we're also going to look a little bit about how much space we start using as we make different decisions, so we'll get to all that. But the first thing is a prerequisite is to fill your database up with data. And so you could write a Python program and you could open some stuff up and you could write some four loops and then write stuff into the database or you could use shell scripts. But it turns out that Postgres has some wonderful mechanisms that allow you to generate random data. And so there's some functions, and we look at each of these with some examples. But the repeat function allows you to create long strings, so you just take a string like, you know, ABC say, give me 1000 and that's like 4000 character, 3000 characters, ABC, ABC, ABC kind of doesn't matter. Then there's random, which gives you a random number between 0 and 1, and we use functions like trunks so you can multiply that by 100 and then trunk it and then get an integer between 0 and 100. Or you can say, give me a number between 1000 and 9000 and then you get a bunch of 4 digit numbers so we can figure all that out. The one that's kind of cool is a thing called generate series, and and it's a way to generate multiple rows. And so generate series is how we generate a lot of rows. And so we'll look at each of these with a simple example. So the first simple example is a random. So basically random just gives you a number between 0 and 1. It's a floating point number, right? So I call it twice, I get two different numbers. And then I just multiply by 100 I trunc it, so trunc says, so that becomes like the random number between 1 and 200. It's a floating point, but trunc says convert that to an integer. Now the repeat one right, so I'm just taking, the word neon with a space and repeating it five times. So that's sort of a horizontal concatenation. And so sometimes when you're building keys or whatever, you just want them long. You'll see in a set, kind of we make, just put some random stuff in it, and then just a bunch of text, and that's usually good enough. But this generate series is really kind of cool. If you think about it from Python function, it's kind of like, it's like the range function in a sense, if you think about Python so far, I in range 5 that actually that then generates 5. And so generate series 1 through 5 generates a series of numbers, but it also then if you can concatenate it, it generates rows. So these first two generated one row, just that's a random number. That's a long string. But this is five rows. And so we combine all these things together to generate lots of rows of long data with some randomness. That's what we're going to accomplish.
Play video starting at :3:36 and follow transcript3:36
So here we see a select statement, right? So we're going to have a string vertical double vertical bars, concatenate string concatenation. So we're going to take, sql4e.com/neon concatenated with a random number that's between 0 and looks like a million concatenated with repeat the word lemon five times and then to create more rows we also concatenate it. This this is all one thing. So this general series is so cool, like explodes into the numbers 1, 2, 3, 4, 5 vertically. And so that forces all these things to be calculated five times. And so we end up with five rows with reasonably long bits of information and a random number and then a sequential number. And so with these techniques, you can construct a whole bunch of data. Okay, so it's random trunc, repeat and generate series, and then you just insert these things in. And so up next, we're going to talk about, we're going to use this and we're going to talk about some of the text functions that we can do with data once we have it in the database.

# Text Function
So now that we've got some text in our database, we're going to talk about the kinds of things we can do manipulating functions, WHERE clauses, etc. So there's a couple of things that are standards in here, and I'll try to call out the things that Postgres does a little bit differently. So we've already looked at WHERE clauses that have equal signs, equality, those are actually really important because they are the most performant, especially if you have an index on a string column. LIKE is like a simple wildcard that you can put a few characters in. SIMILAR TO is kind of like a regular expression. Most people don't like using SIMILAR TO, because I know regular expressions are a little bit hard to learn, we're going to cover them. But once you know regular expressions, they actually translate not just in Postgres, but actually in Unix, and other things like Linux, you can use them. And so I don't think SIMILAR TO is much of a good idea, and it's not even particularly standard. LIKE is actually more standard across databases like MySQL and others. And then you have your basic equal sign, greater than, less than, greater than or equal to, all that stuff in between. And IN. IN gives you a set of things, so you put them in square brackets. So like if WHERE this column IN this, comma, this, comma, this, comma, this, in square brackets. Then you can manipulate the SELECT results, like lowercase, uppercase, taking out a substring. And you can put these in WHERE clauses too. And so we'll talk a little bit about part of the performance, I've always been talking about the p erformance, where sort of it either goes into the database engine and the database engine optimizes. Or ultimately, you've got to pull back all the results, loop through all the results, and then apply the WHERE clause there. So we'll hit some really good examples of this coming up. So Postgres has an outstanding documentation on these things, I really like how they did it. There are just two pages, and they've got little simple examples and like, yep, I got it. So I'm not going to sort of read all these things. You can go into the Postgres documentation, which is, of course, free and online because Postgres is an open source project. And so you can just look at this stuff, and I'll cover some of the ones I want to cover. One thing you'll notice, and I don't quite know the history of this, is that there is a bit of commonality between Python string method names and the Postgres calls. So that they're a little easier, kind of like that generate_series kind of feels a little bit like range. So we're going to start out and do a couple of things. And so I'm going to make this table called textfun that's got a unlimited length TEXT field. We'll talk in a bit about the length of these things. And then I'm going to do an index, just make an index on it. And that basically allows me to go quickly to various things with equality. This, by default, is a B-tree index, that's why I named it underscore b. It's a B-tree index, and B-tree indexes are good for sorting, they're good for exact lookups, and they're good for prefix lookups, and ranges. And so they're kind of the go to index in most databases. We'll talk a little bit about hashing. They sometimes take a little less space, but they have their own compromises. And so when you just say CREATE INDEX, Postgres will generally make you a B-tree index, and you basically say it's in this column of this table. So here is a couple of cool things that we're going to start playing with a little bit. And this is all about trade-offs, space versus speed trade-offs. pg_relation_size basically says how much data is what's in this currently in this relation taking? And then how much size is the index? So what you'll notice is I've just created this table, and I put no data in, but the index is already taking up some space. And the whole idea of an index is you're trading off space for speed. And so don't get too grumpy about the space. And especially in modern computers and disk drives, space is generally not your problem, but speed is your problem. And so I will call attention to these, but don't think to yourself that your goal in life is to make these as small as possible. Actually, when you're using indexes your goal is to make it fast, whether it's fast for inserts, deletes, updates, or reads. But this is really cool. And if you're working on something way more complex than this, you can say hey, how much is this taking up? And how much is the index taking up? But like I said, don't get too concerned, because the reason you're doing an index is to go fast. Okay, so we start here, and so now what I'm showing you is this from the previous section, this random generation. And so this is one SQL statement. You'll notice there's no semicolon here. So I'm going to INSERT INTO textfun, and then this is the columns that I'm going to insert, and then I actually have a SELECT statement. This is a continuation. Normally you'd say values, blah, blah, blah. Here what I'm doing is I'm actually doing a SELECT statement, which is generating a set of rows. And then those rows are going to inserted into textfun. So this is an expression that generates a row, and then it inserts into content. So here's another syntax, the CASE statement. So you can't do if-then-else in SQL because it's not a procedural language. If-then-else is like loop, if, loop, if. What you have to do is write this in a statement that's sort of for all rows, true or false. So they don't give you an if statement. They could give you an if statement, and you'd just say there's a loop implied around it. But they didn't, they call it WHEN. And this is universal across all SQL, they don't call it if. So WHEN random is less than 0.5, THEN this, ELSE that. So this whole CASE bit from here to here with the END, this part in parentheses right here, from CASE to END, that generates one of two strings. It either generates this neon string or this LEMONS string. You see my racing car sort of sneaking in here in some of my thematic stuff. So all it's saying is of half these records that I'm generating are https, blah, blah, blah. The other half are http, blah, blah, blah. And then I can concatenate that with generate_series from 100,000 to 200,000. So this is going to basically generate 100,000 records in one statement, so that's how you do it. So using those things from the previous section, we generate the statements. And now you can see that we got 6 meg here, and the index is even larger, 8.5 meg. And one of the problems here is we're using a big long text field. And the problem is that if you're using a B-tree, it wants to be able to know for an exact match, it goes down the tree and it finds the record, and then it checks to see if it's true. So the thing that's being indexed, the column that's being indexed, content, ends up being completely replicated in the index. So there's two copies of these strings, and I didn't make these particularly long, right? There's just a lot of these now. These are a lot. And I could have made these much longer by concatenating more repeat, like on a repeat or something on here, where I take blah, blah, blah, blah, blah over and over and over again. But you just see that the index grows faster than the data. Now, in a normal world, you would have a lot more columns here. And the actual content of the database would hopefully be much larger. I'm just doing this with one column, so you kind of see how the database grows a bit. I mean, the index grows a bit. And in this case, it's larger than the actual data that's stored in the database.
Play video starting at :7:57 and follow transcript7:57
Again, don't be concerned. We're trying to be fast, but actually just indexing a big long text field is not necessarily, well, the best idea. So if we look at the first five of these records, we kind of see our randomly generated stuff, some are http, some are https, and they have kind of a different string. And this we're going to be doing some text functions and that's why I came up with this stuff and so we can see the sizes of these indexes.
Play video starting at :8:25 and follow transcript8:25
And so that's how we did that. So this is just once those things are set up, we can use a LIKE clause. So the LIKE clause uses percent as a wildcard character kind of like we think of as asterisk in some wildcards. This matches any number of characters, that matches any number of characters. So what this is really saying is one five zero zero zero zero anywhere in the string. You can take the like uppercase of this, you can take the lowercase of this. This is the same LIKE clause, there's a whole bunch of chopping functions like giving you the four right characters, the right four characters, which gives you the end, the four right characters. And give me the four left characters and so you can get that. So these upper, lower, right, and left, they can actually be used in WHERE clauses as well.
Play video starting at :9:14 and follow transcript9:14
So, here's another one. Yeah, that's that one. So you can like ask WHERE strpos. It's actually more like, see, this is actually not Python because it actually looks a lot like PHP too, strpos. Within this string, tell me where the ttps starts and that's in position 2. This is pull out a substring from it. Give me starting at character 2, go 4 characters. So you get https. There is a way to split it. Now, it's not a thing when you split it and then loop through it because there's no looping, right? So you have to tell it, I want the fourth part after splitting it into slashes. So there's a slash, slash, slash, slash. So this is the zero part, the one part, the two part, the one part, two, three, four, and so out we get is neon. Now this translate is just another, you may or may not ever find a thing for it. But this translate is actually from the Unix tr command and it gives you two translation strings and it allows you to do a one-to-one translation. So t lowercase t goes to uppercase T, lowercase h goes to uppercase H, dot goes to exclamation point, p goes to uppercase P, and slash goes to underscore. Again, this is completely silly but the Hs, Hs, Ts, and Ps are all uppercased, the slashes are underscores, and the dots are exclamation points. Why do you want to do this? I don't know. I'm just showing you how to tr. And so there can be one or any number of characters, but this second string has got to have the same number of characters as the first string because it's like a character translation map. That is, that each character goes from the one to the other. So let's talk a little bit about how this performance works. And this is the first time that we're seeing this explain idea. So SELECT content, this is a SQL statement. And in this case, we're going to use a LIKE clause in the prefix. So that says it's got to start with racing and this is actually something that works really well in B-trees. But instead of running it, we're going to do explain analyze. I'm not sure they're SQL commands, but they are commands to Postgres to say, you know, run this but tell me a bunch about it.
Play video starting at :11:42 and follow transcript11:42
So, here's a couple things to look at the output and I've taken a little bit of this output out. This is an index-only scan and that's the kind of thing you want to see. You don't want to see a sequential scan on the whole thing. And so it's using that index textfun_b that we defined. That's our B-tree index. Underscore b is just my way of remembering it's a B-tree. And then you look, so I talk about what happens when you like look at all the records versus what you send into the database engine. And so what it's really doing is it's turned this LIKE into an index condition that says, look for records that are greater than or equal to racing and less than or equal to racinh, racinh. So that's going to get so the greater than or equals and less than, this was constructed by Postgres, right? And so it's like to racing to racinh and that is actually the racing ones because it's less than racinh and they know they're ordered. And again, that's the whole idea of the B-tree that they're ordered and you go from here to here and then you get the records that you want. And this has got whatever, a hundred thousand records in it. And this explain analyze, there's a way to say without analyze, ad it just tells you its strategy. But explain analyze actually runs it and tells you how long it took to run it. So that took one hundredth of a millisecond and a millisecond is a thousandth of a second. So that's one one hundred thousandth of a second. So that's pretty fast, okay? to get a row from our database. And that's the kind of speed that we like to see. Now, if I only change one thing and I make it so that it's racing anywhere in the string, not racing at the beginning, then you see this in the explain. As soon as you see sequential scan, you go, I failed, I've written a bad, bad query. Right? And so what it basically says is the is the filter is we're going to check for this in the text and we're going to read all these things and it read all those rows. It actually couldn't do it in the database using the index. It just had to pull all the rows back and look through all the rows and it was throwing them away and it's like, okay, I'm going to throw away a hundred it had to throw a hundred thousand one rows away because it didn't find any rows because none of these rows in this database look like racing at all. They're not in there. It's got neon and lemons and other stuff like that and it took that 10.271 milliseconds. Now, that's actually, let's see, that's a hundredth of a second. And you might say, well who cares, right? Well, the problem is this is only a hundred thousand records. There's lots of things that have more than a hundred thousand and the other problem is these are short records. So, I have one column in them, that's it. I might have like 40 columns, way more, and so this problem is that this kind of stays the same as your data length and width grows and this gets bigger as your data length and width grows. And so this if you go over here, I think this is a thousand times slower. Right? A sequential scan in this tiny database is a thousand times slower than an index look up, right? So, yeah, here's the CREATE INDEX. And so here we go. Now ILIKE says ignore case, and I don't quite know why it's so bad, but it's three times worse if you're going to ignore case. So, it's really beautiful to write these things. So as you're writing as a database developer or a software developer, you write these and you think, oh, they kind of do the same thing. And the answer is no, one of these is scorchingly fast and one is slow and the ILIKE, which is ignore case, it has to sort of get the data, change its case, change the case of your WHERE clause for string, and then do the searching. And even though this is a prefix, it doesn't do much good. So it's not good. And it's so easy to run these, right? You just make your SELECT statement and they say explain analyze, and you don't have to always look at these numbers. But what you're trying to do is avoid sequential scan. Although for me, I'd be like, okay, I put a percent at the beginning. I'm going to get a sequential scan and then after a while you kind of know this stuff, right? And I said, B-trees are good for sorting, they're good for exact match lookup, and they're good for prefix lookup, which means they don't help you. I mean, they don't hurt you, but they don't help you. They take up space, but they don't speed you up if you're doing like sort of not prefix matching, which is the example that I'm using right here. So this is fun. I guess, I call it fun. Okay, so the thing I want to show you here is how these sequential scans work. And so this is an example where we have two sequential scans. This is the one we did before where it's a sequential scan because it's a LIKE that is in the middle. And so this one's actually going to match, right? And so, this one's going to match but watch this, oops. It has to read all the rows, because it doesn't know if the first row or the last row is going to match. Now, we know, given that these have between 100,000 and 200,000, it's only one, right? So in this case, we've added LIMIT 1, right? So what happens is as it's going through the data, if it encounters it, then it can stop. So this is where, if you know in your mind, there's no way the database can know that there's only one in the case that you're looking for 150,000 anywhere in it. It doesn't know when it can stop because it might find 0, it might find 1, it might find 1,000. And it has to read the whole thing. So it takes 14 milliseconds to read the whole thing. But if we can tell it LIMIT 1, because we kind of out-of-band knew that we generated these things. Well, that's actually halfway through. And so you can see that it drops it to 8.732 milliseconds from 14. And literally depending on the number that I picked, if I made this 100,000, it'd be really fast. And if I made it 199,999, well, then it'd be 14 milliseconds, because it really is a sequential scan. And the sequence is not necessarily sorted, it's just the sequence that we inserted them, which happens to be in order. So the sequential scans can be improved by adding a LIMIT 1 to them. So here's the IN clause, remember I told the IN clause. I just want to show you an example of IN. So explain analyze SELECT FROM textfun WHERE content IN. This is a set, you can almost think of this as where it's equal to this or this or this or this or that. And so this is just two things, right? And so this is either we don't know if it's an http or an https. Remember, 50% of them have it one way or the other. And so this IN clause. And so if you look and you say, how is this going to work? Well, the good news, you see right away, it's an index-only scan, right? And then when it's saying the index condition, it's sort of translated how you expressed it into how the database engine's going to think about it. But the database engine has access to that index. So it's like, I'm going to do two probes to the index. And so you see that this is a little slower than one probe to the index, which was a WHERE clause that's equal, right, or a prefix LIKE clause. But it knows it's going to have to do one for the first one and one for the second one, but that's it. So this took a little bit more time, but it's still some scalable. Now, oops, I hit the up arrow there. Now, this one here is yet another advertisement for why it is that you're not supposed to use sub-selects, right? So you might decide that you're going to sort of do something like, oh, the IN clause is great. So I'm going to do SELECT content FROM textfun WHERE content IN, and then I'll have a sub-select to get those two records, right? Well, so here's the sub-select. And remember, I talked about sub-selects. The sub-select runs. It produces a nice little two-record record set in this particular case. And then the outer part runs really fast, the IN clause runs really fast, but the other thing ran really slow. And that's kind of what it's telling you here, that inside here it's doing a sequential scan. And then the second, the outer part. And so this was a sub-select, right? And then this is the SELECT. And that was fast, but this was horribly slow, and it's still 14 milliseconds, right? So it's 14, a little higher, because it does two little index lookups at the very end, but it took you so much to figure out what those two strings were, it's like no, you lose, you can't do it. So again, this is just an advertisement for not using sub-selects. I'll have plenty of advertisements for not using sub-selects in this class. So we talked a bit about text and text functions and up next, we're going to talk about character sets.

# Character Sets

So now we're going to talk a bit about character sets. And we talked about text strings, but character sets are important to understand. I'll admit that I'm old, and when I started, the character sets didn't even have lowercase, that's how old I am, okay? So some of you watching may be that old, but I think most of you got into into technology some time after we had uppercase and lowercase. So the basic why in the old days we had only uppercase on a Control Data CDC 6500 computer had to do with memory space and the memory of the computers back then was very costly. And so the fact that we could use 6 bits per character instead of use 8 bits per character, which is what ASCII is, was a tremendous saving, because every single bit of memory had to be handmade. And so but upper and lower case is really much better for humans. It keeps us all from shouting all the time, but back then, upper So there is several standards for upper and lower case back then, as we were moving from uppercase, only to upper and lower case. There was EBCDIC, which was the IBM mainframe standard, and then ASCII, which was the standard for the rest of us. And ASCII was a much better standard than EBCDIC, because it was in everything's in order. You can see ABC. So if you look at this ASCII chart, you can see there's a numeric equivalent for every character. So we got the characters. Some of these are non-printing in this first column, non-printing. Then we have like exclamation point, and some stuff we got. So the zero character is encoded inside the computer as the number 48. And these are hexadecimal, or octal, or binary, these are the actual zeros and ones inside the computer. This is an 8-bit format. So there's eight zeros and ones, and so that's a byte, we call that a byte, so if you got megabytes. And then you have some more characters in upper lower, upper case, some more characters, and then lower case, and some more characters, and it stops at 127. So for example, if you're thinking well, why is it that the exponentiation operator, raising to a power, in Python is star star, right? Well, that's because we had that character, and it was on keyboards for a very long time. And so we tend to use these ASCII characters as the only special characters that have great meaning. because just a lot of the programming languages we use are 20 and 30 years old. And so they really kind of stuck with this character set for the essential things. And if you look at like the less thans and greater thans, and all these characters. And there's nothing in here, for example, that's greater than or equal, which if we were doing this in math, we would say greater than or equal. But that's why greater than or equal is greater than followed by equal sign, because that character, greater than or equal character, one character, is not here, okay? And even though now they've got those somewhere, we tend not to use them, because we're sort of always afraid that we're going to run into a keyboard, or character set representation that doesn't have that particular character. And like a superscript squared, a little 2 that's tall, I mean, a 2 that's small and moved up, some character sets have those. So you could say x squared, and this little 2 up there, and so we don't do that. So ASCII was our standard for a long time. It's a great standard, widely used, and we still use it basically. But and so in the old days, each character is a number stored in 8 bits of memory. Actually, you can go up to 256, and we'll talk about that, in 8 bits of memory. We call it a byte, and there's a function in Postgres and many other languages that can map between a character and its according number. So we've got what is the ASCII number for H, capital H? It's 72. What's the ASCII number for lowercase e? It's at 101, l is a 108. And then what is the character that's associated with 72? Well, that's a H, and what's the character associated with 42? It's an asterisk. So that's a good quiz question right there. What is the character associated with 42? Asterisk. Because 42 is very important. And so you can predict these numbers if you have a nice ASCII chart by going to find lowercase a, uppercase H is like right here. And then there's the 72. So you go from H to 72. You go from lowercase e, e to 101. And then if you're going to go, what's the 72? Well, you go down to 72, and you look over. What's 42? Let's go find 42, here's 42. What character's associated with 42, and that's an asterisk. And so you can just figure that out. One of the things we generally assume in ASCII is that like a is less than b numerically, and that is what we use for sorting and things like that. You'll also notice that uppercase is lower lexical, from a sorting perspective than lowercase. And in old days, we would even like subtract the current letter. I'm looking at minus lower uppercase A to get its position within the alphabet. So you'd subtract like 68 minus 65, and you'd get 3 to get to the D. Well, plus 1, and then you get the fourth character in the alphabet. So we used to do with these functions we have done, character, mathematics with characters. But that's not all that common these days. You're probably better off doing a greater than and less than. Now, I mentioned that inside of 8 bits, we can store up to characters 0 to 255. And so what happened is we started with under 127 for most of those printing characters and non-printing characters, but then they started adding things to beyond 127. And so we have like Latin-1, which is very US and European centered, which it has various like umlauts and schwas and various things like that. And they added them, and they picked numbers, right? And then Windows came out, and they have this character set called Windows 1252, and then they had the variations of these things. It sort of started getting to the point where all the ASCII would be the same, but then the second half would be different, and it would be important for you to know the code set. It's like, so okay, this is Cyrillic, this is Spanish, this is whatever. And so these became important, And the problem. The problem happened is you would get a file, and there was nothing in the file that would tell you which character set it was. And so if you misinterpreted it, you would just get messed up characters.
Play video starting at :7:7 and follow transcript7:07
And so there was all these overlapping character sets, because of this 128, extra 128 characters, which was completely different. And because they were not self-documenting, you'd send a file from Spain to the US, and you didn't get it right, or whatever. So they would create standards for what the second half of those numbers were up to 256. And they thought that was good enough, and the answer is no, that's not good enough, and it's a mess. So again, as the world moved towards more and more computers are not just Europe and America, we had to solve this problem. And so I'll be honest, if you had asked me back in the '80s or '90s, probably probably late '80s, you know, how to solve this. I would not have come up with this Unicode idea. I think it's darn clever, very clever. And so the idea is that instead of like code sets where you got here's a file and it's Cyrillic, here's a file and it's Turkish, and here's a file and it's Latin-1, and you've got to just tell people what it is. And then you have to tell your software which, and you have to install all these character sets on your computer. And then you've got to know them, and then you've got to code switch between them as you're looking at files,
Play video starting at :8:23 and follow transcript8:23
make one big character set, 32-bit numbers are 2 billion.
Play video starting at :8:30 and follow transcript8:30
They actually limited it to 21 bits to allow a nice compatibility with another character set called UTF-16. And so there are character sets, and then characters within those character sets. But now what we've done is created one big character set. So if you want like Chinese, there's several different Chinese traditional, Chinese simplified. They're each a character set, and then they get their own set of numbers, and when a new character shows up. And a lot of these character sets are both modern character sets and even historical character sets. There's enough space in Unicode to have historical character sets. And so what happens is if you look at this and say, okay, what is 72? Well, that's an H. What's 231? So this CHR is actually looking up the Unicode, and so there I think that's a cedilla there. And then what's 20013? Well, that's the character for China, right? And so there's this much larger range, and every character has its own specific. So the 20013, that's in a range of characters that is like the China characters. And so if you look near there there, there'd be other Chinese characters, and so there's your slot. Here's your slot, here's your slot. Oh, you got a new character set? Well, we'll get you another one at the end. And and so there's a 150 character sets, and a 137,000 characters. But this whole Unicode is something that evolves over time, so that's why there's a version to Unicode, right? 12.1, it changes pretty rapidly, but then they don't break the old ones. And so they just find a place to put every character. And again, it's awesome, it's just awesome. Except here's the Unicode code chart, so you can see, go look around, it's just impressive. It's just really impressive how they have mapped all these diverse things into one character set. But here's the problem. We can't afford to make every character on disk, on a network connection, in a database, be 32 bits long. That would basically quadruple instantly the amount of space that we do for text. And a lot of what we put in these things is text. We put in numbers, which are automatically very short. Text is by far the greatest thing we put in databases, or send across the Internet. And so we needed a compression scheme, and you can go to that Wikipedia page, UTF-8, and you can watch the research. I think it's kind of fascinating how they came up with ideas. And so the idea is that you go in zero through 127, you leave that be ASCII and you make it so that that's one byte. So the most common format is ASCII, and then you got these code pages that go to the 256, and then they have the extension. So the idea is every character is from somewhere between one and four bytes. And I'm not an expert at this, but basically, what they did was they could have used 32 bits, right? But they actually created signal characters. They reserve some of the bits as signals in a way to make it so they could say you know what, this second character looks a certain way. And so we can kind of assume that the only thing if it's UTF-8, it's 1 and 0 in the first two bits. And then everything else has got to be something else. And what that allowed them to do was it allowed them to have a reasonable conversion, or not perfect, but reasonable detection of Latin, like the old Latin-1, the Cyrillic, and those other ones. So you could look at a file and say, you know what? I don't think this is Unicode. Now, it didn't tell you what it was, but you could say it's probably not Unicode, mostly because of these prefixes were not every single prefix. Okay? So basically, you can ask how long? So there's a concept of characters, octets, that's the fancy way of saying bytes, and then bit length, okay? And so we can look at this little tiny four-character Chinese. I think this stands for learning management. It's four characters, because that's four Chinese characters. And in Unicode, a Chinese character takes the same width from a character perspective. Now, octet length, this is how it's actually stored in the database. So that actually is at 12 bytes to store four characters. So these must be around three-byte characters. These particular ones that I chose are three-byte characters. So there are four characters that turn into 12 bytes. And bit length is basically going to be always this number times 8, so it's 96 bits. And then ASCII tells us what the actual Unicode number is, because this just tells you that's only for a single character. What's the Unicode number for that particular character? Okay, so that's the difference between ASCII, but char_length, bit_length and octet_length, and this could be a column etc., etc,, etc. So it's any string that's sort of once it's kind of inside Postgres, it's going to be UTF-8. And so you can see that this would be 4 times 4 to the 16, if we're like using all 32 bits. So we're even saving on character sets like Chinese. So again, they're using some of these patterns, so they look at these patterns. And if they don't see these patterns in the later bytes, they're like, I don't think this is Unicode. And so they use this as a signal, so that they say look, I mean, this not UTF-8. And so they can at least detect by looking kind of algorithmically, you can say hey, can you tell me if this file is UTF-8 or not? And so because they made it the UTF-8 format more strict, and you can put anything in these four bytes, then they actually can detect like invalid Unicode. And sometimes you'll get it, right? Sometimes you'll be reading a file, or downloading something off the Internet that claims to be UTF-8, and your software will say no, it's not UTF-8, because it violated one of these rules. Now, the the key to UTF-8 was it started in the early 2000s. Like any technology that in effect wants to replace a previous technology, transition is a really important thing. And so the nice thing about UTF-8 is ASCII is the same as UTF-8. Up to 127, ASCII's the same as UTF-8. And so an ASCII file is a UTF-8 file, so a lot of things I did, because I'm from the United States, I'm like, yeah, it's UTF-8. I was only using those characters anyways, right? And so it made it really easy to do it. Now in databases, and on file systems, you actually had to explicitly sometimes convert back in the old days. But now we don't, we just make UTF-8 things in the database and files, and send them across the network. And so by 2004, it was it was a 60% of the web pages were already UTF-8, and then you look at the ASCII there that's by 2012, the ASCII is about 15%. So frankly, ASCII and UTF-8 are the same. ASCII is a perfect subset of UFT-8. So you can see that there's not much. There's Western Europe character sets, that would be kind of like those Cyrillic ones. And there was some Japanese character sets, and like everything else. And so you see them even in 2012. And so at this point, 94% of the pages in the world are just plain old UTF-8, and that's really cool. And so it took 20 years, but these days, we just assume UTF-8. If you look in Postgres, you will see that you can have a database with lots of different character sets. Now these, you should think of these as legacy. You should not say oh, I'm in Japan, so I'm going to use Japanese. You might find yourself in a legacy situation. You do not want to do these for new stuff, okay? These are the old kind of formats, and you want to come up with a scheme that translates these perhaps into UTF-8. So basically, and if you look at other databases like MySQL, it has like a jillion UTF-8s in it. What you really want is just to be UTF-8, and then convert your data on the way in. If you're kind of stuck with some legacy data, get your legacy data converted into UTF-8. And as of 2019 and later, you just use UTF-8 going forward. So now I want to just do a quick review of how we deal with these character sets in Python, and it has to do with external data being used internally in Python.

# Character Sets in Python

So now continuing talking about character sets, we're just going to do a quick review of how character sets work in Python, having to do with sort of when data is at rest versus when data is in motion. So Python 2 to Python 3, a big part of that transition was going to Unicode as the internal format inside of Python. So in Python 2, the internal formats of strings were ASCII 8-bit bytes. But again, that's because it's 20-some years old and was Northern European. And so the whole ASCII thing was Northern Europe and America. And honestly, by now, Python 3 is, everyone accepts it, but there was a lot of debate for well over ten years as to whether Python 2 was so good and Python 3 was like, don't worry, we don't even need this. Well, the answer is Python 3 had to happen and Unicode is the right answer. So with that as sort of background, Python 3 made the decision, and I think it's brilliant, that all the strings in memory are simple Unicode. It's not that big of a deal. They have another type called the bytes type, which is for 8-bit characters. As we'll see in a little bit, there is a purpose for bytes type, especially when you're starting doing compression and hashing and stuff like that. But Unicode is big, because it's 32 bits per character, but it's super fast for like going down looping through characters, like going down a 1,000 characters you can bounce really fast because they're four bytes, you don't have to look at every character to figure out, you go to the 40th character by going to the 160th byte. So Unicode is awesome for in memory. But when you're storing it on disk or sending it across the network or storing in a file, then you want to convert to UTF-8. And so it's both for interoperability, because other languages like PHP, or R, they're going to want to look at UTF-8. They're not going to want to look at Unicode. So even though Python has a Unicode inside, it has to read UTF-8 materials, work on it in Unicode, and then write it back out. And so database tables are in UTF-8, network connections are UTF-8, and files are UTF-8. So because the strings in Python are Unicode, every time that Python is talking to something that it knows to be external. It has to go through a decode process, and that is decode this encoded data that, you can almost think of UTF-8 as a compressed format and we're kind of uncompressing it. That's why I can think of decoding. So from a file or a UTF-8 network or a database, you're going to decode it before you work with it inside Python. And then if you have it inside Python, then you've got to encode it. So it's almost like a compression and decompression. Decompression when you read and compression when you write. And I talked about UTF-8. So if you open a file, you'll see that there is a parameter called encoding, and the default is none. But then the default is general, you can ask what the default is, and you'll find in most cases, 80-90 percent of the cases, maybe a 100 percent of the cases, that you're just going to have the encoding be UTF-8. And the reason UTF-8 works so well is that it's an old ASCII file, and it just works, right? The only time you'd use this as if it was like a Windows 1252 or a Latin-1 or something that had characters above 127 that were not UTF-8. And so most of the time the default works. So we just sort of kind of pretend, because ASCII just kind of grandfathered into UTF-8. So it happens automatically. And if you're reading it, now you can tell it I want to read it binary versus read it text, and then it gives you the bytes if you tell it, I want this to be binary. So when you read network data, so the decoding is happening implicitly once you open the file then it'll decode for you when you read the data. But in network data, if you're talking on sockets, some of the network like urllib does the decoding for you automatically. In this case, I'm talking to a socket directly and I have to do a decoding. So just look for this and if you see a decode, I just want you to understand what that means. It means I got some raw bytes from the outside world, and I'm expecting them to be UTF-8, but the string that I'm going to have inside Python, I want to be Unicode. So decode, decompress, take that data and get it internally. Now, it turns out that if you talk to a database in Python you use a thing called a database connector. And the cool thing about the database connector is that the database is marked as what the character set is, and like I said, you just want it to be UTF-8, but even if you had like a weird old character set, a weird old database that had ASCII character set, you'd be like, oh, okay, the connector actually as it's reading and giving you the rows back it's like, oh, I'm going to take this ASCII and convert it to UTF-8, or if you even had like one that had a legacy Japanese character set in a database, the database connector would convert from the legacy Japanese character set in the database on the way in into Unicode and it will convert Unicode to the legacy Japanese character set. So it turns out, in databases, it's worked really well for a long time. It worked pretty well because the connector knows the format the database has been created in. And so all the database stuff is stored in that format and then it does the conversion automatically. So you just say, get me that row in Python, and then you have a nice Python Unicode string and it's done automatically. So the file's pretty much done automatically, network stuff is done mostly automatically, and the database stuff is done on, and I just want you to be aware of the fact that you might have to move this data back and forth and you might have to convert that data. So if you're seeing something and it's really weird, and certain characters aren't showing up the way you do, look for the encoding and the decoding as potentially the problem that's causing sort of your confusion in terms of character sets.

# Inside Hashes
Now I want to take a little side side jaunt and talk to you about hashes. The key to why I like talking about hashes is I just want to be able to use it in a sentence and people think that hashes are magic, but they're not really magic. There's a lot of science behind hashes. They're critical for things like dictionaries and database indexes, and they're really, really important. So I want to dig in a little bit not so much so you could build a hash function, but so that you know how they're built so that you know they're not magic. So a hash function is basically a function that takes a large amount of data, from one character to a million characters, and performs some computation on that data and ends up with a fixed-length resulting value. And it's not a random value. It is a value that if you take the same data and run it through the hash function, you'll get the same value. But the idea of the output of the hash function is a fixed size. It might be 64 bits, or 128 bits, or 256 bits, but that's the idea. Now, within that, there is a whole bunch of science. Now, why do we need hashes? So, you send a bunch of data over a network, and that network might have some flaw. It might confuse something. It might flip a bit or something. And so what we would do is we would send the message and then we'd compute a hash as we were sending the message. And then we would send a fixed-length 128-bit checksum in the message. And so then we would receive the message, run the same calculation on the data as it comes in. We check the checksum. If something goes wrong, it's thrown away. And the checksum is often done in networking on much lower level protocols like Ethernet or Wi-Fi. They would automatically retransmit. So checksums are not something you're going to see a lot in your use of networks, say, in Python or whatever. But in the underlying media like the Wi-Fi or the Ethernet or even a phone connection, there's a lot of data checking going back and forth. Because Wi-Fi's not perfect, right? Even Ethernet's not perfect and phone modems are also not perfect. And if you're using sort of wireless on your phone, that's not perfect. And so when your data is being sent between your phone and a tower, there's a little checksum. And then what happens is the tower rejects it and says, you know, I didn't get all that because somebody did something, like a car started their engine or caused some interference. And so it ate part of the message, but it knows that because of this checksum. So hashes are a checksum. Another one is a cryptographic hash. And so we can use hashes sort of for basic testing or we can use them for like checking a signature. And in a sense, when we're using a hash for cryptography we hold it to a higher standard. And hash functions are essential to Python dictionaries and database tables and database indexes. And so it's really an important kind of underlying technology concept that I want you to understand. So there's a couple of things that we look for in hash functions. And people spend their whole life researching hash functions, literally. It's an area of research in computer science. We don't have to know it, but we can be thankful that some people build these really good hash functions. So deterministic means that it's not a random number. You put the same large amount of data in and the same thing goes out. Sensitive means if you send 100,000 characters and you change one character, the hash better be different, right? So uniform distribution, is kind of related to sensitivity in that you shouldn't just like generate, if you just take a bunch of random data and compute hashes on all the data, you should hope that it uses all the bits. So if it's like a 64-bit hash, it shouldn't just give you numbers from one to ten. It needs to use all possible combinations of 64 bits. So just uniform distribution. So it's pretty easy to take a bunch of random data, you run it through the hash function, and you kind of see where it happens. And that's so that they don't cluster or they don't collide. Because if you have a bunch of hundred-character strings, you want them spread out in the hash. Sensitive again, any change in input, whether it's adding a character or changing a character, should provide a change in output. One-way means that you should not be able to derive the input looking at the output. You can't go backwards. Now in a sense, if you have a million characters, and it comes down to 64 bits, it's kind of impossible to go backwards. But sometimes you're hashing things that are pretty short, like passwords. And actually hashing passwords is a common technique. so you're not storing the plaintext password, but it can also get you in trouble. Short passwords can become hashes, but then you can go backwards. And this is kind of the one of the reasons that they want passwords to be really long. Because if you have a really long password, then it's longer than the hash and it's harder to reverse it. And so we'll talk a little bit more about cryptographic problems. So I want to show you, I know it's just easy to read the Wikipedia. But I want to actually show you a hash computation. Okay? So I'm going to show you a couple of operators in Python that you probably will never use and after this, you'll probably never see, but I don't care. I think it's awesome. Okay, so what we're going to do here, let me get this nice pointer thing. So remember I was talking about the actual bits, right? So this is the zeros and ones. And this is the zeros and ones are how the computer actually stores the data and then things like 15. So this 11111, so this is 1, 2, 4, 8, 16. So this 15 is 8 plus 4 plus 2 plus 1 that's 11111 and that's 15. And so that's how we represent digital integer numbers. And you can print numbers out. So I got like x equals 15 and y is the ordinal position of H, which is 72. And I'm using this Python format and this 08b to say print this out. Print out x followed by x as an integer. But then I want to see x as bits. And so that's why we see x 15 and then 00001111, right? So I can print these things out. And I'm only showing you this because I want to show you debugging, okay? So these operators shift operate on the bits. So left shift the 1 shifts these bits over. An exclusive OR combines the bits in a way that if the bits are different it becomes a 1 and if the bits are the same it becomes a 0. So 1 plus 0 is 1, 0 plus 1 is 1 and 1 plus 1 is 0. And 0 plus 0 is 0. So if they're different, it's a 1. And it's a way of doing a, it's kind of an addition, but it's a merging addition. So addition makes numbers get bigger all the time. You can exclusive OR two numbers together and it's kind of a merger of the two numbers, but it doesn't get bigger. So if I keep adding 10 to something and so it's a way of taking each letter and sort of merging it into the same width. So I could take just the exclusive OR of a bunch of letters. And AND is what's called masking. So if we take this 15, which is all 1 bits here and 0 bits at the top. So it's four 0 bits and four 1 bits. It's a way to take this x, which is 72 right here. And it basically masks off and forces all the first four bits to be 0 and then it copies those bits. So that's an AND, okay? So that's this and you can take a little time to understand this but then I want to move on and show you how a hash function works. So this is like a very simple hash function in Python and you can grab the code for this. And the only line that really matters is right here. And so we're going to enter a string, we're going to stick with ASCII but it sort of doesn't matter. I'll just get out of this loop if there's nothing there and this hash value is an accumulation. And so what we're going to do is go through each line, each character, and we're to take the old hash value and we're going to left shift it one, I'll show you this over here. Then we're going to exclusive OR it with the ordinal number, the integer number that's equivalent to the letter, and then we're not going to let it get bigger than 32 bits. And so we're going to AND it with 0x, that's a hexadecimal digit, ffffff, six f's, and that just means this thing is going to get bigger, we're going to jam in. So it's like we're taking a character, we're shoving it over to the left, we're merging in the next character, we're moving that to the left, merge the next character, move it to the left, merge the next character, move it to the left, merge the next character, move it to the left. And then what we're doing is we're chopping it off anything that gets bigger than 6 times 4, 24. No. Yeah. Well, I got it. Well it stops at that size. So, and so then what I'm doing is if this hash values out to less than 2000, I'm printing it out in the letter, the bits of the actual letter, the working running hash value, and then the letter itself that we're looking at and a hash value in an integer. So if you look at Hello, so what you see is that the first character is this that's an H, and the hash value is just the first character. Then we get this e. So what happens is the current hash value gets shifted left one and then we combine using exclusive OR into that left shift so the e affects all these bits. So we're like shift, merge, shift, merge, shift, merge and you see this thing growing and eventually it will stop growing because we're only going to let them get as long as this.
Play video starting at :10:29 and follow transcript10:29
We are only going to get them this long, okay? But I'm only showing you the first part. So you don't have to fully understand this. You kind of see this shift, merge, shift, merge, shift, merge with a maximum length and part of hash functions is that they're a maximum length. And so if I end up with Hello, the hash result is 1 7 1 1 and this is a sensitivity. I change one character, the uppercase H to the lowercase h, and then the hash result is 11 99, so one character change needs to change the resulting hash. Now if I just flip two characters, e and h, that also changes, so this is the same characters but in slightly different order and so this is 1047. And again, that's why you shift then merge and shift and merge. So if you put an H then shift and merge an e it's quite different than putting in an e and then shifting and merging the H. So order matters, length matters, the characters matter, upper lowercase matters. What I'm showing you is a lousy hash function, okay? But it's one that if you take a look at you can kind of understand, you can run the code. and you see this shift, merge, shift. Shift, merge, mask, shift, merge, mask, shift, merge, mask. Okay? Now, a thing you might want to try is can you come up with two strings, just short short strings, that come up with the same hash? So far I've shown you no collisions, right? But a hashing function is bad if you can take two input strings that come to the same hash value. This one's not too hard because it's a terrible hashing function, but it is the essence of how hashing functions work. Okay? So designing real hash calculations is serious work. There is this organization called the National Institute of Standards and Technology called NIST that runs multi-year competitions when new hashing algorithms are needed. Now, what happens is we have a hashing algorithm, somebody comes up with it, maybe they publish a paper. In the old days they just like here's my hashing function. We'll be like, yeah, that looks pretty good and then everyone starts using it. And then someone would do research on what's wrong with that hashing function and we call these flaws. We're like well, I have these two strings and I run them through your hashing function, so my hashing function is highly flawed. All you have to do to kind of discredit my hashing function, this one here, if you find two strings, and it's not hard. If you find two strings that come up with the same hash that are different then you're like that's a lousy hashing function. And so what happens is this is in some ways some of the purest computer science research because a hashing is a beautiful simple algorithm that's easily expressed. And I can put it out there and then another person can find the flaw in it and they just say here's the two strings that crash, your hashing function is flawed, or I fixed your hashing function and I think mine is better than yours. So what happens is NIST will have competitions where teams from all over the world will propose new hashing functions. And then they have so that everyone has to sort of like put out their hashing function for the new hash, the new technique, and then all the teams go attack each other's hashing function. And so then they like, so you got 30 teams doing it, you're one, you put yours in, and then you go attack 29 of them, right? And so you have 29 people beating on your code to try to find out what's wrong with yours, right? And then they have a round where they, okay, here's the top 10 these 20 are [SOUND] everyone blew those up. So and then what they do is have another round of those 10, so they maybe let them change them a little bit from the learning of all the things that were wrong with them, so they tweak them a little more. And then they have a series of things and this takes more than a year and they have conferences about it. And it's really a super fascinating thing because when it's all said and done it's a little tiny function that we call MD5, okay? So tons of research, lots of people. MD5 is the wrong answer. the next one, SHA-256, is the one that was a multi-year. MD5 is a very early hash that was beloved, right? It's a 128-bit hash, which means you get a 128 bits out no matter what. And so when you go look at hashing, you go start reading up on hashing in Wikipedia, you'll see these pictures, right? And so what we're basically doing here is these are shifts, combinations, and exclusive ORs, okay? And so that's what's going on here, this is like exclusive OR this. So this is like that old hv value before and after because I wrote a loop that goes through every character and this is telling you what to do when you encounter a character. And I think this is going through four characters at a time A B C and D and it's doing something to those characters and shifting it and exclusive ORing it and doing a whole bunch of stuff and so that's what's going on here. And so somebody came up with this. It's a loop just like the one showed you except the middle part's a lot more complex because it's actually going through four characters at a time. And I don't even know the year but this has been around a long time and it's widely implemented. There's a function inside Postgres called MD5 that takes any string and gives you back the shortened version of MD5, we'll show you that in a second. Widely implemented, but it was also ferociously broken for cryptography. Now people will tell you that a MD5 is terrible, and for like if you have a thousand documents and you want to check which ones are unique, MD5 is fine for that. The problem with cryptography because the people making those documents aren't trying to construct the documents to cause a hash collision. If you're just looking at documents that are not intentionally trying to break MD5, it's actually not that easy to break MD5. It turns out there's whole conferences these days on breaking MD5, which is I think wonderfully fascinating. It's all been broken, but now they're trying to break it faster. And if it's cryptography and you are sending data to a trusted source, two trusted sources, and I can grab your data and I can twiddle with it and then move it on and the signature verifies, that's when it's broken for cryptography. So if it's going by, you signed it with MD5. I grab it, I alter it, and I keep the same MD5 signature, then I can fake your data. And so that's why MD5 is broken in those situations where someone with an intention to alter your data in transit can do so. The other problem that happened with MD5 that's kind of funny and fun is MD5 is not technically reversible. But because people used MD5 to hash passwords, so they would take your your password which is puppy123, they would run it through MD5 which gets this random number. They would store that in the database and then when you type your password puppy123 it would hash that and check the database. And that way they're not storing your plaintext password in the database. And so you think oh, that's great because MD5 is not really reversible to do all possible combinations of all possible passwords. But the problem is that the way we do passwords is there's a lot of common things. There's a lot of sequences of numbers like 1, 2, 3, the word puppy, the word puppy with the u turned into something. And so what's happened is the world has collected a lot of plaintext passwords and then run an MD5 computation on those plaintext passwords and ended up with terabytes of data. But if you type in an MD5 that is in the strings that they've pre-calculated, you can in an instant get the string back. So it's not really running the calculation backwards, you can't sort of run the calculation backwards. But if you run the calculation forwards on a large enough set of input strings you can then do a seeming backwards calculation back for those strings and for passwords in particular. So you can Google rainbow tables and you can create an MD5 of a puppy123 and then you can go backwards go to rainbow table and then type in the hash that comes out and it will come back with puppy123. And so again, it's broken for cryptography, whether it's checksums for signatures or whether it is storing data in a table. But it turns out it's not entirely broken for things like uniqueness and so we can use it for things in.
Play video starting at :19:17 and follow transcript19:17
Now the hash function that's the SHA-2 family of hashes that started in 2001 and took a couple years to build. The SHA-2 is of various lengths and SHA-256 is a 256-bit version of it. And again, you see this picture of like taking stuff and shifting and masking and ORing and doing and moving and shifting and doing this, that, and the other thing, right? with exclusive ORs and it's very complex and there was many hashes that competed to be the SHA-2. And so SHA-256 is built in and so you'll see it's a longer hash. The MD5, you can do MD5 function on any text that we have inside of Postgres and SHA-256. And this isn't just in Postgres, there's all kinds of places that have functions like MD5 and SHA-256 to compute these things.
Play video starting at :20:9 and follow transcript20:09
So that's a little bit of how hashes work and I just want you to basically understand hashes. And up next we're going to talk about how we use hashes and other kinds of index to really speed things up.

# Bruce Schneier - Cryptography, Hashing, and NIST
One of the things we've learned from the Snowden documents is that cryptography, broadly applied, gives the NSA trouble, at least at scale. So the NSA does a lot of cryptanalysis and they break a lot of systems, but well designed, well implemented cryptography does stymie them. And it's important to understand how it does. Because if the NSA wanted to be in my computer, they'd be in my computer. Done, period. No question about it. They would hack into my computer. And they have a lot of tools to do that. If they are not in my computer, one of two things are true. One, it's against the law, and the NSA is following the law. And two, I'm not high enough on their priorities list. Now what cryptography does is it forces the attacker, whether the NSA or the Chinese government or cyber criminals or whoever, to have a priorities list. And depending on their budget, they'll go down the priorities list, and the hope is you're not there, right? You are below their budget line. Without cryptography, an organization like the NSA can bulk collect data on everybody. With cryptography, they are forced to target. And that's extraordinarily valuable, because it means the FBI will go after the criminals. The NSA will go after the agents of a foreign power. And the Chinese government will go after the US Government officials that rise to whatever level they want to spy on. The cyber criminals will just go after a few of us. And the rest of us are protected. That makes cryptography a very important tool.
Play video starting at :2:11 and follow transcript2:11
Now, cryptography doesn't actually provide any security because cryptography is mathematics. When we say we trust the cryptography, what we're saying is we trust the mathematics and I think there's a lot of reason to say that. I trust the mathematics. Everything I know about cryptography tells me the mathematics is good. Certainly there will be cryptographic advances, certainly some things will be broken in the future, but by and large, the math works. But math has no agency. Math can't do anything. It's equations on a piece of paper. In order for math to do something, someone has to take that math and write code and embed that code in a program and embed that program in some bigger system and put that bigger system on a computer with an operating system, on a network with a user. And all of those things add insecurity. When the NSA breaks cryptography, by and large, they don't break the mathematics. They break something else. They break the implementation, they break the software, they break the network, they break the hardware the software is running on. They do something somewhere else. And again and again, we learned this lesson, that the math works, but putting stuff around it is much harder. Now there's an important corollary here, that complexity is the worst enemy of security. What these things do is they add complexity. The more complex you make your system, the less secure it's going to be because the more vulnerabilities you'll have, the more mistakes you'll make somewhere in that system. And we learn again and again when we see analyses of voting systems, embedded systems, your cell phone, messaging systems, email systems, that it's always something around the crypto. Something that the designers, the implementers, the coders, the users, got wrong. And the simpler we can make systems, the more secure they are. So what NIST is doing is they're trying to build standards around as much as possible. So they have a standard for a crypto algorithm. AES is the standard crypto algorithm. It was a public process where multiple groups submitted algorithms and the community as a whole picked a winner. It wasn't dictated on high. There weren't secret criteria. The AES algorithm was the one that most of us thought should be AES. Actually, there were several we thought were good candidates. They picked one. But there's a lot of trust in the process because there's a very public, open international process, right? SHA-3, the new secure hash standard, the same sort of process. Now it's really fun as a cryptographer to be involved in this process. I mean, I think of it is as a great crypto demolition derby. We all put our algorithms in the ring, beat each other up, the last one left standing wins. It was kind of like that, you know. We would all publish papers analyzing each other and one of the ones left standing won. But that's just such a small part of what NIST does. They have standards for random number generation, they have standards for key agreement, for different protocols. I mean, trying to standardize these components so the implementers make fewer mistakes. But still there's a lot that you can't standardize and those bigger pieces are where you're going to still find most of your vulnerabilities. I believe that's where the NSA finds most of vulnerabilities, that it's out there. Recently, we learned about vulnerabilities in the key agreement protocols that are used to secure a lot of the VPNs and Internet connections, right? And if you look at where that vulnerability was, it's because of a shortcut that was made and copied that allowed for a massive pre-computation. The math worked great. If you want to make a standard worse, you make it super complex, and you're just building in vectors at that point. And this is why the normal IETF process for Internet standards doesn't really work for security because those standards are compromises. Let's put in all the options and make everyone happy. Let's put in much flexibility as necessary to make the system as comprehensive as possible. That is sort of anti-cybersecurity Security needs as few options as possible. As simple as possible. You don't want compromise. You want one group to win because that group has a self-contained vision. When you have a piece of this and a piece of that and a piece of that, there's going to be some interaction you didn't notice. And that interaction will be the interaction that breaks your system. You didn't win AES, right? You were in it, you were in the demolition derby with your helmet on. Tell me a little bit about what it's like to be in the demolition derby toward the end and what it's like to sort of not win the demolition derby. So AES was an interesting process. It started out with 64 algorithms, of which 56 met the submission criteria. Then NIST whittled it down to I think it was 15 or 16 and then the next round whittled it down to five and then chose one. So it's a constant winnowing process. And Twofish, which was my submission, made it all the way into the top five. And those top five were all good algorithms. I mean, there was no bad algorithms there. And the arguments were more about security margin and implementability in hardware versus embedded systems versus constrained systems, eight-bit, 32-bit. So we were making distinctions about how we thought it would be used and and to me, it came down to, I think, three algorithms, and I thought these were all good choices. Twofish was one. Rijndael, the eventual winner, was one. And actually, at this point I forget what the third is. And what I said on my blog at the time is, you know, any of these three are good. And sure, it would been great to have been the winner. But there's a lot of value in NIST picking a non-US algorithm. And by picking an algorithm from Belgium, it said to the world that NIST is picking what they thought was the best and not trying to pick American. So that was an important consideration I hadn't thought of at the time. So I can't fault NIST in this process it all. It would have been great to win. It actually was really fun to participate. And, you know, I would do it again. And I participated in the SHA-3 competition, which again was picked by someone else won. My entry was called Skein. And you know these are lots of fun for cryptographers. Also for students, because they give students a whole bunch of targets. One of the hard things, if you're a crypto student, is you have to break stuff. The only way you learn how to make things is by breaking things. It's back to that security mindset, right? Anybody can create a security system that he can't break. So if you show up with a security system and say, I can't break this, my first question is, well, who are you? Why should I trust your attestation that you can't break it as something that's meaningful? What else have you broken? And these competitions give a whole bunch of targets so students can start breaking things that haven't been broken before, get papers out of them, get publications, get cred in the field as someone who can break stuff and therefore, as someone who can design stuff. It's a source of new problems. It's a source of new targets, but this is what I said to start. Security is inherently adversarial, and that adversarial nature makes it different. Unlike any other field in computer science, you go to a security conference, a crypto conference, and they are going to be papers of people who break each other's stuff and you have to get a thick skin. You have to understand that we are all learning. Now I produce a protocol and you break it, sure, I'm unhappy. But I've learned something, and so have you and so has everyone else. And that knowledge is more important than my particular creation surviving. And you have to understand that and accept that. And that has to excite you. 

# Index Choices and Index Techniques
So now we're going to get back to performance. Now that we know about hashes and indexes, we're going to talk a little bit more about performance. So the application that I want to play with is a hypothetical web crawler. And the way web crawlers work is a Python web crawler would grab a page from the Internet, it would look through all the links in that page, and then add links to a queue. But we need to know the ones we've already seen, because a lot of web pages point to the same web pages because they got like a navigation on the left hand side or something. So we need to know which pages we've already seen in our database. And the other thing that we do in a web crawler is you want to use databases because you can't crawl the entire web or whatever you're doing into a dictionary, and then do your database analysis. And so these processes of like scraping the web, they take a long time and you want them to be able to be restarted. So if you retrieved 10,000 URLs, it might take you like twelve hours to do that, because you might have rate limitations or whatever, and you just don't want to have to start from zero all the time, so databases in web crawling are essential. And so we're going to have URLs, and there's going to be a lot of tables, and there's going to be just a lot of URLs and we've got to decide if we've seen them before. And so when you're building something like this, its super tempting and incorrect to do something like say, you know what? Depending on your application, but how long is a URL? Is a VARCHAR 128 long enough for the UNIQUE index? If we knew that URLs were only 128 characters, then this would be a fine way to represent the data. So we've got our page itself in the content field, and then you got the URL in the url field. And you could decide you're just going to treat all characters, you're going to have your uniqueness constraint just be on the first 128 characters of the URL, or maybe make it 1024, and you could do that, right? The problem is in the real world other than truncating URLs to 1028 or to 1024, you just can't assume that URLs are long. You can't assume that URLs are a fixed length, right? And so the problem is if we sort of build something that's too long and we tell it it's 128 and we try to search something that's bigger than 128, Postgres just blows up. And you could truncate it, and that's up to you. That depends on your application and how much you want to know. But it's often a lot easier just to say, look, the URL is a text character too. But then we have another problem, okay? And so if we do that, and now I'm generating quite long URLs in this case, and so you'll notice we're going to have this text be a 4,000 long. So it's 4,000 Neon, Neon, Neon, Neon and we're going to have 5,000 rows that are 4,000 characters long. And so that's what this INSERT INTO cr2 is doing. We have an unlimited length TEXT field for the URL. And so we can insert these 5,000 so it's like 5,000 records times 4,000 characters, and we end up with like about five megabytes in the relation. We don't have any index, right? So what we're going to do is we're going to basically create a unique index, which is a slight variation. So you can create an index which is a B-tree, which allows duplicates. UNIQUE INDEX is a B-tree that doesn't allow duplicates. The B-tree sort of functions the same. So UNIQUE just is a sort of a rule that says you're not allowed to insert the same thing twice. And in something like this url, and that's our goal, we want to make it unique, because then we can try to insert it and the insert blows up, we know something. And we've talked about that before, you can like ON DUPLICATE KEY kind of stuff. So we don't have to add the index into the CREATE statement. We can say CREATE UNIQUE INDEX. This is the name of the index ON cr2 url. That's the URL column in table cr2, and this is the name of the index. And it does it. And then we can see, right? Because we already put the records in, so it's going to make the index for the records. You can see that basically the database is a certain size and the index sort of tracks almost directly. And that's because all of these URLs, every bit of the text, 4,000 characters long, ends up in the index. Because the index wants to know a perfect match, whatever we told it to do, and the longer these get, the bigger this index gets. Now, we don't have any content in any of these records. So ultimately, if you started putting pages in, this would have grown a lot faster. The size of the relational data would grow a lot faster than the index size.
Play video starting at :4:52 and follow transcript4:52
So one of the things that we do is I'll get rid of that one. I'll get rid of the cr2 unique index, because you can get rid of the indexes too, and you can do this live even if you're running an online application. Now, I'm going to create a different index, create a unique index on name the index cr2_md5. That's just my name for the index. CREATE INDEX cr2_md5 ON cr2. In this parentheses, this is the index expression. So I'm saying I want to compute the MD5 hash function of the URL. So this index is 4,000 characters, the URL is 4,000 characters long, but then we're going to end up with a 64-bit or a 128-bit MD5 result, and it's going to build a B-tree based on that. So the B-tree is much smaller. And so you can see, after we make this index, the index size is a lot smaller because we're only indexing the MD5 of the URL, okay? And so now, we can do something like we can SELECT star FROM cr2 WHERE url equals lemons.
Play video starting at :6: and follow transcript6:00
So because this index is not on the actual column, this does not use the index. So it has to do a full table scan because our url equals lemons, and the way you trigger the index is you say, where the md5 URL is equal the md5 of lemons. So now, it actually uses the index to do that. Okay? In the other one it would have used the index for this url equals lemons, but given that we made an index expression here, away it goes. And so this index condition, that's the part that gets sent to the database engine to go look these things up. So you have to kind of match in your WHERE clause the expression that you used to do the index. Now, this kind of looks arbitrary but it detects it, It is like oh, sweet, this is the thing I got an index for, so I know how to make the best use of that.
Play video starting at :6:52 and follow transcript6:52
So again you can do an explain and you can do the analyze. And the difference between the analyze is the explain just tells you what it would do to figure it out and what's it's going to ultimately do. But then the analyze says tell me how long it runs and so this is the one that's using the index and it's 0.14 milliseconds and this is the one without the index because it's not using MD5 here and so it's about a 100 times slower. That's because I've only got 5,000 rows. So I didn't put quite as many because in the previous ones I was showing a 100,000 rows but this is the difference between the explain, analyze, and just the explain and actually analyze causes it to run it and it tells you how long. It does the query and it tells you how long it takes. Now, a technique that I've used in these situations is that I've created this MD5 myself and put that in a column and then use a hash index on that. So it turns out that MD5 there's this datatype called UUID that is exactly the width and a very small column that is exactly the width of an MD5. It's kind of designed to be part of MD5. The fact that I call this url_md5, that's just the name I gave it. UUID is a datatype and I want that to be unique. So this url I'm actually not going to create an index on this. I'm going to create an index on this, but then I'm going to set that up. So here's how it goes where I'm going to insert into this making my 4K-long 5,000 rows, 4,000 in each row, and then I'm going to state an UPDATE statement where I'm going to calculate this md5 column based on taking the md5 urlL. This is a cast statement, colon colon uuid is casting it to the UUID type. That turns out to be right, it works, and so then it stores these things. But as it's storing in this UPDATE statement, it is actually computing this but then creating an index. Now you can see that I've got a little larger amount of data here because I've got this extra column, not just that column but I've got this extra column but the index size is smaller, right? Because it's doing a unique B-tree index based on this. And so I haven't told to do a hash index yet, but it's a B-tree index, and you can see as long as I'm doing the same thing. Now this is my column that I've computed and I know that this is an MD5 column, so this is the string I'm looking for. I do MD5 of lemons and then convert that to a UUID. And then it knows that's what's in the index and so it can do that particular query with an index scan and it's scorching fast, as you would expect. So if we kind of compare these index strategies we see we use just the URL as text with no index. We get a relation size of five meg and it's 1.7 milliseconds to read it. I do a basic B-tree index of MD5. It's smaller. This is a little smaller. The relation size doesn't change and the SELECT is 10 times faster. And then this one here is where I'm manually doing it. So you see the relation size is a little bit bigger but the SELECT is super fast. So you see like there's a speedup. So these are the kind of things where you can look at the alternatives and decide amongst yourself how but space you're willing to give up and how much speed that you're going to try to do. So there's no sort of one thing. It all depends on your application and how often you're going to do selects, how often you're going to do inserts, etc., etc., etc. But we've got a mechanism so look at these. So again, the B-tree maintains order, it's the default. It helps on exact lookup, prefix lookup, less than, greater than, ranging, and sorting. So that's a lot. The one thing it doesn't if you're doing like wildcards that are anywhere in the string. Hash is only good for exact lookup and in earlier versions of Postgres,
Play video starting at :11:11 and follow transcript11:11
if you read the warnings, you just don't use it, but Postgres 10 did a much better job and made it so that it does stuff so if it blows up halfway through, the index doesn't have to be rebuilt. And so I'm going to just show you one little example of using a hash. So, here's my stuff. So I've got just url, content, no special column. And I want to make cr4_hash. And USING hash, this is where I'm adding right here. Normally that would be USING B-tree. If you just let it do it or don't say USING at all, it's going to use B-tree. So I'm going to force it to use hash. Okay? So now that the data's all in, the same data that I've been using up to this point, the relation size is five meg and the index size is two meg, which is the smallest we've seen for an index size so far. And if you do a look for url equals lemons, you don't have to do the MD5 thing because it is doing a hash on the URL. I don't know if it using an MD5 internally. It might be usings SHA-256. We don't care. We don't need to know. And so this is a very small index with a very fast lookup time and we don't have to do any fancy bit. We just have a WHERE clause and you'll notice it's an equality. Greater than, less than, or LIKE with a percent at the end would not speed up on this one, right? So that's the danger of using, it only speeds up one thing. But if you're looking for an exact match in our little database crawling we are kind of are looking for exact match. And there's no sorting advantage on a hash as well.
Play video starting at :12:54 and follow transcript12:54
So if you look at the hash versus B-tree,
Play video starting at :13:2 and follow transcript13:02
this was sort of my fastest and cleverest way of doing it and I had to do a little bit of work with this. The relation size got larger because I made my own explicit column. But it was super fast. But I also then made a hash, just let Postgres make a hash based on the url, and so then the index size is much smaller, the relation size is much smaller but this particular one. I don't quite know. I would want this to be faster but I'm going to guess partly because I'm using I think Postgres 11 in this that hash is not as widely used. So they probably have something that just runs better on a B-tree than a hash. But hash saves you space and is roughly the same. In this case it's 1.5 times slower but I think if you made bigger data, it wouldn't be quite that much. So that's sort of another romp through looking at different index choices and different index techniques and now the up next we're going to talk about regular expressions.

# Demonstration: Generating and Scanning Text

Hello and welcome to another SQL walkthrough. In this one, we're going to go through a bit of the material on how to handle text. So here we go, there's some some things that are kind of cool. So, sometimes you want to generate a bunch of data because you're going to do some queries, you're going to do some explains or explain analyzes. And you want to fill up a table with a bunch of stuff and you don't want to type a bunch in. So let's take a look at some of the sort of building blocks of doing this. First, there's the number random, so if we do SELECT random random. Random is a number between zero and one that's random, if you call it more than once you're going to get a different number. So if I do this over and over, different number, different number, different number. And if you want to sort of get a number between 0 and 100, you do a trunc of the random times 100. So the floating point number is multiplied by 100, away you go, so that's one thing. You can also sort of concatenate a string together to make long strings, if you want. So if you want to make some gibberish, this generate_series is really awesome. because what it does is it looks like it's just one thing but sort of every time it hits it, it forces the generation of a new row. So this generate_series is like five rows with the number 1 through 5 in it. And then we can concatenate that, SELECT and so this double vertical bar is a string concatenation. This is one field, right? Except that this is going to generate five rows, okay? So we take a look at this guy, it generates five rows and each row is concatenating the word Neon to 1, 2, 3, 4, 5. And if you think about this from a Python perspective, this is kind of like a list comprehension where it's making a five-element list, Neon1, Neon2, etc. And so we can fill some things in, right? Let's make a little table here called textfun and we can use this. So this SELECT Neon, that generates five rows, but we can also just say, INSERT INTO textfun. Instead of saying values, we can say, SELECT. Now the key to it is there's got to be a column here. And there's got to be the same number of results of that SELECT at the end as there is in the INSERT column list. And we have INSERT INTO textfun content, and then a SELECT that comes right after that. And I can say then SELECT star FROM textfun, or I can fix my typo because I type too fast. So that's how it works, right? So again, that's inserting multiple rows, the SELECT is sort of driving it but the generate_series is what's driving the multiple SELECT. So, let's throw that little bit away. Let's play a little bit with some larger bits of data, let's make ourselves,
Play video starting at :3:8 and follow transcript3:08
Let's add an index, I named my index CREATE INDEX on text. textfun_b is the name of the index, textfun is the name of the table. And then content is the column, This shows that you can sort of create an index of your choice after the fact. And so it's like how ALTER TABLE works on a running system. Now you can ask, how much data is allocated on disk for the relation itself, which is basically the rows, and then how much is in the all of the indexes? And right now we have 8K, but they're empty so we can fill it up. Now, this is another one of those things that's going to generate a bunch of rows. You'll see that it's concatenated with generate_series 1000 through 1005. And this is just a CASE statement, it's kind of like an if-then-else, except that it's not like choosing a code path, it's actually generating one of two strings. So the random, remember every time you hit, it gives you another number, if it's less than 0.5, i.e., less than 50% of the time. So CASE WHEN, this is kind of like the if clause, then the word THEN. And then this is a string, and then ELSE, that's a string. And so what this does, this part right here, in the parentheses, is it either returns one of those two strings 50% of the time. And then concatenates that. So if I just do this part right here.
Play video starting at :4:39 and follow transcript4:39
I'm going to put a semicolon after it, it'll give me that one, it will give me that one. It will give me LEMONS. So SELECT CASE WHEN random less than 0.5 THEN. So this is returning a string but it's flip-flopping based on a random number. So the CASE is not exactly an if-then-else, it's sort of a more of a WHEN because it's evaluated over and over and over again, each time there is a row. But then when we concatenate that with generate_series from 1000 through 1005, then we get five rows and the random concatenation, LEMONS, neon, LEMONS, LEMONS, neon and it's driven five rows in the generate_series. So you can think of it as every time that CASE bit is hit, it picks randomly between those two strings. And then it does it five times with that number going from 1000 to 1005. So we can use this sort of mechanism and we can fill up some data. We're going to do an INSERT INTO textfun and then we're going to have those rows. Except now we're going to go and do a 100000. Now, I would say watch out when you're doing this, because it took a little bit of time because we're going across the network actually. That's kind of impressive that there's a database on the far end that can insert those rows and make indexes, etc., etc., etc. So that's been inserted and so I can ask how big is the relation size and the relation size is this number here, which is about six meg. The relation size correlates very directly to the rows, the index's size is pretty big. It can be bigger than it. In this particular case, it's because we're only indexing one column. And so the index's size is pretty big. But you can see how the index grows as we insert data and how the relation grows. So let's take a look at some string operations. So the part that's the most standard across databases is the LIKE operation and it has a wildcard that is a percent sign. So that basically says is go through here, find the content that has this number somewhere in it. So the percent is like any number of characters followed by any number of characters. And so that's like anywhere in the string. It happens to find it here at the end.
Play video starting at :7:11 and follow transcript7:11
We can ask for that material at the, I mean, oops, that didn't come out right.
Play video starting at :7:20 and follow transcript7:20
Oh, you can take the uppercase of that, sorry. I was missing what the difference was, that's the uppercase. So that does the upper, you can do lowercase, you can grab the rightmost four characters. You can grab the leftmost four characters. You can call strpos. And you can say where within the string am I looking at. Now that says, find the position in the string WHERE content LIKE under 150000. So that's going to show us that it's in position 2 because it would be in ttp, ttps colon, where is it?
Play video starting at :8:3 and follow transcript8:03
Oh, that's a 0 because it's http, not https at all, it's actually http, so it didn't find it. I take the s off, and it worked just fine, so it shows us in position 2.
Play video starting at :8:20 and follow transcript8:20
All right, sometimes it's http and sometimes it's s, I should probably fix that. So in the future when you're doing this, it'll just work a lot better. Okay, so we got a couple other functions we can play with. The split_part is like a lot of languages that allow you to do splitting. You give it how many pieces you want and which piece you want. So that's basically breaking it into pieces. Let's just say SELECT.
Play video starting at :9:2 and follow transcript9:02
It splits it based on slashes and gives us the fourth one, which is one, two, three, four, so it's the fourth one. So it gives us LEMONS. So you can use that just like a split in normal programming language. This translate is kind of weird in that it's going to do a one-to-one mapping. The lower case t is going to map to uppercase T, the lowercase h to uppercase H, the dot becomes exclamation, etc. So this has got five characters and this is just going to transform five characters. Why you want to do this? Who knows? You'll probably not do something nearly as silly as that. So we see the HTTPs are all uppercase, the slashes are underscores. And so, you can kind of see. That's just a tool that you may or may not need to use. So,
Play video starting at :9:56 and follow transcript9:56
another kind of wildcard, the percents are our wildcards that match many characters. And in this LIKE style you can, oops, there is another wildcard which matches a single character and that's an underscore. So these are not regular expressions, they're not command line things. They're not those kind of things. They are just that's a single character. So it's 150 followed by some character followed by two zeros. And so that's how we get these things right here. You can also say you can give a list of things with an IN, WHERE content IN and then a parenthesized list. So that, okay, is that because we don't have s's. Yeah, I'm sure I mismatched my s's and p's and the neons, etc. got my s's backwards, but that I'll fix that later. I think this should be s, and you can find it where it matches. Yeah, I still didn't find it, that's because the random numbers didn't work out in my favor this time, so. So there you go. Now when we're all done with this, let's just drop that table and so we don't fill our server up, okay?