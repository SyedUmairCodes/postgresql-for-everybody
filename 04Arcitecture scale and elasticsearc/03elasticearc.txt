# Elasticsearch Overview
So now, I want to talk a little bit about Elasticsearch. Elasticsearch is one of many possible eventual consistency databases that you might use. I don't have a lot of experience, but it's the one I have the most experience in. So we'll use it. And it's really commonly used to add value to an otherwise ACID-based system. So the history of Elasticsearch is that it emerged from an earlier open source project called Lucene, and the idea was to replicate the things that Google would do. It wanted to be super distributed. It wanted to take a firehose of input data. It wanted to be able to handle terabytes of data at rest, do large-scale parallel searching really fast, throw cheap commodity hardware at it and make it go faster. It's very much an inverted index with value add, with languages, with stemming, ranking, and relevance, and all that kind of stuff. Probably far more sophisticated ranking and relevance than Postgres ranking and relevance, which is not bad, but it's not great. A recommendation engine, all the kinds of things that you might want to do in an application that wanted to have Google-like features. And at some point there's this thing that says, like every application has to have a search box. And I think that's a fair statement, that every application should have a search box as long as it has information that people might want to find. There was an earlier project called Apache Lucene, which is really the indexing part of the technology. Elasticsearch is a really smoothing layer on top of a lot of otherwise difficult to do. It almost is like when you're outsourcing to Amazon DynamoDB, it's like there's something in there. It's really complex. And the same is true for Elasticsearch. There's something in there that's really complex, but you don't worry too much about that. It self-organizes within Elasticsearch. And what happened was what started out as kind of a search box feature for applications has become a NoSQL database in its own right, primarily because the inverted index had to be super performant, and updated distributed, and there's a lot of really nice things about it that basically made it, almost out of the box, a really good NoSQL or BASE-style database. I am really sensitive when I talk to you about technologies, what their license is. And Postgres is an open source community, one of my favorite open source communities. It just impresses me how long they function and the high-level function that they have. Elasticsearch is not entirely open source. They use a license called a open core. There's a core that is Apache licensed. And then there's this Elastic company, I think it's called Elastic NV, that supports it. And they're a very large company and very successful company.
Play video starting at :2:54 and follow transcript2:54
There's a whole range of whether I like open core or not. There are some open core vendors that I think are monsters that just are open core's bait-and-switch. Elastic somehow is a reasonably sized company that has a pretty good commitment to open source to the point where I think a lot of people use Elasticsearch without even realizing that it's not an open source project, it's an open core project. Everything I've ever done with Elasticsearch is the open source version. The problem with most open core companies, if their open source version is lame, and then they're like, "Well, you should just pay," well, then that's pretty bad. So it's okay I think for Elastic to charge for hosting and consulting. As long as we can do the things we want with a pure open source version and they don't show any urge to sort of like deprecate the open source version or sort of like bait-and-switch you, I'm okay with that. But you take care of that yourself. You take a look at that yourself, and hopefully there's enough of an open source community around the open source parts of Elasticsearch, which is the core parts, that they're not going to be motivated to go sort of evil on us. Kind of the way I've talked to you about how I'm nervous about MySQL, which is not particularly an open core project, just because they own it. It's Oracle. So I use Elasticsearch in project that is called Sakai, which is an open source learning management system. A real open source, not open core. And we used it in a kind of a hybrid situation. So we have a database system, MySQL or Oracle, MySQL is my preferred, for all the transactional bits, and grades, and memberships in classes, and things like that. We outsource the file storage for PDFs and things that students and teachers might upload, and then we have an Elasticsearch instance. And we feed both the blog posting, discussion postings, and pages, and everything, we feed all that into Elasticsearch as JSON documents by extracting the text and then sending the text in as a JSON document. And we also have extractors that read through PDFs and Excel spreadsheets and Microsoft Word documents. And there are extractors that extract the features from those, and then they feed those into a Elasticsearch index as well. And then we have a search UI that we talk in the user interface, we talk to Elasticsearch. When you type something like relational into the search engine, we find the posts that mention relational, and the Excel spreadsheets that mention relational, etc., etc., etc. So it's a very hybrid system. We're not really using it as a NoSQL database. So that's just sort of a lot of how Elasticsearch has historically been used. Probably, one of the more popular applications of Elasticsearch is called the ELK Stack. Elasticsearch, Logstash. and Kibana, all open source mostly. And this is beautiful. They're using really Elasticsearch, in this case, less as a search engine and more as a NoSQL distributed database or eventual consistency database. Logs are the things that all these production services are generating as fast as they can. A lot of data analysis is done with logs, and so Logstash is basically receive these things and then just blast it into Elasticsearch. And I talk often about how most database activity is read mostly, but this ELK is often write mostly. I mean, on average, it's probably doing more writing and has a higher demand for quick write performance than it does read performance, which is really kind of great, in that it stresses the underlying Elasticsearch database in ways that you might not stress it. Like we don't stress it that way in Sakai because how many PDFs get uploaded? Well, a couple an hour, which is different than a log thing is coming out 100 times a second. And Elasticsearch can absorb a firehose of data. A lot of the early NoSQL work was read mostly. And so it could absorb it at a certain rate and it had a great read performance, but its ability to absorb writes was kind of troublesome. A lot of magic systems have trouble with writes, but this ELK application or this ELK kit means that write performance is really good with Elasticsearch. Then there's Kibana, which is a visualization system that you may run into. And once your data's in Elasticsearch, you can create dashboards that just kind of blast the queries out. And in this, you see multi-readers, supermassive performance, and nice parallel distributed scatter/gather, and all that stuff comes into play when you want to basically ask what happened in the last 24 hours, and boom, you have a dashboard. So this ELK is a really beautiful use of Elasticsearch as a NoSQL database that strongly pushes the envelope of write performance and read performance, and it simply couldn't be done with a relational database system. So you just make this Elasticsearch thing bigger and smaller based on the resources you throw at it. It automatically reorganizes itself. And in there, there's Lucene and a whole bunch of other things, but Elasticsearch is kind of like the nice wrapper that works around it. I mean, I haven't yet done anything with Kibana, but I really want to. I mean, I like Elasticsearch and I like its write performance and I like its read performance, and Kibana seems like a no-brainer. It's pretty dang cool. So if you look kind of at the internal architecture of it, it's all based on REST Web Services. That's why I say Elasticsearch is a wrapper for a whole bunch of complex things that themselves might be difficult. So in a sense, Elasticsearch is your DynamoDB, except that you can install it and run it yourself. So you can feed it data at a high rate of speed and you can take data out and get queries, and it's all inside, completely distributed. There's all this eventual consistency, everything is communicated, indexes recalculated and everything are all like just magically delicious. And we have a beautiful little REST Web Service API, which makes it really easy to talk to it in just about any language because you're using JSON and REST Web Services. If you're using Python, or PHP, or Java, or whatever, this is really pretty straightforward. And the Elastic folks have built really cool clients that make talking to Elastic pretty easy from a wide range of programming languages. Of course, given that the data can come in to any of those servers, that's how it has really super-fast write performance, it is an eventual consistency system because the indexes are done sort of after the fact. So any of those servers can receive a new document. Any of those servers can start the indexing of that document, then add those to the inverted index, and then the inverted index itself is widely distributed. So the newly indexed documents are sent across and then exchanged in a few seconds, to maybe a minute or so, they are all eventually consistent, but the indexes are eventually consistent. I would say probably because it's really based on a search engine paradigm, the distributed indexing is one of the most advanced NoSQL databases when it comes to distributed indexing because it started as a distributed indexing problem and then kind of wrapped a NoSQL database around a super-fast distributed index. So this gives you a sense of the structure of the URL. And the ones we'll be using in this class, we're going to use HTTPS. You can send the credentials right on the HTTP command. But the key is, on the end of it, there's two parts of the URL. One is an index. You could think of the index, the index is kind of like a table. So those URLs that you'll see when you start writing code, that's the structure. Again, it's a REST Web Service, so I'm showing you a URL that captures this. So next, we'll just kind of talk a little bit about the programming pattern for Elasticsearch. But then, the most fun will be in the actual code walkthroughs.

# Programming Elasticsearch
So now I want to give you a short overview of the techniques used to program Elasticsearch, but the best way will be when I start showing you actual code and walk you through some of the different things that we use to program Elasticsearch. One of the harder things that you're going to see when you're using the Elasticsearch documentation, especially from the Elastic company, is they do everything in the form of REST APIs. And they are telling you basically the URL to hit, right? You're supposed to like hit a URL with a GET request and send a GET request to this URL and then send a JSON document that expresses what you want to do. If you're a real REST person, you might argue that this is not REST, it's more of an RPC pattern, RPC pattern, but we don't need to argue about that right now, a lot of philosophy in computer science. So you just send sort of your queries. So like this is a particular query that says match everything. This is a way to iterate through all of the documents. In Python, the simplest way to talk to any REST API is to use the requests library. And so you have a queryurl and that queryurl is constructed based on the documentation. In this case, my queryurl is going to talk to the table named testindex, and I append _search. This question mark pretty makes it so that the stuff returns with a nice curly brace. And then I'm going to convert this little bit of a dictionary into a string, so I'm going to use use JSON library dumps, which converts a dictionary to a string version of it. And we're going to send some headers that says this is the kind of content I went back, I'd like some JSON. And then I send a POST request, requests.post, to a queryurl with these headers and with this JSON body. Then I get back some text like the actual curly brace JSON stuff, and the status, which might be a 200 if it's good, a 404 if it's not found, or a 500 if things blew up inside. So that's the HTTP status code. And then I might parse that from a JSON string into an actual dictionary object so that I can walk through it. And so that's just a simple way of talking to any REST web service. It's very low level, but a lot of systems build a library to do that and so there is an Elasticsearch library in Python, so I'll just pip install it, and then I import the thing and then I from elasticsearch import Elasticsearch and then I sort of tell it all the stuff to make the actual connection. It knows the patterns for the index and then they send the body in as a dictionary. And so this makes it a lot simpler. So I will show you sample code of both, sort of the REST way of doing it and the Elasticsearch library way of doing it as well. And so we'll cover those in some sample code. So Elasticsearch is awesome. Whenever I'm like confused about how Elasticsearch works, I kind of fall back to this notion that it's like Google, but for me, right? I mean, I can feed it a bunch of things, I can hit it with queries. How would Google handle this? And so that's kind of the easy thing. It's super scalable, both the data ingest, the size of the data, and search performance at really high scales of data size. And it is nicely accessible through a REST API, which hides all of the complex detail of the pieces and parts that actually make up Elasticsearch. You can add it as a full-text search engine that's better and higher scale than something like the Postgres full-text search engine. In a lot of applications, that's their first encounter with Elasticsearch. But because its inverse index is so clever, if you set your mappings up the right way, you can make it quite formidable simple document store BASE-style NoSQL database. And so Elasticsearch is just one of many that I've chosen to show you.

# Demonstration: Loading a Book into Elasticsearch
Hello and welcome to another walkthrough for Postgres for Everybody. Of course, today we're going to play with the code for Elasticsearch, which is in effect a compare and contrast. So we've got a number of different bits of sample code.
Play video starting at ::19 and follow transcript0:19
And I'm going to start with elasticbook. So let's take a look at elasticbook.py. So elasticbook.py makes a connection and fills up a bunch of stuff with elastic, reads a book text from Gutenberg and then fills it up. So the first thing you've got to do is you've got to put your credentials. Oops, that would be my hidden one. hidden-dist has some sample credentials, and it's important that you set the elastic here.
Play video starting at ::53 and follow transcript0:53
In this particular example we are using the same user and index, so we're going to have an index that's the same as the username. Then whatever your autograder or whatever will give you these details to place in here. So you copy this into hidden.py and then you edit and put the real values in. So you will notice that in the beginning of elasticbook.py, you see that it's going to import hidden. And then later it's going to grab the Elasticsearch secrets, and then it's going to create an Elasticsearch client. And then we're going to actually use in this case the same index as for the user. We'll reuse one index over and over and over again. So if I take a look at the basic outline, it's going to wipe out the index, which is like dropping a table. And then it's going to recreate the index, and then it's going to read through these paragraphs and do some parsing. So I'll come back to that because it's going to take a while. And so I want to go ahead and start this, python3 elasticbook.py. And it asks me for .txt, so you'll see it's going to drop.
Play video starting at :2:14 and follow transcript2:14
Oh. what did I type wrong? 14091.txt. What did I type? 14091, ls p14*.
Play video starting at :2:27 and follow transcript2:27
ls *.txt. Oh, pg14091.txt, I forgot the g. So let's go ahead and start it. Clear.
Play video starting at :2:43 and follow transcript2:43
pg14091.txt. Okay, so it's going to drop the index. So it did that, and it started the index, tells us something about the index, and now it's off and running. So we'll come back to this. It's got a bit of work to do. So it's filling up the Elasticsearch right now. So let's take a look. We saw it open the book file, get our secrets, set up an Elasticsearch instance in Python using, and again you have to do a pip to install this elasticsearch library. Once that's installed in your virtual environment or whatever, you're good to go. And then we're going to basically just read, and we're basically looking for blank lines. And we're going to concatenate these lines together, and make paragraphs out of them. Okay?
Play video starting at :3:44 and follow transcript3:44
Here we are concatenating the lines together, it's a bunch of blank lines. And let's just take a look at the file. We got pg14 text. So what I'm doing if you look at this is I'm taking, taking and reading all these lines looking for a blank line. And then concatenating these so that they're one long line. Which means I'm getting rid of the newlines at the end, I'm concatenating, throwing a blank in. So you see I read a line, I trim it, then I read the next line, I trim it and concatenate it with a blank, and then at some point I get a blank line. And then I insert this whole paragraph, this whole paragraph gets inserted at this point in this body doc, right?
Play video starting at :4:27 and follow transcript4:27
Okay, so it's accumulating it, and then when it finds here a blank line, then it counts how many paragraphs it is, puts the content in. Now one of the things that's important for a Elasticsearch strategy is you do want a primary key. And so there's a couple of ways I could do primary keys here. I could have them just go up, one, two, three, four, five, six, seven, eight, nine, ten. That'd be fine. I could pick random numbers. But I've chosen instead to actually compute a predictable hash of the document contents. And so this I'm going to do a SHA-256. I'm going to do a SHA-256 on this whole thing.
Play video starting at :5:10 and follow transcript5:10
And what this means is, I don't know if this is what you want to do but it's whated I wanted to do, is I want each paragraph just to be here. Now I am keeping track of where it's offset into the document. So I'm making this document a JSON document, in this case it's just a dictionary. So we have the offset, which is which paragraph, and the content. And then I'm going to insert it and have a primary key, which is a big long hex number. But it's a hash, it's a really well-built hash because it's a SHA-256 hash. And so what happens is if I'm going to use the primary key here, and I'm sticking it in the index with that primary key, it does mean that if I have exactly identical text,
Play video starting at :5:56 and follow transcript5:56
I will only get one copy of the paragraph. Now you could do something different. You might decide that you want to make your primary key just be one, two, three, four, five. Now one of the things you don't want to do is you're not supposed to change the type of your primary key. So this is a string. If we used uuid4 hexdigest will give me back a string, uuid4 will give me back a string. pcount will give me a number, and either of those would work. Just don't change it once you start building it. And so we are adding these documents, you see it's an added document. It's still busily adding documents. I don't even know how many it's done. See how far it is, 2,000. 2,000 paragraphs have been added to our, can I go to the end. There we go. We've got to wait till it's all done, right? So at 2,200 it's still loading. Let's see, is there anything else while we're waiting?
Play video starting at :6:50 and follow transcript6:50
Oh, let me tell you about the index refresh. So one of the things that we do is,
Play video starting at :6:59 and follow transcript6:59
Elasticsearch sort of delays its index processing. And we've seen this in other databases where the index processing is delayed. And this says stop now and refresh your index. This is not something that you want to do all of the time, you want to say stop and let's recompute the index. Now if this was a highly scalable server with a whole bunch of clients, you wouldn't want to do this index refresh. You certainly wouldn't want to do it every time we went through this loop, right? This says, let's wait and catch up with the index. And that's what's going on here. And if I was going to start doing some reading of it, I would have to do a refresh to recompute the index before we did it. So let's see how we did here.
Play video starting at :7:41 and follow transcript7:41
Okay, so there we go. We loaded 2,600 paragraphs, 17,000 lines, and 875,000 characters. So I mean I built a little tool that we'll cover in a separate video called elastictool, python3 elastictool. Because there is no psql or any way that's a client. So I built one, and this knows from hidden.py, let's type help here, this knows for hidden.py. It went and grabbed that index. I can say match_all, and that will actually not match all of them and just get a bunch of them. And so it's showing you the documents, the first five documents. And so like okay, then we can see, like I can say search for penmanship. And that will find me a list of a number of the documents that have penmanship in them, right? So I can search for repose. Now one of the things is we've got these IDs here. Now if everything goes well, I should be able to do a GET based on the primary, not penmanship. I should be able to get a document based on its primary key. Let's see if that works. There we go. And so I just retrieved a document based on its primary key. So that primary key is the SHA-256 of that data right here. I probably should have, whoops, trimmed the left there, it's not perfect. So that's what this does. You can also just wipe out, if I type delete it will wipe out the entire index, which I don't want to do. But that's it. So I think that's pretty much all I wanted to show you for this elasticbook loader. So it's just a parsing of a lot of data and pushing it into a Elasticsearch database. So cheers, hope this helps.

# Demonstration: Loading Email into Elasticsearch
Hello and welcome to another walkthrough for Postgres for Everybody. We are currently walking through the sample code called elasticmail. And as always, you have to set up your hidden values to get whatever your key and your secrets and your prefixes and posts and ports and all that stuff. And so this particular assignment we are going to do something I did from Python for Everybody, where we have an archive of email lists for the Sakai open source project for 2005. And so this is some data that I used in all of my courses and it's on a super-fast server, its on dr-chuck.net. I think I actually crashed some of the people, so I made my own copy that's really super performant, this is really fast. And so we're going to hit this. And so email is kind of nasty, and so this is mailbox format. And this is pretty nasty stuff. and there's all these headers. The key to a mailbox format is it starts with From space followed by a series of headers, which are sort of key-value pairs with a colon and a space, and then a blank line and then the body. And so that's basically how email looks. And so a lot of this code unfortunately is just crazy date parsing and crazy whatever, and different email systems. And so I'm not sure how much value this all is. But that's the data we're looking at and I guess let me just run this, python3 elasticmail. And I can go grab ten messages and it grabs them. So it's retrieving this and it's parsing it and adding a document. Pulling a document over and over and over again. So I'm in effect really crawling an API full of data like this and I'm filling it into my Elasticsearch. So let's walk through the code.
Play video starting at :2:19 and follow transcript2:19
We're going to use the elasticsearch client that's in Python. Some of the parsing has this parsemaildate, that's because different versions of Python have different mail parsers and this is like backwards compatibility. So we connect up, we're going to use the same index as our user. We're going to drop it and then we're going to create it and then we're going to start walking through these mail messages, there's the base URL. And you can see that I can get ten and I can get two more. And so sometimes these are real long running processes and you want to restart it. If you're really going to do this, you probably wouldn't drop this index this way. I gave you a way, so let me just get out of here.
Play video starting at :3:12 and follow transcript3:12
Quit is not the right thing. So here's this elastictool, python3 elastictool. So I can say match_all. So there's a bunch of things in there. Now, one of the things you can do here, you could actually make it so that this would restart itself by commenting out this dropping and then if you wanted to restart it, you'd have to say delete because that would basically wipe out the index. Because now if I say match_all, there's nothing there, okay?
Play video starting at :3:45 and follow transcript3:45
And this delete command here is the same as running this, but I'm making it simple so that when you run this over and over and over, it just starts over and deletes it. So let me go ahead and start this back up. It's going to delete it, but I also deleted it once over here. So I'll throw ten messages in, get caught up with the ten messages. And so we're reading using requests, we're going to read this a bunch of code to read that API. If something goes wrong with that API, it counts the failures. It prints the URL and how many things we've got. So I just retrieved 2,434 from that URL. Then it's looking for From space, and again, that's if we were going to retrieve more than one at the same time. Then it's going to do some breaking to get the email with a couple of different formats. Then we're going to get date, hack the date. Then we're going take the headers and split them. And the headers, again, are all of these guys from here to there with the colon space. So you'll see I'm going to split things based on colon. So that says, let's use a regular expression. Go find everything that's not a colon or a space, up to including a colon a space and then extract all that stuff out. So we're extracting two parameters. We're extracting everything up to and including the colon space with a regular expression. And then we're extracting this second half with a regular expression. And then we're cleaning all this stuff up, converting it all to lowercase and then making a big giant dictionary of key-value pairs where this is the key and this is the value. Not including the colon, I believe. So that'll be the key because it stops at that point. And so now in this dictionary we have the date, which is separately parsed. And so now, we're going to actually insert this stuff. So we're going to make a document, we're going to have the email address, we're going to have the headers, and we're going to have the body. And then we are going to put that in
Play video starting at :6:20 and follow transcript6:20
store that email into the Elasticsearch index, then we're going to print out for our own debugging the fact that we've added the document, it was created on, here we go. That's the position. In this case, I'm using the start, which is the number. Goes up by 1, 2, 3, 4, 5, because I want to be able to restart it. So I want to know that I'm on the fourth email message or the 50th email message in printing out the set and every 100th document, you can see that I added it. I added it and it was successfully created. And then every 100th document, so if I do like 100 more it pauses, or at 200 more it'll run. Okay? So this is the nature of like scraping imperfect data where you just don't know how good the data is. And so let's see, that's running. I'll let that run for a while. So in elastictool.py, now I can do a match_all and you see that there's a bunch of stuff in there. And I can do a search for like Zhen. And you see some mail messages from Zhen. And you see, in this case, you see the index that it's in, the ID now is a increasing integer. And then the source document has got these headers, so you'll notice that the headers have been turned into a series of key-value pairs,
Play video starting at :8:3 and follow transcript8:03
which is really just, the headers are a JSON object. I'm trying to give this to Elasticsearch in a way that can be the best possible way. And then of course, the body is the actual mail message that Zhen sent to No, actually Glenn sent it to, it must be he sent that to from Zhen, or mentioned Zhen in it somewhere. So there you go. So I guess you would just hit Enter and we get out and so this can repeatedly just go and retrieve and retrieve and retrieve and then put it into an Elasticsearch database. So that's a quick walkthrough of the elasticmail code and I hope that you found this useful. Cheers.

# Demonstration: Elasticsearch Tweets

Hello and welcome to another walkthrough for Postgres for Everybody. In this one, we're going to walk through a really simple example
Play video starting at ::9 and follow transcript0:09
sort of from the Elasticsearch client documentation. And so, in some ways it's simple, but it's also very complete. And so again, you've got to get your hidden values set up the right way so that your import hidden, you've got to use pip3 to install elasticsearch if you haven't already done that. And away we go. So, print secrets, why do we want to print secrets? Let's not print secrets.
Play video starting at ::41 and follow transcript0:41
We don't want to show that in here. Then you would see all my secrets. So we won't print secrets. Secrets is just key-value pairs that come in a dictionary. There we go. And so we set the Elasticsearch client up with the host, the user, the password, the prefix, and so it takes care of all that. As we've seen in other examples, it's not really that hard to talk directly to it using requests, just hitting it, but away we go. So we're going to basically start just always by wiping out the index so that when we fill it up we know what we're doing and then create the index. And then basically, Elasticsearch thinks of the world in terms of documents, right? And so it's just a thing we start with. And you can put arrays in here, and lists in here, and other dictionaries in here, etc. The outer thing is always a dictionary if you're doing it right. And then the insert, you have to have a key, primary key in this case, I'm just calling to call that primary key abc. And es.index, I think of it as the insert. This thinks so much about the fact that it's the world's awesomest inverted index that it doesn't even think of inserting the documents, it thinks of indexing the documents. But there you go. We find out if it's a success or not. We can retrieve the document if we want. This refresh is in effect delay until all of the indexes are finished. This is a look up by primary key, so it's not that big of a deal.
Play video starting at :2:24 and follow transcript2:24
This says I'm going to actually use the inverted index, so finish. This can be costly, so I'm only doing it for demo purposes, tell it to recompute the index normally would take up to 30 seconds. And so would you like to pause? Now this is a more sophisticated query and you can take a look at this documentation. We're kind of following this particular documentation. So this is a query, Boolean is a combination. It just is a way to combine multiple in effect like and, right? And so this is like a WHERE clause. And this must match, that is looking for some text and that's actually not just a, that's a somewhat soft match, whereas this filter is like a hard match. This is like a WHERE clause. I'm looking up in this particular case, this is like applying a WHERE clause. A filter is like really and truly reducing the number of documents that are being searched. This bonsai is more of an approximation, something that looks like or sounds like bonzai, maybe with stemming, etc., etc. And so if you look at the document, I'm putting a type in here that it's a type tweet. Now, I'm only putting one document, so it doesn't matter. This is kind of your WHERE clause and this is like your soft text look up and the bool basically says both these things have to be true.
Play video starting at :4: and follow transcript4:00
Right, so, the bool wraps both the match and the filter. Well, the must and the filter, okay? And so then what we do is we basically call an es.search with this as the body, in this case, the elasticsearch library is conveniently convinced turning this from a dictionary into a string. And then we get our results and we can parse through those results and pull various pieces of the results out. So, this is going to just, there's no interactivity to it, it just runs. If you elastictweet, we drop the index, we create the index, we add a document, that's success add document. We retrieve the document based on the primary key that we just did and so somewhere is, there's the text. It's of type tweet, right? It's just a JSON. And then we do the index refresh, which again is pause and wait until the indexing catches up so that we can do other searches. And then we do the search, which basically says let's hit this. And we have a WHERE clause, the WHERE clause is going to match the type equals tweet, and then we got one hit. And then I go through and I parse this and I read that stuff out. And so that's just a simple walkthrough of a completely self-contained with no external data to put a document in, wait for the index to finish, and then retrieve the document back out. And so this is just a good starting point because you know it completely works, okay? So I hope that this little quick walkthrough allows you to work with and change that code and build inserts of various kinds that you like. Cheers!

# 